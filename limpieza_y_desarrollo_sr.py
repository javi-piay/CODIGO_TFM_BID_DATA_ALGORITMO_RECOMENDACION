# -*- coding: utf-8 -*-
"""Limpieza y desarrollo SR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P7gH2typDsKgg64zxZSSdwbEXruVKKn4

# Cap√≠tulo 1: Ingesta y Limpieza Base

En este cap√≠tulo cargamos los datos originales desde la API de Tripadvisor y Google Places,
exploramos su estructura y realizamos los primeros pasos de limpieza:

- Eliminaci√≥n de restaurantes cerrados permanentemente.  
- Normalizaci√≥n de campos clave.
- Detecci√≥n y eliminaci√≥n de duplicados (priorizando filas con m√°s informaci√≥n).   
- Primeras gr√°ficas exploratorias para entender la distribuci√≥n de ratings, rese√±as y ciudades.

### Ingesta de datos
"""

# ============================================
# CAP√çTULO 1: INGESTA Y LIMPIEZA BASE
# ============================================

# 1. Importar librer√≠as
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns

# 2. Cargar JSON inicial
file_path = "/content/restaurantes_estado.json"
with open(file_path, "r", encoding="utf-8") as f:
    data = json.load(f)

df = pd.json_normalize(data)

print("‚úÖ Dataset cargado")
print("Dimensiones:", df.shape)
print("Columnas iniciales:", len(df.columns))

# Vista previa
df.head(3)

"""Los datos iniciales provienen de dos fuentes principales:  
- **TripAdvisor (API a trav√©s de RapidAPI)**: aporta informaci√≥n sobre rese√±as, valoraciones y etiquetas asociadas.  
- **Google Places API**: complementa con atributos adicionales como ubicaci√≥n, horarios de apertura, rango de precios o n√∫mero de rese√±as.

El archivo `restaurantes_estado.json` combina ambas fuentes en un √∫nico dataset.  
Tras la carga, el dataset contiene **3.958 registros** y **66 columnas**.  
Cada fila representa un establecimiento hostelero identificado con un conjunto heterog√©neo de variables.

### Exploraci√≥n inicial del dataset
"""

# Info general
df.info()

# Nulos por columna (top 20)
df.isna().sum().sort_values(ascending=False).head(20)

"""#### Tipos de datos
La mayor√≠a de columnas son de tipo `object` (cadenas de texto o estructuras JSON anidadas), aunque tambi√©n se incluyen:
- Variables num√©ricas (`rating`, `reviews`, `latitude`, `longitude`, etc.).
- Variables booleanas/flags (`is_sponsored`, `reservable`, `delivery`).
- Listas y estructuras anidadas (`cuisines`, `open_hours`, `review_keywords`).

#### Valores nulos
La inspecci√≥n de valores nulos revela diferencias importantes en la completitud de los datos:
- **Alta cobertura**: campos como `id`, `name`, `rating`, `reviews`, `ciudad` y `address` est√°n completos para el 100% de los registros.  
- **Cobertura parcial**: `rating_google` y `reviews_count_google` aparecen en ~3.900 registros (‚âà98%), pero no en todos.  
- **Baja cobertura**: variables como `price_range_usd` (837 registros), `menu_link` (316), `reservation_link` (216) o `description` (300).  
- **Cobertura nula**: algunas columnas (`detailed_address.state`, `ranking`) no contienen ning√∫n dato √∫til.  

#### Primeras conclusiones
- La estructura inicial es muy rica, pero redundante y con una calidad de datos desigual.  
- Ser√° necesario un proceso de **normalizaci√≥n y limpieza** para reducir la complejidad de 66 columnas a un subconjunto manejable.  
- La exploraci√≥n de valores nulos servir√° como gu√≠a para definir qu√© atributos mantener, transformar o descartar en el modelado posterior.

### Eliminaci√≥n de restaurantes cerrados permanentemente
"""

# =========================================================
# Eliminaci√≥n de restaurantes cerrados permanentemente
# =========================================================

# Identificar restaurantes cerrados permanentemente
cerrados_perm = df[df["business_status"] == "CLOSED_PERMANENTLY"]
print("Restaurantes cerrados permanentemente:", cerrados_perm.shape[0])

# Eliminar del dataset
df = df[df["business_status"] != "CLOSED_PERMANENTLY"].reset_index(drop=True)

print("‚úÖ Dataset tras eliminaci√≥n de cerrados:", df.shape)

"""Uno de los primeros pasos en la limpieza consiste en asegurar que el dataset solo contenga restaurantes **en funcionamiento**.  
El campo `business_status` (proporcionado por Google Places) indica si un establecimiento est√° **activo** o **cerrado de forma permanente**.

En esta fase se identificaron y eliminaron **289 restaurantes cerrados permanentemente**.  
Esto es necesario porque:

- No aportan valor al sistema de recomendaci√≥n (no son opciones disponibles para el usuario).  
- Podr√≠an sesgar m√©tricas como el n√∫mero de restaurantes por ciudad o el promedio de valoraciones.  

Tras esta limpieza, el dataset qued√≥ reducido a **3.669 registros v√°lidos**, garantizando que el modelo solo trabaje con establecimientos activos.

### Normalizaci√≥n de campos clave

Antes de abordar los duplicados, se realizaron varios procesos de **normalizaci√≥n** para unificar formatos y reducir inconsistencias entre TripAdvisor y Google Places.

#### Construcci√≥n de un campo homog√©neo de horarios
"""

import ast
import numpy as np

def build_open_hours(row):
    days = ["sun","mon","tue","wed","thu","fri","sat"]
    horario = {}
    for d in days:
        col = f"open_hours.{d}"
        if col in row.index:
            val = row[col]
            if val is None or (isinstance(val, float) and np.isnan(val)):
                horario[d] = []
            else:
                try:
                    if isinstance(val, str):
                        # Intentar parsear a lista de dicts
                        parsed = ast.literal_eval(val)
                        horario[d] = parsed if isinstance(parsed, list) else []
                    elif isinstance(val, list):
                        horario[d] = val
                    else:
                        horario[d] = []
                except:
                    horario[d] = []
        else:
            horario[d] = []
    return horario


df['open_hours_dict'] = df.apply(build_open_hours, axis=1)

"""
La informaci√≥n sobre horarios de apertura aparece en m√∫ltiples columnas (`open_hours.mon`, `open_hours.tue`, etc.), con formatos heterog√©neos que combinan **listas**, **strings serializados** o valores nulos.

Para simplificar el an√°lisis, se construy√≥ un nuevo campo unificado llamado `open_hours_dict`, que contiene un diccionario estructurado por d√≠a de la semana:

```python
{
  "mon": [{"open": "13:00:00", "close": "16:30:00"}, {"open": "20:00:00", "close": "23:30:00"}],
  "tue": [...],
  ...
}
"""

# Seleccionar un restaurante concreto (por ejemplo el primero)
ejemplo = df[['open_hours_dict']].iloc[1]

# Imprimir de forma vertical y clara
print("üìå Ejemplo de horarios unificados\n")
for dia, horarios in ejemplo['open_hours_dict'].items():
    print(f"{dia.upper():<4} -> {horarios}")

"""#### Priorizaci√≥n de Google frente a TripAdvisor"""

import pandas as pd
import numpy as np
import unicodedata
import ast
import matplotlib.pyplot as plt
import seaborn as sns
# -------------------------
# 1. Priorizar Google > TripAdvisor
# -------------------------
df['nombre_final']   = df.get('name_google').fillna(df.get('name'))
df['rating_final']   = df.get('rating_google').fillna(df.get('rating'))
df['reviews_final']  = df.get('reviews_count_google').fillna(df.get('reviews'))
df['maps_url_final'] = df.get('google_maps_url').fillna(df.get('maps_url'))

# Construcci√≥n de horarios_final con prioridad Google > TripAdvisor
df['horarios_final'] = (
    df.get('open_hours_google', pd.Series(index=df.index))
      .fillna(df.get('open_hours_google.weekdayDescriptions', pd.Series(index=df.index)))
      .fillna(df['open_hours_dict'])
)


print("‚úÖ Ejemplo nombres unificados:")
display(df[['name','name_google','nombre_final']].sample(5))

print("‚úÖ Ejemplo rating/reviews unificados:")
display(df[['rating','rating_google','reviews','reviews_count_google','rating_final','reviews_final']].sample(5))

print("‚úÖ Ejemplo horarios unificados (horarios_final):")
display(df[['nombre_final','open_hours_google','open_hours_google.weekdayDescriptions','open_hours_dict','horarios_final']].sample(5))

"""Dado que los datos provienen de dos fuentes distintas, se defini√≥ un criterio de **prioridad** para evitar inconsistencias:

- **Nombre del restaurante**: se toma `name_google` y, en caso de estar vac√≠o, se usa `name` de TripAdvisor.  
- **Rating y n√∫mero de rese√±as**: se priorizan `rating_google` y `reviews_count_google`, ya que suelen estar m√°s actualizados.  
- **Mapa y localizaci√≥n**: se usa `google_maps_url` como referencia principal, con `maps_url` como respaldo.  
- **Horarios de apertura**: se emplea `open_hours_google` o `open_hours_google.weekdayDescriptions` (estructura textual de Google) y, en ausencia de estos, el campo unificado `open_hours_dict` creado anteriormente.  

El resultado son los campos:  
- `nombre_final`  
- `rating_final`  
- `reviews_final`  
- `maps_url_final`  
- `horarios_final`  

que concentran la informaci√≥n m√°s fiable y homog√©nea de cada restaurante.

#### Normalizaci√≥n de horarios
"""

from collections import defaultdict

tipos = defaultdict(int)
ejemplos = {}

for i, val in df['horarios_final'].dropna().items():
    if isinstance(val, dict):
        tipos["dict_tripadvisor"] += 1
        if "dict_tripadvisor" not in ejemplos:
            ejemplos["dict_tripadvisor"] = val

    elif isinstance(val, list):
        if any("lunes" in str(x).lower() for x in val):
            tipos["list_es"] += 1
            if "list_es" not in ejemplos:
                ejemplos["list_es"] = val
        elif any("monday" in str(x).lower() for x in val):
            tipos["list_en"] += 1
            if "list_en" not in ejemplos:
                ejemplos["list_en"] = val
        else:
            tipos["list_otro"] += 1
            if "list_otro" not in ejemplos:
                ejemplos["list_otro"] = val

print("üìä Distribuci√≥n de tipos en horarios_final:")
for t, c in tipos.items():
    print(f"- {t}: {c}")

print("\nüîé Ejemplos de cada tipo:\n")
for t, ex in ejemplos.items():
    print(f"‚ñ∂ {t}\n{ex}\n{'-'*50}")

"""Durante la exploraci√≥n de la columna `horarios_final` se comprob√≥ que los datos de horarios no segu√≠an un √∫nico formato.  
En concreto, se identificaron tres estructuras distintas:

**Listas en espa√±ol (`list_es`)**  
   - D√≠as escritos en castellano (`lunes`, `martes`, ‚Ä¶).  
   - Horas en formato 24h (`10:00‚Äì17:00, 19:00‚Äì24:00`).  
   - Ejemplo:  
     ```python
     [
       'lunes: 10:00‚Äì17:00, 19:00‚Äì24:00',
       'martes: 10:00‚Äì17:00, 19:00‚Äì24:00',
       'mi√©rcoles: 10:00‚Äì17:00, 19:00‚Äì24:00',
       'jueves: 10:00‚Äì17:00, 19:00‚Äì24:00',
       'viernes: 10:00‚Äì17:00, 19:00‚Äì24:00',
       's√°bado: 10:00‚Äì17:00, 19:00‚Äì24:00',
       'domingo: Cerrado'
     ]
     ```

**Listas en ingl√©s (`list_en`)**  
   - D√≠as en ingl√©s (`Monday`, `Tuesday`, ‚Ä¶).  
   - Horas en formato AM/PM (`9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM`).  
   - Ejemplo:  
     ```python
     [
       'Monday: 9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM',
       'Tuesday: 9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM',
       'Wednesday: 9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM',
       'Thursday: 9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM',
       'Friday: 9:00 AM ‚Äì 1:30 PM, 4:00 ‚Äì 7:30 PM',
       'Saturday: 10:00 AM ‚Äì 2:00 PM',
       'Sunday: Closed'
     ]
     ```

**Diccionario estilo TripAdvisor (`dict_tripadvisor`)**  
   - Claves en ingl√©s (`mon`, `tue`, ‚Ä¶, `sun`).  
   - Horarios en formato 24h con campos `open` y `close`.  
   - Ejemplo:  
     ```python
     {
       'sun': [{'open': '08:30:00', 'close': '22:00:00'}],
       'mon': [{'open': '08:30:00', 'close': '22:00:00'}],
       'tue': [{'open': '08:30:00', 'close': '22:00:00'}],
       'wed': [],
       'thu': [{'open': '08:30:00', 'close': '22:00:00'}],
       'fri': [{'open': '08:30:00', 'close': '22:00:00'}],
       'sat': [{'open': '08:30:00', 'close': '22:00:00'}]
     }

**Problema detectado**

La coexistencia de distintos formatos dificulta el an√°lisis y la explotaci√≥n de los horarios, ya que:

* Las claves de los d√≠as aparecen en diferentes idiomas (sun‚Äìsat en ingl√©s vs. lunes‚Äìdomingo en espa√±ol).

* Los intervalos se expresan en distintos formatos (24h, AM-PM o hh:mm:ss).

* Algunos tramos cruzan medianoche (ej. cierre a las 00:00 o m√°s tarde).  

**Soluci√≥n propuesta**

Se dise√±ar√° una funci√≥n de **normalizaci√≥n de horarios** que convierta todos los registros a una misma estructura est√°ndar en espa√±ol, con las siguientes caracter√≠sticas:

- **Claves en espa√±ol**: cada d√≠a de la semana (`lunes`, `martes`, ‚Ä¶, `domingo`) aparece como texto al inicio de la cadena.  
- **Valores**: franjas horarias en formato `HH:MM‚ÄìHH:MM`. Si existen varios tramos, se separan por comas (ejemplo: `lunes: 10:00‚Äì17:00, 19:00‚Äì24:00`).  
- **Cerrado**: se representa expl√≠citamente con la palabra `"Cerrado"` (ejemplo: `martes: Cerrado`).  
- **Cruces de medianoche**: se mantienen tal cual en el mismo d√≠a, por ejemplo `lunes: 20:30‚Äì04:00`.
"""

import re
from datetime import datetime

# Diccionarios de d√≠as
dias_en = ["monday","tuesday","wednesday","thursday","friday","saturday","sunday"]
dias_es = ["lunes","martes","mi√©rcoles","jueves","viernes","s√°bado","domingo"]
map_days_en_to_es = dict(zip(dias_en, dias_es))
map_days_short_to_es = {
    "mon":"lunes","tue":"martes","wed":"mi√©rcoles","thu":"jueves",
    "fri":"viernes","sat":"s√°bado","sun":"domingo"
}

def to_24h(hora: str) -> str:
    """Convierte strings con AM/PM o 24h a HH:MM (24h)."""
    hora = (
        str(hora)
        .strip()
        .replace("\u202f", " ")
        .replace("\u2009", " ")
        .replace("‚Äî", "-")
        .replace("‚Äì", "-")
    )
    formatos = ["%I:%M %p", "%I %p", "%H:%M", "%H"]
    for fmt in formatos:
        try:
            return datetime.strptime(hora, fmt).strftime("%H:%M")
        except:
            continue
    return hora

def parse_interval(tramo: str) -> str:
    """Convierte un tramo tipo '5:00 ‚Äì 7:00 PM' a formato 24h."""
    tramo = (
        tramo.replace("\u202f", " ")
             .replace("\u2009", " ")
             .replace("‚Äì", "-")
             .replace("‚Äî", "-")
    )
    partes = [p.strip() for p in re.split(r"\s*-\s*", tramo) if p.strip()]

    if len(partes) == 2:
        ini, fin = partes
        # Si fin tiene AM/PM y ini no ‚Üí heredamos
        if re.search(r"(AM|PM)", fin, re.IGNORECASE) and not re.search(r"(AM|PM)", ini, re.IGNORECASE):
            sufijo = re.search(r"(AM|PM)", fin, re.IGNORECASE).group()
            ini = f"{ini} {sufijo}"
        return f"{to_24h(ini)}‚Äì{to_24h(fin)}"
    else:
        return "‚Äì".join(to_24h(p) for p in partes)

def normalizar_horarios(horarios):
    salida = {d: "Cerrado" for d in dias_es}

    # list_es
    if isinstance(horarios, list) and any(h.lower().startswith("lunes") for h in horarios):
        for h in horarios:
            dia, horas = h.split(":", 1)
            salida[dia.strip()] = horas.strip()

    # list_en
    elif isinstance(horarios, list) and any(h.lower().startswith("monday") for h in horarios):
        for h in horarios:
            dia, horas = h.split(":", 1)
            dia_es = map_days_en_to_es[dia.strip().lower()]
            if "closed" in horas.lower():
                salida[dia_es] = "Cerrado"
            else:
                intervalos = [parse_interval(tramo) for tramo in horas.split(",")]
                salida[dia_es] = ", ".join(intervalos)

    # dict_tripadvisor
    elif isinstance(horarios, dict):
        for dia_en, tramos in horarios.items():
            dia_es = map_days_short_to_es[dia_en]
            if not tramos:
                salida[dia_es] = "Cerrado"
            else:
                intervalos = [f"{t['open'][:-3]}‚Äì{t['close'][:-3]}" for t in tramos]
                salida[dia_es] = ", ".join(intervalos)

    # ‚ö†Ô∏è Caso especial: todos los d√≠as = Cerrado
    if all(v == "Cerrado" for v in salida.values()):
        return ["Horario no disponible"]

    return [f"{d}: {salida[d]}" for d in dias_es]


# Aplicar al dataframe
df['horarios_norm'] = df['horarios_final'].apply(normalizar_horarios)

print("‚úÖ Ejemplo horarios normalizados:")
ejemplos = df[['nombre_final','horarios_final','horarios_norm']].sample(5, random_state=45)
for _, row in ejemplos.iterrows():
    print("‚ñ∂ Nombre :", row['nombre_final'])
    print("‚ñ∂ Entrada:", row['horarios_final'])
    print("‚û° Normalizado:", row['horarios_norm'])
    print("-"*80)

"""La funci√≥n de normalizaci√≥n implementada convierte autom√°ticamente cualquiera de estos casos en un formato **homog√©neo, en espa√±ol y legible**, siguiendo las siguientes reglas:

- **D√≠as en espa√±ol**: `lunes` a `domingo`.  
- **Horas en 24h (`HH:MM`)**: se convierten de forma uniforme tanto desde AM/PM como desde 24h.  
- **M√∫ltiples tramos**: se conservan y se separan por comas.  
- **D√≠as cerrados**: representados como `"Cerrado"`.  
- **{'sun': [], 'mon': [], 'tue': [], 'wed': [], 'thu': [], 'fri': [], 'sat': []}**: representados como `Horario no disponible"`.  
- **Horarios vac√≠os**: si todos los d√≠as est√°n cerrados, se etiqueta como `"Horario no disponible"`.  
- **Medianoche (`12:00 AM`)**: se interpreta como `24:00` o `00:00` para reflejar cierre al final de la jornada y evitar ambig√ºedad.  

Gracias a esta normalizaci√≥n, los horarios son consistentes y comparables en todo el dataset, lo que facilita su an√°lisis, visualizaci√≥n y uso posterior en el sistema de recomendaci√≥n.
"""

# Info general
df.info()

"""### Normalizaci√≥n de variables textuales y categ√≥ricas

Adem√°s de los horarios, fue necesario homogeneizar otras variables del dataset relacionadas con el texto libre y las listas categ√≥ricas. Este proceso evita inconsistencias y duplicidades en los datos, facilitando su an√°lisis y posterior uso en el sistema de recomendaci√≥n.

"""

import re
import unicodedata
import ast
import numpy as np

def clean_text(value, mode="generic"):
    """
    Funci√≥n gen√©rica de limpieza de texto para distintas columnas.

    mode:
      - "city"        ‚Üí nombres de ciudades
      - "name"        ‚Üí nombres de restaurantes
      - "list"        ‚Üí columnas categ√≥ricas tipo cuisines, diets, etc.
      - "description" ‚Üí texto libre de descripci√≥n
      - "address"     ‚Üí direcciones postales
      - "generic"     ‚Üí limpieza b√°sica (min√∫sculas + quitar acentos)
    """
    # -------------------
    # Casos nulos
    # -------------------
    if value is None or (isinstance(value,float) and np.isnan(value)):
        return np.nan
    if not isinstance(value, str):
        return value

    # -------------------
    # Limpieza base
    # -------------------
    text = unicodedata.normalize("NFD", value)
    text = "".join(c for c in text if unicodedata.category(c) != "Mn")  # quitar acentos
    text = text.lower().strip()

    # -------------------
    # Casos seg√∫n modo
    # -------------------
    if mode == "city":
        return re.sub(r"\s+", " ", text)

    if mode == "name":
        text = re.sub(r"[^a-z0-9\s]", " ", text)   # dejar solo letras/n√∫meros
        text = re.sub(r"\s+", " ", text).strip()
        return text if text else np.nan

    if mode == "list":
        try:
            parsed = ast.literal_eval(value)
            if isinstance(parsed, list):
                items = parsed
            else:
                items = value.split(",")
        except:
            items = value.split(",")
        return sorted(set(
            re.sub(r"\s+", " ", i.strip(" []'\"")) for i in items if i.strip()
        ))

    if mode == "description":
        text = text.replace("\n"," ").replace("\r"," ")
        text = re.sub(r"[^a-z0-9\s]", " ", text)  # despu√©s de quitar acentos
        text = re.sub(r"\s+", " ", text).strip()
        if len(text) < 15:
            return np.nan
        return text

    if mode == "address":
        text = re.sub(r"\s+", " ", text).strip()

        # Unificar pa√≠s
        text = text.replace(" spain", " espana")

        text = re.sub(r"\s+", " ", text).strip()
        return text

    return text

"""#### 1. Normalizaci√≥n de ciudad, nombre final y direcci√≥n

- **Columna `ciudad`**  
  Transformada a min√∫sculas, eliminando espacios sobrantes y corrigiendo errores comunes (ej. `"a"` ‚Üí `"a coru√±a"`).  

- **Columna `nombre_final`**  
  Normalizada para mejorar la detecci√≥n de duplicados y asegurar consistencia:  
  - Conversi√≥n a min√∫sculas.  
  - Eliminaci√≥n de acentos.  
  - Limpieza de caracteres especiales y espacios m√∫ltiples.  

- **Columna `address`**  
  Normalizada para reducir variaciones en la escritura de la misma direcci√≥n:  
  - Conversi√≥n a min√∫sculas y eliminaci√≥n de acentos.  
  - Unificaci√≥n de sufijos de pa√≠s (`"spain"` ‚Üí `"espana"`).  
  - Limpieza de espacios m√∫ltiples y caracteres sobrantes.  


"""

# -------------------------
# Normalizar ciudad

df['ciudad'] = df['ciudad'].apply(lambda x: clean_text(x, mode="city"))
df['ciudad'] = df['ciudad'].replace({'a': 'a coru√±a'})  # correcci√≥n manual

print("‚úÖ Ejemplo ciudades normalizadas:")
print(df['ciudad'].value_counts().head(10))

# Normalizar nombre_final
df['nombre_final_norm'] = df['nombre_final'].apply(lambda x: clean_text(x, mode="name"))

print("‚úÖ Ejemplo nombres normalizados:")
display(df[['nombre_final','nombre_final_norm']].sample(10))

# -------------------------
# Normalizar direcci√≥n
# -------------------------
df['address_norm'] = df['address'].apply(lambda x: clean_text(x, mode="address"))

print("‚úÖ Ejemplo direcciones normalizadas:")
display(df[['address','address_norm']].sample(10))

"""#### 2. Normalizaci√≥n de listas categ√≥ricas
Columnas: `cuisines`, `diets`, `dining_options`, `meal_types`, `top_tags`, `review_keywords`.

- Conversi√≥n de valores a min√∫sculas.  
- Eliminaci√≥n de acentos.  
- Conversi√≥n de cadenas que representan listas (ej. `"['Spanish','Galician']"`) a listas reales de Python.  
- Eliminaci√≥n de duplicados y limpieza de s√≠mbolos extra.
"""

# Normalizar listas categ√≥ricas
for col in ['cuisines','diets','dining_options','meal_types','top_tags','review_keywords']:
    if col in df.columns:
        df[col] = df[col].apply(lambda x: clean_text(x, mode="list"))
        print(f"‚úÖ Ejemplo columna {col}:")
        display(df[[col]].sample(5))

"""
#### 3. Normalizaci√≥n de la descripci√≥n
- Conversi√≥n a min√∫sculas.  
- Eliminaci√≥n de acentos, saltos de l√≠nea y caracteres especiales.  
- Si el texto es muy corto o no aporta informaci√≥n, se reemplaza por `NaN`.

"""

# Normalizar descripci√≥n
df['description_clean'] = df['description'].apply(lambda x: clean_text(x, mode="description"))

print("‚úÖ Ejemplo descripciones normalizadas:")
display(df[['description','description_clean']].dropna().sample(5))

"""### Unificaci√≥n de precios

Los precios aparec√≠an en diferentes formatos seg√∫n la fuente:
- `price_level_google`: categor√≠as de Google (ej. `PRICE_LEVEL_MODERATE`).
- `price_range_usd` y `price_range`: rangos expresados en s√≠mbolos `$`.

Para homogenizar la informaci√≥n, se aplic√≥ el siguiente proceso:
1. Conversi√≥n de s√≠mbolos `$` a valores num√©ricos (ej. `$$` ‚Üí 2).
2. Mapeo de las categor√≠as de Google a la misma escala 1‚Äì4.
3. Selecci√≥n prioritaria de la fuente m√°s informativa disponible.
4. Creaci√≥n de una columna final unificada: `price_num`.

"""

cols = ['price_level_google', 'price_range_usd', 'price_range']

for c in cols:
    if c in df.columns:
        print(f"\nüîé Valores √∫nicos en {c}:")
        print(df[c].dropna().unique())
        print(f"Total distintos: {df[c].nunique()}")
    else:
        print(f"\n‚ö†Ô∏è Columna {c} no existe en el DataFrame")

# -------------------------
# 4. Unificaci√≥n de precios
# -------------------------
def price_from_usd(x):
    if not isinstance(x, str) or not x.strip():
        return np.nan
    tokens = [t.strip() for t in x.split('-')]
    vals = [t.count('$') for t in tokens if t]
    return np.mean(vals) if vals else np.nan

MAP_GOOGLE_PRICE = {
    'PRICE_LEVEL_INEXPENSIVE': 1,
    'PRICE_LEVEL_MODERATE': 2,
    'PRICE_LEVEL_EXPENSIVE': 3,
    'PRICE_LEVEL_VERY_EXPENSIVE': 4,
    0: 1, '0': 1,
    1: 1, '1': 1,
    2: 2, '2': 2,
    3: 3, '3': 3,
    4: 4, '4': 4
}

df['price_google_num'] = df.get('price_level_google').map(MAP_GOOGLE_PRICE) if 'price_level_google' in df.columns else np.nan
df['price_usd_num']    = df.get('price_range_usd').apply(price_from_usd)    if 'price_range_usd' in df.columns else np.nan
df['price_range_num']  = df.get('price_range').apply(price_from_usd)        if 'price_range' in df.columns else np.nan

for col in ['price_google_num','price_usd_num','price_range_num']:
    df[col] = pd.to_numeric(df[col], errors='coerce')

def unify_price(row):
    for col in ['price_google_num','price_usd_num','price_range_num']:
        if pd.notna(row[col]):
            return row[col]
    return np.nan

df['price_num'] = df.apply(unify_price, axis=1)

print("‚úÖ Ejemplo precios unificados:")
display(df[['price_level_google','price_range_usd','price_range','price_num']].sample(5))

"""### Reglas de calidad y detecci√≥n de anomal√≠as

Una vez normalizadas las variables principales, se aplicaron una serie de reglas de calidad para detectar posibles inconsistencias en los datos:

- **Ratings fuera de rango**: valores menores que 0 o mayores que 5.  
- **N√∫mero de rese√±as negativas**: no deber√≠an existir.  
- **Ciudades vac√≠as**: registros sin informaci√≥n de ubicaci√≥n.  
- **Precios fuera de rango**: la variable `price_num` deb√≠a estar entre 1 y 4.
- **Coordenadas fuera de Galicia** ‚Üí detecci√≥n de puntos cuyo rango geogr√°fico cae fuera de `[41.8 ‚Äì 44.1 lat, -9.3 ‚Äì -6.5 lon]`.

Estos registros no se eliminan autom√°ticamente, pero se marcan para revisi√≥n manual o tratamiento espec√≠fico en fases posteriores.

"""

# -------------------------
# 5. Reglas de calidad y anomal√≠as
# -------------------------
# Rating fuera de rango
anomalous_ratings = df[(df['rating_final'] < 0) | (df['rating_final'] > 5)]
print(f"‚ö†Ô∏è Restaurantes con rating fuera de rango: {anomalous_ratings.shape[0]}")

# Reviews negativos
anomalous_reviews = df[df['reviews_final'] < 0]
print(f"‚ö†Ô∏è Restaurantes con n√∫mero de rese√±as negativo: {anomalous_reviews.shape[0]}")

# Ciudades vac√≠as
empty_city = df[df['ciudad'].isna()]
print(f"‚ö†Ô∏è Restaurantes sin ciudad: {empty_city.shape[0]}")

# Precios fuera de rango
anomalous_prices = df[(df['price_num'] < 1) | (df['price_num'] > 4)]
print(f"‚ö†Ô∏è Restaurantes con precio fuera de rango: {anomalous_prices.shape[0]}")

# Coordenadas fuera de Galicia (sin contar NaN)
mask_valid_coords = df['latitude'].notna() & df['longitude'].notna()

mask_outside_galicia = mask_valid_coords & ~(
    (df['latitude'].between(41.8, 44.1)) &
    (df['longitude'].between(-9.3, -6.5))
)

anomalous_coords = df[mask_outside_galicia]
print(f"‚ö†Ô∏è Restaurantes fuera de Galicia (con coords v√°lidas): {anomalous_coords.shape[0]}")

display(anomalous_coords[['nombre_final','ciudad','latitude','longitude','maps_url_final']].head(20))

""" üåç Visualizaci√≥n geogr√°fica"""

plt.figure(figsize=(8,6))
plt.scatter(df["longitude"], df["latitude"], s=10, alpha=0.5, color="teal")
plt.title("Mapa de restaurantes en Galicia")
plt.xlabel("Longitud")
plt.ylabel("Latitud")
plt.show()

"""#### Correcci√≥n manual de coordenadas

Se detectaron restaurantes con coordenadas fuera de Galicia.  
A partir de los enlaces de **Google Maps (`maps_url_final`)**, se extrajeron las posiciones correctas y se actualizaron en el DataFrame mediante el diccionario `correcciones_coords`.  

Esto garantiza que todas las ubicaciones est√©n dentro del √°rea geogr√°fica de estudio.

"""

# Diccionario con las coordenadas corregidas
correcciones_coords = {
    1814: (43.0083959, -7.5519545),   # El Sagrado Taquer√≠a (Lugo)
    1855: (42.8949786, -8.5341789),   # O Repouso (Santiago)
    1882: (42.2331587, -8.7080739),   # El Encuentro Peruano (Vigo)
    1883: (42.2194759, -8.7348255),   # La fondue mexicana florida (Vigo)
    1987: (43.3567284, -8.4033846),   # La Boutique Trattoria Viajera (A Coru√±a)
    1993: (43.3701642, -8.3956364),   # Batanero (A Coru√±a)
    2074: (42.3395801, -7.8725682),   # Arroutada (Ourense)
    2460: (43.3525273, -8.2138447),   # Parrillada O Canti√±o (Santiago)
    2976: (43.4721015, -7.3005438),   # Lourenz√° (Lugo)
    3037: (43.0170398, -7.5578652),   # Bar Trebol (Santiago)
    3048: (42.2394645, -8.7266216),   # Sopapo Taverna (Vigo)
    3051: (42.2207513, -8.7512345),   # Bar restaurante Rocio (Vigo)
    3158: (42.4336149, -8.6432796),   # Terra Restaurante Pontevedra (Pontevedra)
    3245: (42.4736878, -8.6516147),   # Bar A Ca√±ota (A Coru√±a)
}

# Aplicar correcciones al DataFrame
for idx, (lat, lon) in correcciones_coords.items():
    df.loc[df.index == idx, ['latitude', 'longitude']] = lat, lon

# Verifica que se aplicaron correctamente
print(df.loc[correcciones_coords.keys(), ['nombre_final', 'ciudad', 'latitude', 'longitude']])

!pip install contextily

import matplotlib.pyplot as plt
import geopandas as gpd
from shapely.geometry import Point
import contextily as ctx

# Filtrar filas con coordenadas v√°lidas
df_coords = df.dropna(subset=["latitude", "longitude"]).copy()

# Crear geometr√≠as solo para esas filas
geometry = [Point(xy) for xy in zip(df_coords["longitude"], df_coords["latitude"])]

# Crear GeoDataFrame
gdf = gpd.GeoDataFrame(df_coords, geometry=geometry, crs="EPSG:4326")

# Reproyectar a Web Mercator (necesario para contextily)
gdf = gdf.to_crs(epsg=3857)

# Dibujar
fig, ax = plt.subplots(figsize=(8, 8))
gdf.plot(
    ax=ax,
    markersize=15,        # un poco m√°s grandes
    alpha=0.8,
    color="red",          # puntos en rojo
    edgecolor="black"     # borde negro para que destaquen
)

# A√±adir mapa base
ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)

# Ajustar autom√°ticamente al bounding box
ax.set_xlim(gdf.total_bounds[0], gdf.total_bounds[2])
ax.set_ylim(gdf.total_bounds[1], gdf.total_bounds[3])

ax.set_title("Mapa de restaurantes en Galicia", fontsize=14)
plt.show()

"""### Visualizaciones exploratorias

Para comprender mejor la distribuci√≥n de las variables clave se generaron varias gr√°ficas exploratorias:

- **Distribuci√≥n de ratings**: histograma con densidad.  
- **Distribuci√≥n del n√∫mero de rese√±as**: muestra la concentraci√≥n en restaurantes poco rese√±ados frente a los m√°s populares.  
- **Ciudades con m√°s restaurantes**: gr√°fico de barras del top 15.  
- **Distribuci√≥n de precios**: histograma en escala 1‚Äì4.  

Estas visualizaciones permiten identificar sesgos y tendencias relevantes en el dataset.

"""

import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2 filas x 2 columnas

# -------------------------
# Distribuci√≥n de ratings finales
# -------------------------
sns.histplot(df['rating_final'], bins=20, kde=True, color="skyblue", ax=axes[0,0])
axes[0,0].set_title("Distribuci√≥n de ratings finales")
axes[0,0].set_xlabel("Rating")
axes[0,0].set_ylabel("N¬∫ restaurantes")

# -------------------------
# Distribuci√≥n de rese√±as finales
# -------------------------
axes[0,1].hist(df['reviews_final'].dropna(), bins=30, color="salmon", edgecolor="black")
axes[0,1].set_title("Distribuci√≥n de rese√±as finales")
axes[0,1].set_xlabel("N¬∫ rese√±as")
axes[0,1].set_ylabel("Frecuencia")

# -------------------------
# Top ciudades con m√°s restaurantes
# -------------------------
top_cities = df['ciudad'].value_counts().head(15)
top_cities.plot(kind="bar", ax=axes[1,0], color="seagreen", rot =45)
axes[1,0].set_title("Top ciudades con m√°s restaurantes")
axes[1,0].set_xlabel("Ciudad")
axes[1,0].set_ylabel("N¬∫ restaurantes")

# -------------------------
# Distribuci√≥n de precios unificados
# -------------------------
axes[1,1].hist(df['price_num'].dropna(), bins=4, color="orange", edgecolor="black")
axes[1,1].set_title("Distribuci√≥n de precios unificados")
axes[1,1].set_xlabel("Nivel de precio (1=barato, 4=caro)")
axes[1,1].set_ylabel("N¬∫ restaurantes")

# Ajustar layout
plt.tight_layout()
plt.show()

"""Interpretaci√≥n de los gr√°ficos

1. **Distribuci√≥n de ratings finales**  
   La mayor√≠a de los restaurantes presentan valoraciones entre **4 y 4.5 estrellas**, con pocos casos por debajo de 3.  
   Esto refleja una tendencia general a que los usuarios tienden a puntuar alto.

2. **Distribuci√≥n de rese√±as finales**  
   La gran mayor√≠a de los locales tienen **pocas rese√±as** (menos de 500).  
   Sin embargo, existen algunos outliers con **miles o incluso m√°s de 20 000 rese√±as**, que corresponden a restaurantes muy populares.

3. **Top 15 ciudades con m√°s restaurantes**  
   Las ciudades con mayor concentraci√≥n son **A Coru√±a** y **Vigo**, superando los 900 restaurantes cada una.  
   Le siguen **Santiago, Ourense, Lugo y Pontevedra**, con cifras m√°s moderadas.

4. **Distribuci√≥n de precios unificados**  
   Predominan los restaurantes de **nivel 1 (m√°s econ√≥micos)**.  
   Conforme aumenta el nivel de precio (hasta 4), el n√∫mero de locales disminuye de forma progresiva.

## Revisi√≥n de duplicados

Una vez normalizados los campos principales, se revisa la presencia de **posibles duplicados** en el dataset.  
Los duplicados pueden surgir porque la misma entidad aparezca tanto en **Google Places** como en **TripAdvisor**, o por peque√±as variaciones en los datos.  

Para la detecci√≥n se emplean dos criterios principales:

1. **Nombre final normalizado + Direcci√≥n (`address`)**  
   - Permite detectar restaurantes con el mismo nombre y direcci√≥n, aunque provengan de diferentes fuentes.  

2. **Nombre final normalizado + Coordenadas (`lat`, `lng`)**  
   - Permite identificar registros que se refieren al mismo lugar f√≠sico, incluso si la direcci√≥n presenta variaciones textuales.  

Estos dos enfoques permiten capturar la mayor√≠a de los duplicados sin eliminar falsos positivos.
"""

# -------------------------
# Revisi√≥n de duplicados
# -------------------------

# Redondear coordenadas
df["lat_round"] = df["latitude"].round(4)
df["lon_round"] = df["longitude"].round(4)

# 1. Duplicados por nombre normalizado + direcci√≥n normalizada
dupes_name_address = (
    df[df.duplicated(subset=['nombre_final_norm', 'address_norm'], keep=False)]
    .sort_values(['nombre_final_norm', 'address_norm'])
)

print("‚úÖ N¬∫ duplicados por nombre_norm + direcci√≥n_norm:", len(dupes_name_address))
display(dupes_name_address[['nombre_final_norm','address_norm','address_norm','maps_url_final']].head(20))


# 2. Duplicados por nombre normalizado + coordenadas
dupes_name_coords = (
    df[df.duplicated(subset=['nombre_final_norm', 'latitude', 'longitude'], keep=False)]
    .sort_values(['nombre_final_norm', 'latitude', 'longitude'])
)

print("‚úÖ N¬∫ duplicados por nombre_norm + coordenadas:", len(dupes_name_coords))
display(dupes_name_coords[['nombre_final_norm','latitude','longitude','maps_url_final']].head(20))

"""### Selecci√≥n del registro m√°s completo entre duplicados

Para cada grupo de registros duplicados (detectados por nombre normalizado + direcci√≥n o nombre normalizado + coordenadas), se defini√≥ una **funci√≥n de scoring** que asigna un puntaje a cada fila en base a:

- **Cantidad de valores no nulos**  
- **Calidad y volumen de ratings y rese√±as**  
  - Google (peso alto)  
  - TripAdvisor (peso menor)  
- **Campos clave presentes** (horarios, URL de Maps, direcci√≥n, rango de precios, etc.)  
- **Detalles adicionales** (descripci√≥n, tags, dietas, meal types, etc.)  

De esta forma, en cada grupo de duplicados se conserva **la fila con mayor informaci√≥n y fiabilidad**, eliminando los registros redundantes.

"""

import numpy as np
import pandas as pd

# -------------------------
# Funci√≥n de scoring
# -------------------------
def score_fila(row):
    non_nulls = row.notna().sum()
    bonus = 0

    # --- Google rating + rese√±as ---
    rating_g = row.get("rating_google", 0) or 0
    reviews_g = row.get("reviews_count_google", 0) or 0
    score_google = rating_g * np.log1p(reviews_g) * 4  # m√°s peso

    # --- Tripadvisor rating + rese√±as ---
    rating_ta = row.get("rating", 0) or 0
    reviews_ta = row.get("reviews", 0) or 0
    score_trip = rating_ta * np.log1p(reviews_ta) * 1.5  # menos peso

    # --- Bonus fuerte: Google ---
    if pd.notna(row.get("rating_google")):
        bonus += 15
    if pd.notna(row.get("reviews_count_google")) and row["reviews_count_google"] > 0:
        bonus += 10
    if row.get("open_hours") not in [None, {}, [], ""]:
        bonus += 8

    # Cubrimos ambos campos de mapas
    if any([
        pd.notna(row.get("maps_url")) and str(row["maps_url"]).strip(),
        pd.notna(row.get("google_maps_url")) and str(row["google_maps_url"]).strip()
    ]):
        bonus += 5

    # --- Bonus medio: Tripadvisor ---
    if pd.notna(row.get("address")) and str(row["address"]).strip():
        bonus += 5
    if pd.notna(row.get("price_range")) and str(row["price_range"]).strip():
        bonus += 4

    cuis = row.get("cuisines")
    if isinstance(cuis, (list, tuple, set)):
        if len(cuis) > 0:
            bonus += 4
    elif isinstance(cuis, str) and cuis.strip() not in ["", "[]"]:
        bonus += 4

    if pd.notna(row.get("ciudad")) and str(row["ciudad"]).strip():
        bonus += 3

    # --- Bonus bajo: detalles extra (TA) ---
    for campo in ["description", "top_tags", "diets", "meal_types", "dining_options", "review_keywords"]:
        val = row.get(campo)
        if val not in [None, "", [], {}]:
            bonus += 2

    return non_nulls + score_google + score_trip + bonus

"""### Eliminaci√≥n de duplicados con scoring

Para cada grupo de duplicados detectados, se calcul√≥ un **row_score** que mide la cantidad y calidad de informaci√≥n de cada fila.  

- Se conserva √∫nicamente la fila con mayor `row_score`.  
- Se eliminan las dem√°s.  
- Se reporta cu√°ntos registros se eliminaron.  
- Se muestran ejemplos concretos para verificar que la deduplicaci√≥n funciona como se espera.

"""

# -------------------------
# 1. Calcular score por fila
# -------------------------
df["row_score"] = df.apply(score_fila, axis=1)

# -------------------------
# 2. Deduplicaci√≥n jer√°rquica
# -------------------------
df_before = len(df)

# Paso 1: nombre + direcci√≥n
df_stage1 = (
    df.sort_values("row_score", ascending=False)
      .drop_duplicates(subset=["nombre_final_norm","address_norm"], keep="first")
)

# Paso 2: nombre + coordenadas redondeadas
df_dedup = (
    df_stage1.sort_values("row_score", ascending=False)
             .drop_duplicates(subset=["nombre_final_norm","lat_round","lon_round"], keep="first")
)

df_after = len(df_dedup)

print(f"‚úÖ Registros antes: {df_before}")
print(f"‚úÖ Despu√©s de deduplicar (2 pasos): {df_after}")
print(f"üóëÔ∏è Eliminados: {df_before - df_after}")

# -------------------------
# 3. Ejemplo pr√°ctico de duplicado
# -------------------------
ejemplo = df[
    df["nombre_final_norm"].str.contains("o bo xantar", case=False, na=False)
].sort_values("row_score", ascending=False)

print("üîé Ejemplo de duplicado (se conserva la fila con mayor row_score):")
display(
    ejemplo[[
        "id","nombre_final_norm","latitude","longitude","address",
        "ciudad","rating_google","reviews_count_google","price_level_google",
        "dining_options","top_tags","description","review_keywords",
        "price_range","price_range_usd","maps_url","google_maps_url",
        "open_hours_google","row_score"
    ]]
)

# -------------------------
# 4. Verificaci√≥n final
# -------------------------
check_name_address = df_dedup.duplicated(subset=['nombre_final_norm','address_norm']).sum()
check_name_coords = df_dedup.duplicated(subset=['nombre_final_norm','lat_round','lon_round']).sum()

print("üîé Verificaci√≥n final de duplicados:")
print(f"- Duplicados por nombre+direcci√≥n: {check_name_address}")
print(f"- Duplicados por nombre+coordenadas: {check_name_coords}")

if check_name_address == 0 and check_name_coords == 0:
    print("‚úÖ No quedan duplicados en el dataset.")
else:
    print("‚ö†Ô∏è Atenci√≥n: a√∫n quedan posibles duplicados.")

df.shape

"""## üìä Resumen del Cap√≠tulo 1

| Paso | N¬∫ registros | N¬∫ columnas | Descripci√≥n |
|------|--------------|-------------|-------------|
| Dataset inicial | 3.958 | 66 | Datos crudos desde TripAdvisor + Google Places |
| Eliminaci√≥n de cerrados | 3.669 | 66 | Se eliminan 289 registros cerrados permanentemente |
| Tras deduplicaci√≥n | 3.544 | 83 | Se unifican duplicados (Google + TA) conservando la fila m√°s completa |

**Variables clave creadas:**
- `nombre_final`, `nombre_final_norm` ‚Üí Identificaci√≥n limpia y homog√©nea  
- `rating_final`, `reviews_final` ‚Üí M√©tricas priorizando Google  
- `maps_url_final` ‚Üí URL confiable de localizaci√≥n  
- `horarios_norm` ‚Üí Horarios normalizados en formato est√°ndar  
- `price_num` ‚Üí Escala unificada de precios (1=barato, 4=caro)  

---

### ‚úÖ Checklist limpieza

- ‚úÖ Eliminaci√≥n de cerrados permanentemente  
- ‚úÖ Unificaci√≥n y normalizaci√≥n de horarios  
- ‚úÖ Normalizaci√≥n de nombres, direcciones y ciudades  
- ‚úÖ Normalizaci√≥n de listas categ√≥ricas y descripciones  
- ‚úÖ Unificaci√≥n de precios en escala 1‚Äì4  
- ‚úÖ Detecci√≥n y eliminaci√≥n de duplicados  
- ‚úÖ Revisi√≥n de anomal√≠as detectadas

# Cap√≠tulo 2: Traducci√≥n de variables textuales y categ√≥ricas

Para homogeneizar el dataset y evitar la mezcla de idiomas, se tradujeron al espa√±ol todas las columnas categ√≥ricas y textuales relevantes:

- **Cuisines**: traducidas con un diccionario de mapeo manual (`cuisine_dict`), ya que el vocabulario es limitado y requiere control sem√°ntico.  
- **Description, review_keywords, diets, meal_types, dining_options, top_tags**: traducidas autom√°ticamente con la librer√≠a `deep-translator`, usando el motor de Google Translate.  

De esta forma, se obtiene un dataset completamente en espa√±ol, m√°s legible y preparado para aplicar las reglas de mapeo y la creaci√≥n de flags en pasos posteriores.
"""

from collections import Counter

# ============================
# 1. Contar frecuencias iniciales
# ============================
def contar_frecuencias(df, col):
    todas = []
    df[col].dropna().apply(lambda x: todas.extend(x) if isinstance(x, list) else None)
    return Counter([str(i).lower().strip() for i in todas])

print("üçΩÔ∏è Cocinas m√°s frecuentes:")
print(contar_frecuencias(df, "cuisines"))

"""## `cuisines`

La columna `cuisines` conten√≠a etiquetas en ingl√©s con mucha variabilidad (`italian`, `pizza`, `southern-italian`, `spanish`, `galician`, etc.).  
El conteo de frecuencias muestra que predominan estilos como **espa√±ola**, **mediterr√°nea**, **europea**, **cafeter√≠a** e **italiana**.

Para estandarizarlas se aplic√≥ un **diccionario manual (`cuisine_dict`)**, que permite controlar mejor las agrupaciones y asignar traducciones consistentes en espa√±ol.  

"""

import ast

# ============================
# 1. Diccionario de traducci√≥n
# ============================
cuisine_dict = {
    # --- Principales ---
    'spanish': 'espa√±ola',
    'mediterranean': 'mediterr√°nea',
    'european': 'europea',
    'cafe': 'cafeter√≠a',
    'bar': 'bar',
    'pub': 'pub',
    'seafood': 'mariscos/pescados',
    'healthy': 'saludable',
    'steakhouse': 'parrilla/carnes',
    'fast food': 'comida r√°pida',
    'italian': 'italiana',
    'pizza': 'pizza',
    'american': 'americana',
    'fusion': 'fusi√≥n',
    'asian': 'asi√°tica',
    'contemporary': 'contempor√°nea',
    'barbecue': 'barbacoa',
    'wine bar': 'bar de vinos',
    'south american': 'sudamericana',
    'central american': 'centroamericana',
    'international': 'internacional',
    'japanese': 'japonesa',
    'diner': 'comedor',
    'mexican': 'mexicana',
    'grill': 'parrilla',
    'dining bars': 'bares/restaurantes',
    'sushi': 'sushi',
    'chinese': 'china',
    'latin': 'latina',
    'venezuelan': 'venezolana',
    'street food': 'comida callejera',
    'turkish': 'turca',
    'argentinean': 'argentina',
    'deli': 'charcuter√≠a/delicatessen',
    'brew pub': 'cervecer√≠a artesanal',
    'gastropub': 'gastrobar',
    'beer restaurants': 'cervecer√≠as',
    'buffet': 'buffet',
    'soups': 'sopas',
    'vegetarian': 'vegetariana',
    'vegan': 'vegana',

    # --- Europeas espec√≠ficas ---
    'british': 'brit√°nica',
    'french': 'francesa',
    'greek': 'griega',
    'german': 'alemana',
    'portuguese': 'portuguesa',
    'basque': 'vasca',
    'central european': 'centroeuropea',
    'eastern european': 'este europea',
    'northern-italian': 'italiana del norte',
    'southern-italian': 'italiana del sur',
    'central-italian': 'italiana del centro',
    'campania': 'campania',
    'sicilian': 'siciliana',
    'neapolitan': 'napolitana',
    'romana': 'romana',
    'tuscan': 'toscana',
    'sardinian': 'sarda',

    # --- Latinoam√©rica ---
    'brazilian': 'brasile√±a',
    'colombian': 'colombiana',
    'cuban': 'cubana',
    'peruvian': 'peruana',
    'chifa': 'chifa (peruana-china)',
    'salvadoran': 'salvadore√±a',

    # --- Asia ---
    'indian': 'india',
    'bangladeshi': 'banglades√≠',
    'thai': 'tailandesa',
    'lebanese': 'libanesa',
    'moroccan': 'marroqu√≠',
    'korean': 'coreana',
    'hawaiian': 'hawaiana',
    'polynesian': 'polinesia',
    'afghan': 'afgana',
    'armenian': 'armenia',
    'albanian': 'albanesa',
    'indonesian': 'indonesia',
    'filipino': 'filipina',
    'malaysian': 'malaya',
    'mongolian': 'mongola',
    'cantonese': 'cantonesa',
    'fujian': 'fujianesa',
    'szechuan': 'sichuanesa',
    'shanghai': 'shanghainesa',
    'taiwanese': 'taiwanesa',
    'kyoto cuisine': 'cocina de kyoto',
    'pakistani': 'paquistan√≠',
    'middle eastern': 'oriente medio',
    'cajun & creole': 'caj√∫n y criolla',
    'central asian': 'asi√°tica central',

    # --- √Åfrica ---
    'african': 'africana',
    'nigerian': 'nigeriana',

    # --- Otros
    'caribbean': 'caribe√±a',
    'native american': 'nativo americana',
    'australian': 'australiana',
    'belgian': 'belga',
    'chilean': 'chilena',
    'balti': 'balti (india-pakist√°n)',
    'lazio': 'italiana',
    'southwestern': 'americana del suroeste',
    'cambodian': 'camboyana',
    'japanese sweets parlour': 'dulces japoneses',
    'medicinal foods': 'comida medicinal',
    'arabic': '√°rabe'
}

# ============================
# 2. Convertir string -> lista
# ============================
df['cuisines'] = df['cuisines'].apply(
    lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else x
)

# ============================
# 3. Aplicar traducci√≥n con fallback
# ============================
def traducir_cuisines(lista):
    if isinstance(lista, list):
        traducidas = []
        for i in lista:
            clave = i.lower().strip()
            if clave in cuisine_dict:  # si existe traducci√≥n en el diccionario
                traducidas.append(cuisine_dict[clave])
            else:
                traducidas.append(i)  # si ya est√° en espa√±ol o no mapeado ‚Üí mantener
        return traducidas
    return lista  # si no es lista, devolver tal cual

df['cuisines_traducidas'] = df['cuisines'].apply(traducir_cuisines)


# ============================
# 4. Vista previa
# ============================
print("‚úÖ Ejemplo de traducci√≥n de cuisines:")
display(df[['cuisines','cuisines_traducidas']].sample(10))

!pip install deep-translator tqdm

"""## Otras columnas"""

# ============================
# Chequeo de tipos en una columna
# ============================
col = "review_keywords"

tipos = df[col].dropna().apply(lambda x: type(x)).value_counts()
print(f"üìä Tipos detectados en {col}:")
print(tipos)

# Ver ejemplos concretos de cada tipo
for t in tipos.index:
    print(f"\nüîé Ejemplos tipo {t}:")
    muestra = df[df[col].apply(lambda x: isinstance(x, t))][col].head(5)
    print(muestra.tolist())

import pandas as pd
import ast
from deep_translator import GoogleTranslator
from tqdm import tqdm
import time
import numpy as np

# ============================
# Traducir columnas con GoogleTranslator
# ============================
translator = GoogleTranslator(source='auto', target='es')

cols_to_translate = [
    'description', 'review_keywords', 'diets',
    'meal_types', 'dining_options', 'top_tags'
]

def translate_value(x):
    """Traduce un valor seg√∫n si es texto libre o lista."""
    # Casos nulos
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return x

    # Si es lista
    if isinstance(x, list):
        translated_items = []
        for item in x:
            if not item or str(item).strip().lower() in ["nan", "none", "null", "[]"]:
                continue
            time.sleep(0.25)  # respetar l√≠mite de la API (m√°x 4 req/s)
            translated_items.append(translator.translate(str(item)))
        return translated_items if translated_items else np.nan

    # Si es string "parece lista" ‚Üí convertir
    if isinstance(x, str) and x.strip().startswith("[") and x.strip().endswith("]"):
        try:
            items = ast.literal_eval(x)
            return translate_value(items)
        except:
            pass

    # Texto libre (ej. description)
    if isinstance(x, str):
        time.sleep(0.25)
        return translator.translate(x.strip())

    return x

# Aplicar traducci√≥n columna por columna
for col in cols_to_translate:
    new_col = col + "_es"
    print(f"üîÑ Traduciendo columna: {col} ‚Üí {new_col}")
    df[new_col] = [
        translate_value(x) for x in tqdm(df[col], desc=f"Traduciendo {col}", unit="fila")
    ]

# Vista previa
cols_es = [col + "_es" for col in cols_to_translate]
display(df[cols_to_translate + cols_es].head(10))

# Guardar resultados
output_excel = "/content/restaurantes_traducidos_google_full.xlsx"
df.to_excel(output_excel, index=False)
print(f"‚úÖ Archivo guardado en: {output_excel}")

"""Traducci√≥n autom√°tica de otras columnas

Las columnas `description`, `review_keywords`, `diets`, `meal_types`, `dining_options` y `top_tags` presentan textos libres o listas con etiquetas en ingl√©s.  

Para homogeneizarlas:  
1. Se utiliz√≥ **Google Translate** a trav√©s de la librer√≠a `deep-translator`.  
2. Cada elemento se tradujo de forma independiente con una pausa (`0.25s`) para respetar los l√≠mites de la API.  
3. El resultado se guard√≥ en nuevas columnas terminadas en `_es` (ej. `diets_es`, `top_tags_es`).  

Estas traducciones autom√°ticas facilitan el an√°lisis en espa√±ol, aunque requieren una **fase posterior de correcci√≥n manual** para limpiar errores de traducci√≥n o sin√≥nimos poco naturales.

"""

# Guardar resultados
output_excel = "/content/restaurantes_traducidos_google_full.xlsx"
df_dedup.to_excel(output_excel, index=False)
print(f"‚úÖ Archivo guardado en: {output_excel}")

"""## Revisi√≥n de resultados de la traducci√≥n

Tras aplicar la traducci√≥n autom√°tica a las columnas textuales y categ√≥ricas, es necesario revisar el contenido generado:

- **Objetivo**: detectar traducciones literales, t√©rminos poco naturales o sin√≥nimos que deber√≠an unificarse.  
- **Metodolog√≠a**: listamos las palabras √∫nicas encontradas en cada columna traducida para inspeccionarlas manualmente.  
- **Pr√≥ximo paso**: en base a esta revisi√≥n se definir√°n **diccionarios de reemplazo (`replacements_*`)** para corregir y estandarizar el vocabulario.  

"""

# 1. Cargar librer√≠as
import pandas as pd
import numpy as np
# ============================================
# 2. Cargar dataset enriquecido
# ============================================
df = pd.read_excel("/content/restaurantes_traducidos_google_full.xlsx")
print("‚úÖ Dataset cargado:", df.shape)
df.head(3)

import ast
from collections import Counter

def contar_frecuencias(df, col):
    todas = []
    for val in df[col].dropna():
        # Caso 1: ya es lista
        if isinstance(val, list):
            todas.extend(val)
        # Caso 2: string que parece lista
        elif isinstance(val, str) and val.strip().startswith("[") and val.strip().endswith("]"):
            try:
                parsed = ast.literal_eval(val)
                if isinstance(parsed, list):
                    todas.extend(parsed)
                else:
                    todas.append(parsed)
            except:
                todas.append(val)

    return Counter([str(i).lower().strip() for i in todas if i and str(i).strip()])

for col in [
    "cuisines_traducidas",
    "review_keywords_es",
    "diets_es",
    "meal_types_es",
    "dining_options_es",
    "top_tags_es"
]:
    if col in df.columns:
        print(f"\nüçΩÔ∏è Frecuencias en {col}:")
        print(contar_frecuencias(df, col))

"""### Correcci√≥n manual de traducciones

Tras revisar las frecuencias, se definieron diccionarios de reemplazo (`replacements_*`) para:

- Unificar sin√≥nimos o traducciones inconsistentes.  
- Corregir errores de traducci√≥n autom√°tica.  
- Eliminar ruido (palabras sin valor sem√°ntico, duplicadas o err√≥neas).  

Cada columna categ√≥rica se limpi√≥ aplicando estos diccionarios y se volvieron a listar las frecuencias resultantes para confirmar que las correcciones fueron efectivas.  

"""

# -----------------------------
# Diccionarios de reemplazos
# -----------------------------
replacements_diets = {
    "vegetariano amigable": "apto para vegetarianos",
    "opciones sin gluten": "sin gluten",
    "opciones veganas": "vegano"
}

replacements_meal_types = {
    "desalmuerzo": "brunch",
    "nota tarde": "comida nocturna",
    "bebidas": "copas"
}

replacements_dining_options = {
    "amigable para perros": "se admiten perros",
    "asiento al aire libre": "terraza",
    "asientos al aire libre": "terraza",
    "auto-servicio": "autoservicio",
    "barras deportivas": "bares deportivos",
    "entrega": "entrega a domicilio",
    "sacar": "para llevar",
    "solamente efectivo": "solo efectivo",
    "stillespersples disponibles": "sillas altas disponibles",
    "restaurantes no fumadores": "restaurante para no fumadores",
    "estacionamiento gratuito fuera de la calle": "parking privado gratuito",
    "estacionamiento de valet": "valet parking",
    "estacionamiento en la calle": "parking en la calle",
    "estacionamiento disponible": "parking",
    "estacionamiento validado": "parking validado",
    "pagos digitales": "pago digital",
    "tarjetas de regalo disponibles": "tarjeta regalo",
    "sirve alcohol": "alcohol",
    "vino y cerveza": "bar",
    "asientos": "mesa",
    "servicio de mesa": "mesa",
    "reservaciones": "reserva",
    "estilo familiar": "familias",
    # eliminar ruido
    "descubrir": None,
    "porci√≥n": None
}

replacements_top_tags = {
    "aparcamiento": "estacionamiento",
    "barras de comedor": "barra de bar",
    "bocados r√°pidos": "picoteo",
    "del suroeste": "cocina del suroeste",
    "europa central": "cocina de Europa Central",
    "fiambres": "embutidos",
    "fumar permitido": "se permite fumar",
    "gluten de pecado": "sin gluten",
    "lat√≠n": "cocina latina",
    "llevar": "para llevar",
    "parilla": "parrilla",
    "pub de elaboraci√≥n de cerveza": "cervecer√≠a artesanal",
    "restaurantes de cerveza": "cervecer√≠a",
    "medio oriente": "oriente medio",
    "comidas baratas": "barato",
    "de rango medio": "medio",
    # eliminar ruido
    "cepillar": None,
    "√≥rdenes": None
}

replacements_review_keywords = {
    "albarino": "albari√±o",
    "parilla": "parrilla",
    "picadura de carne": "carne picada",
    "acial": "percebes",
    "almejas de afeitar": "navajas",
    "berbama": "berberechos",
    "el di√°metro del men√∫ del men√∫": "men√∫ del d√≠a",
    "pez√≥n": "pastel",
    "estrella michel√≠n": "estrella michelin",
    "men√∫s": "men√∫",
    "men√∫ ingl√©s": "men√∫ en ingl√©s",
    "cursos": "platos",
    # eliminar ruido
    "cepillar": None,
    "mierda de manzana": None,
    "camino walk": None,
    "el autob√∫s": None,
    "red el√©ctrica": None,
    "tocar": None,
    "y": None,
    "dos": None,
    "la": None,
    "el": None
}

# -----------------------------
# Funci√≥n auxiliar
# -----------------------------
import ast

def replace_in_list(cell, replacements):
    """Reemplaza valores dentro de listas o strings de listas con normalizaci√≥n a min√∫sculas."""
    if pd.isna(cell):
        return cell
    # Convertir a lista si es string con formato de lista
    if isinstance(cell, str) and cell.strip().startswith("[") and cell.strip().endswith("]"):
        try:
            cell = ast.literal_eval(cell)
        except:
            return cell
    if not isinstance(cell, list):
        return cell

    new_items = []
    for item in cell:
        key = str(item).lower().strip()
        if key in replacements:
            if replacements[key] is None:
                continue
            else:
                new_items.append(replacements[key])
        else:
            new_items.append(item)
    return list(set(new_items)) if new_items else np.nan

# -----------------------------
# Aplicar reemplazos
# -----------------------------
df["diets_es"] = df["diets_es"].apply(lambda x: replace_in_list(x, replacements_diets))
df["meal_types_es"] = df["meal_types_es"].apply(lambda x: replace_in_list(x, replacements_meal_types))
df["dining_options_es"] = df["dining_options_es"].apply(lambda x: replace_in_list(x, replacements_dining_options))
df["top_tags_es"] = df["top_tags_es"].apply(lambda x: replace_in_list(x, replacements_top_tags))
df["review_keywords_es"] = df["review_keywords_es"].apply(lambda x: replace_in_list(x, replacements_review_keywords))

# -----------------------------
# Verificaci√≥n post-limpieza
# -----------------------------
for col in ["cuisines_traducidas","review_keywords_es","diets_es","meal_types_es","dining_options_es","top_tags_es"]:
    print(f"\n‚úÖ Frecuencias finales en {col}:")
    print(contar_frecuencias(df, col))

"""## üìä Resumen del Cap√≠tiulo 2
- Columnas traducidas: cuisines, description, review_keywords, diets, meal_types, dining_options, top_tags  
- M√©todo:
  - `cuisines` ‚Üí diccionario manual (control sem√°ntico)  
  - resto ‚Üí Google Translate v√≠a deep-translator  
- Nuevas columnas generadas: *_es  
- Diccionarios de correcci√≥n manual aplicados: replacements_*

# Cap√≠tulo 3: Enriquecimiento Categ√≥rico y Creaci√≥n de Categor√≠as Maestras
En este cap√≠tulo se lleva a cabo la **limpieza y el enriquecimiento categ√≥rico** del dataset de restaurantes.  
El proceso combina reglas autom√°ticas (detecci√≥n de platos y categor√≠as) con reglas manuales (eliminaci√≥n, renombrado y completado), con el fin de obtener etiquetas homog√©neas en espa√±ol y preparar los datos para la creaci√≥n de flags binarios en el sistema de recomendaci√≥n.  

---

### üéØ Objetivos
- Homogeneizar etiquetas categ√≥ricas en espa√±ol.  
- Detectar autom√°ticamente **platos caracter√≠sticos** en nombres, descripciones y keywords.  
- Aplicar reglas manuales de **eliminaci√≥n, renombrado y completado**.  
- Crear nuevas columnas enriquecidas (`platos_detectados`, `categorias_asignadas`).  
- Dejar el dataset listo para consultas y an√°lisis posteriores.  

---

### üõ†Ô∏è Metodolog√≠a

1. **Detecci√≥n de platos** ‚Üí construcci√≥n de un diccionario `MAP_PLATOS` y aplicaci√≥n de la funci√≥n `detectar_platos` sobre nombre, descripci√≥n y keywords para generar la columna `platos_detectados`.  

2. **Identificaci√≥n de filas vac√≠as** ‚Üí revisi√≥n de las columnas `top_tags_es`, `cuisines_traducidas`, `review_keywords_es` y `platos_detectados` para detectar registros sin informaci√≥n √∫til, que se exportan a Excel para control manual.  

3. **Reglas de enriquecimiento manual** ‚Üí definici√≥n de tres bloques:  
   - **Eliminados**: registros que no corresponden a restaurantes.  
   - **Renombrados**: ajuste de nombres para evitar duplicados o inconsistencias.  
   - **Completados**: adici√≥n manual de cocinas, platos o etiquetas en casos incompletos.  

4. **Normalizaci√≥n de nombres** ‚Üí aplicaci√≥n de funciones auxiliares para unificar acentos, may√∫sculas y formatos en `nombre_final_norm`, garantizando que las reglas de completado se apliquen correctamente.  

5. **Aplicaci√≥n de reglas sobre el dataset** ‚Üí ejecuci√≥n de los pasos de eliminaci√≥n, renombrado y completado, actualizando la informaci√≥n categ√≥rica de cada restaurante.  

6. **Asignaci√≥n de categor√≠as maestras** ‚Üí definici√≥n de un diccionario jer√°rquico `categories` con bloques tem√°ticos (Pizza, Hamburguesas, Asi√°tica, Espa√±ola, Veggie, Dulces, etc.), compilaci√≥n de expresiones regulares y creaci√≥n de la columna `categorias_asignadas`.  

7. **Revisi√≥n final** ‚Üí detecci√≥n de registros que permanecen sin categor√≠as asignadas, eliminaci√≥n manual de los irrelevantes y exportaci√≥n del dataset limpio.  

8. **Exportaci√≥n de resultados** ‚Üí guardado del dataset final en **Excel** (`restaurantes_final.xlsx`) y **JSON** (`restaurantes_final.json`).

### Funci√≥n de detecci√≥n de platos

La funci√≥n `detectar_platos` recorre varias columnas textuales (`nombre_final_norm`, `description_es`,
`review_keywords_es`, etc.), unifica todo en un texto en min√∫sculas y busca coincidencias con el
diccionario `MAP_PLATOS`.  

El resultado es una lista de categor√≠as detectadas, que se guarda en una nueva columna `platos_detectados`.
"""

MAP_PLATOS = {
    # === üçï Italianos ===
    "pizza": "pizzas",
    "pizzeria": "pizzas",
    "pizzer√≠a": "pizzas",
    "pizza movil": "pizzas",
    "pizzeria carlos": "pizzas",
    "grosso napoletano": "pizzas",
    "isola bella": "pizzas",
    "cosa nostra": "pizzas",
    "pomodoro": "pizzas",
    "napolitana": "italiana",
    "napoletano": "italiana",
    "tagliatella": "italiana",
    "roma": "italiana",
    "sicilia": "italiana",
    "siciliano": "italiana",
    "nonna": "italiana",
    "trattoria": "italiana",
    "ristorante": "italiana",
    "ginos": "italiana",
    "pasta": "pasta",
    "spaghetti": "pasta",
    "lasa√±a": "pasta",
    "cannoli": "italiana",
    "arancini": "italiana",

    # === üçî Hamburguesas / fast food ===
    "hamburguesa": "hamburguesas",
    "hamburguesas": "hamburguesas",
    "hamburgueseria": "hamburguesas",
    "hamburgueser√≠a": "hamburguesas",
    "burger": "hamburguesas",
    "burguer": "hamburguesas",
    "smashburger": "hamburguesas",
    "mcdonald": "hamburguesas",
    "burger king": "hamburguesas",
    "vips": "hamburguesas",
    "foster": "hamburguesas",
    "tgb": "hamburguesas",
    "goiko": "hamburguesas",
    "pepita burger bar": "hamburguesas",
    "capital burger": "hamburguesas",
    "devoto burger": "hamburguesas",
    "mister burguer": "hamburguesas",
    "leyenda burger": "hamburguesas",
    "baruta burger": "hamburguesas",

    # === üåÆ Mexicana ===
    "taco": "tacos",
    "tacos": "tacos",
    "taqueria": "tacos",
    "taquer√≠a": "tacos",
    "nachos": "nachos",
    "burrito": "burritos",
    "burritos": "burritos",
    "tex mex": "mexicana",
    "mexican": "mexicana",
    "cantina mexicana": "mexicana",
    "cantina": "mexicana",
    "la catrina": "mexicana",
    "sol maya": "mexicana",
    "cactus mexican": "mexicana",
    "quesadilla": "mexicana",
    "chilaquiles": "mexicana",
    "margarita": "mexicana",

    # === ü•¢ Asi√°tica ===
    "sushi": "sushi",
    "sibuya": "sushi",
    "totoro sushi": "sushi",
    "wantai sushi": "sushi",
    "sakura sushi": "sushi",
    "kisoro sushi": "sushi",
    "umi sushi": "sushi",
    "takimi sushi": "sushi",
    "sushi fafa": "sushi",
    "poke": "poke",
    "koa poke": "poke",
    "mahalo poke": "poke",
    "ramen": "ramen",
    "buga ramen": "ramen",
    "udon": "noodles",
    "noodles": "noodles",
    "wok": "wok",
    "izakaya": "japonesa",
    "oriental": "asi√°tica",
    "asian": "asi√°tica",
    "thai": "tailandesa",
    "tokyo": "japonesa",
    "rokuseki": "japonesa",
    "yakitori": "japonesa",
    "china": "china",
    "chino": "china",
    "cantonesa": "china",
    "shangai": "china",
    "kebab": "kebab",
    "doner": "kebab",
    "shawarma": "kebab",
    "turco": "kebab",

    # === ü•© Carnes / parrilla ===
    "parrillada": "carnes",
    "parrilla": "carnes",
    "asador": "carnes",
    "brasa": "carnes",
    "brasaylena": "carnes",
    "rodizio": "carnes",
    "rodizio": "carnes",
    "churrasco": "carnes",
    "chulet√≥n": "carnes",
    "steak": "carnes",
    "t-bone": "carnes",
    "costilla": "carnes",
    "cordero": "carnes",
    "pollo": "pollo",
    "pollo crujiente": "pollo frito",
    "pollo frito": "pollo frito",
    "kfc": "pollo frito",
    "popeyes": "pollo frito",

    # === üêü Mariscos / pescado ===
    "marisqueria": "mariscos/pescados",
    "marisquer√≠a": "mariscos/pescados",
    "pulperia": "mariscos/pescados",
    "pulpeira": "mariscos/pescados",
    "ostras": "mariscos/pescados",
    "mejillones": "mariscos/pescados",
    "vieiras": "mariscos/pescados",
    "navajas": "mariscos/pescados",
    "bogavante": "mariscos/pescados",
    "bacalao": "mariscos/pescados",
    "caldeirada": "mariscos/pescados",
    "arroz con bogavante": "mariscos/pescados",

    # === üç≥ Tapas / tradicionales ===
    "tapas": "tapas",
    "taperia": "tapas",
    "taper√≠a": "tapas",
    "taberna": "tapas",
    "tasca": "tapas",
    "meson": "tapas",
    "bodegon": "tapas",
    "furancho": "tapas",
    "callos": "tapas",
    "croquetas": "tapas",
    "raxo": "tapas",
    "tortilla": "tapas",

    # === ü•ü Empanadas / gallega ===
    "empanada": "empanadas",
    "empanadas": "empanadas",
    "casa de las empanadas": "empanadas",

    # === ü•ñ Pan / dulces ===
    "panaderia": "pan/boller√≠a",
    "bolleria": "pan/boller√≠a",
    "granier": "pan/boller√≠a",
    "pasteleria": "dulces",
    "confiteria": "dulces",
    "reposteria": "dulces",
    "churreria": "dulces",
    "heladeria": "helados",
    "helado": "helados",
    "creperia": "crepes",
    "crepe": "crepes",
    "gofre": "gofres",
    "yogurteria": "yogures",
    "haagen dazs": "helados",
    "bico de xeado": "helados",

    # === ‚òï Caf√©s / bares ===
    "cafe": "caf√©",
    "cafeteria": "caf√©",
    "bar": "bar",
    "pub": "pub",
    "cerveceria": "cerveza",
    "estrella galicia": "cerveza",
    "brew": "cerveza",
    "teteria": "t√©",

    # === üç∑ Bebidas ===
    "vinoteca": "vino",
    "bodega": "vino",
    "sidreria": "sidra",
    "cocteleria": "c√≥cteles",
    "cocktail": "c√≥cteles",
    "vermouth": "vermut",
    "verm√∫": "vermut",
    "mojito": "c√≥cteles",
    "gin tonic": "c√≥cteles",
    "margarita": "c√≥cteles",
    "orujito": "orujo",

    # === üõí Supermercados / tiendas ===
    "eroski": "supermercado",
    "froiz": "supermercado",
    "dia": "supermercado",
    "mercadona": "supermercado",
    "carrefour": "supermercado",
    "lidl": "supermercado",
    "cash": "supermercado",
    "hiper": "supermercado",

    # === üå± Veggie / healthy ===
    "vegano": "vegano",
    "vegetariano": "vegetariano",
    "brunch": "brunch",
    "ecol√≥gico": "ecol√≥gico",
    "org√°nico": "ecol√≥gico",
    "bio": "ecol√≥gico",

    # === Otros generales ===
    "bocateria": "bocatas/s√°ndwiches",
    "bocata": "bocatas/s√°ndwiches",
    "s√°ndwich": "bocatas/s√°ndwiches",
    "montaditos": "bocatas/s√°ndwiches",
    "pan": "pan/boller√≠a",
    "mercado": "comida variada",
    "gastrobar": "moderno",
    "buffet": "buffet",
}

import re
import numpy as np

# -------------------------
# Funci√≥n de detecci√≥n
# -------------------------
def detectar_platos(row, map_dict):
    encontrados = set()

    # Unir todas las columnas relevantes en un √∫nico texto
    campos = [
        row.get("nombre_final_norm", ""),
        row.get("description_es", ""),
        row.get("review_keywords_es", ""),
        row.get("top_tags_es", ""),
        row.get("meal_types_es", ""),
        row.get("diets_es", ""),
        row.get("cuisines_traducidas", "")
    ]

    # Convertir listas a strings, concatenar y pasar a min√∫sculas
    texto = " ".join(
        " ".join(val) if isinstance(val, list) else str(val)
        for val in campos if val is not None
    ).lower()

    # Buscar coincidencias
    for clave, categoria in map_dict.items():
        if re.search(rf"\b{re.escape(clave)}\b", texto):
            encontrados.add(categoria)

    return sorted(list(encontrados)) if encontrados else np.nan


# -------------------------
# Crear columna platos_detectados
# -------------------------
df["platos_detectados"] = df.apply(lambda row: detectar_platos(row, MAP_PLATOS), axis=1)

# -------------------------
# Ejemplo de revisi√≥n
# -------------------------
ejemplo = df[[
    "nombre_final_norm","description_es",
    "review_keywords_es","top_tags_es","meal_types_es","diets_es","cuisines_traducidas",
    "platos_detectados"
]].sample(10, random_state=40)

display(ejemplo)

"""### Detecci√≥n de filas con informaci√≥n vac√≠a

Para identificar restaurantes sin informaci√≥n relevante, se revisan las columnas:
- `top_tags_es`  
- `cuisines_traducidas`  
- `review_keywords_es`  
- `platos_detectados`  

Si todas est√°n vac√≠as, el registro se considera pendiente de enriquecimiento.

"""

# -------------------------------
# Detectar filas con todas las columnas vac√≠as
# -------------------------------
import numpy as np
import pandas as pd

# Columnas a revisar
cols_revisar = ["top_tags_es", "cuisines_traducidas", "review_keywords_es", "platos_detectados"]

# Funci√≥n auxiliar: considerar vac√≠o si es NaN, lista vac√≠a, ndarray vac√≠o o string "[]"
def is_empty(val):
    # Nulos expl√≠citos
    if val is None:
        return True

    # Arrays numpy
    if isinstance(val, np.ndarray):
        return val.size == 0  # vac√≠o si no tiene elementos

    # Listas
    if isinstance(val, list):
        return len(val) == 0

    # Strings
    if isinstance(val, str):
        val_str = val.strip().lower()
        if val_str in ("[]", "", "nan", "none", "null"):
            return True
        return False

    # Num√©ricos tipo NaN
    if isinstance(val, (float, int)) and pd.isna(val):
        return True

    return False

# Crear m√°scara: True solo si TODAS las columnas est√°n vac√≠as
mask_all_empty = df[cols_revisar].apply(lambda col: col.map(is_empty)).all(axis=1)

# Filtrar esas filas
df_vacios = df.loc[mask_all_empty].copy()

# Guardar a Excel
output_path = "/content/restaurantes_campos_vacios.xlsx"
df_vacios.to_excel(output_path, index=False, engine="openpyxl")

print(f"‚úÖ Archivo generado: {output_path}")
print(f"üîé Total de filas con las 4 columnas vac√≠as: {df_vacios.shape[0]}")

"""### Reglas de enriquecimiento manual

Se definen tres bloques de reglas:  
- **Eliminados**: registros que no son restaurantes o que deben descartarse.  
- **Renombrados**: ajustes de nombres para unificarlos.  
- **Completados**: enriquecimiento manual de informaci√≥n faltante (cuisines, platos, tags, keywords).

"""

# Enriquecimiento de Restaurantes
# Este notebook carga el Excel base, aplica reglas de limpieza (eliminados, renombrados, completados) y exporta los resultados finales.

import pandas as pd
import json

# === Diccionario de reglas ===

# Restaurantes a eliminar
eliminados = [
    # --- Bloque 1 ---
    "distintos",
    "O Faneco Casco Vello",
    "Oselle",
    "CLUB DE GOLF LUGO",
    "Verxel Herbolario",
    "Woocost",
    "Flor de Toxo",
    "A Pipa Da L√∫a -actividades",
    "Dayu's",
    "Garigolo",
    "Box 46",
    "Sayanes Mar",
    "Brindis",
    "Le Coc√≥",
    "David Rodriguez Prieto",
    "Amirea",
    "Flor de Lis",
    "El Buen Gusto",
    "El Dulce Vigo",
    "Point",
    "La Pomada",
    "Sabores",
    "Meraki",
    "La Maleta",
    "Dragonenos",
    "Estanco Dos Vi√±os",
    "Partido Socialista Obrero Espa√±ol",  # no es restaurante
    "Regalos Miramar",
    "Lolita¬¥s",
    "Dor√©",
    "Ultramarinos Jesusa",
    "Mar√≠a Gloria Hern√°ndez L√≥pez",
    "Coraz√≥n de Carballo",
    "Alc√°zar",
    "Baiona",
    "Marypaz",
    "Estaleiros e Varadoiros San Gregorio Asvagre SL",
    "Land√≠n Inform√°tica",
    "Basic",
    "Domus",
    "A L√≥pez Alvarez",
    "Koala",
    "Lobeira",
    "Milano",
    "Servytem Global",
    "D'Marino",
    "Cui√±a",
    "Lateral",
    "Peneda",
    "Bartapat√©",
    "Cafetar√≠a Don Bal√≥n",
    "Caf√©s Campinas S. Paulo- Empresas Reunidas S.A.",
    "Jamones G√≥mez",
    "La Barraca de Travesia de Vigo",
    "O Barqueiro",
    "Pastelerias",
    "Pescader√≠a Os Pepes"
]




# Restaurantes a renombrar
renombrados = {
    "Restaurante Ramblade": "Restaurante Las Barricas",
    "Anghara": "Taberna A Pedra"
    # --- Bloque 1 ---
    # (no hubo renombrados en este bloque)
}

renombrados.update({
    "Alambique A Coru√±a": "Alameda",
    "O¬¥raxo": "Cervecer√≠a O' Barullo",
    "Grietax": "A Lagareta",
    "Rico Rico -p√°gina": "Mes√≥n O Rico",
    "Mallos 28": "Restaurante La Mas√≠a",
    "Coru√±a": "Esencia Restaurante Coru√±a",
    "Ansu": "Cervecer√≠a Ansu 2",
    "La Reserva": "La Churre",
    "Camino de Santiago": "Taberna de √Ålvaro",
    "Caf√©s de la √ìpera": "Siboney ~ Caf√©s de Especialidad"
})

renombrados.update({
    "Mediterranean": "Mediterranean Buffet",
    "La Ibense": "Taberna O Enxebre",
    "Foro Liceo": "√Ågape Restaurante",
    "Peregrinus": "Pulper√≠a y Churrasquer√≠a Peregrinus",
    "El Pasillo": "Bar El Pasillo"
})

renombrados.update({
    "La Reserva": "La Churre"
})


# Restaurantes completados
completados = {}

# --- Bloque 1 ---
completados.update({
    "Atlastapas_vigo": {
        "cuisines": ["tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante A Chispa": {
        "cuisines": ["carnes", "tapas", "espa√±ola", "gallega"],
        "platos_detectados": ["carne", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Parador la Rocha": {
        "cuisines": ["cafeter√≠a", "boller√≠a", "desayunos"],
        "platos_detectados": ["caf√©", "cerveza", "boller√≠a"],
        "top_tags": ["take away"],
        "review_keywords": []
    },
    "Prebe": {
        "cuisines": ["carnes", "arroces", "pescados", "mariscos", "vinos"],
        "platos_detectados": ["carne", "arroz", "pescado", "marisco", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "Danubio Degustaci√≥n": {
        "cuisines": ["tapas", "bocadillos", "hamburguesas"],
        "platos_detectados": ["tapas", "bocadillos", "hamburguesas"],
        "top_tags": ["cerrado temporalmente"],
        "review_keywords": []
    },
    "Restaurante Andi√≥n": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["caf√©", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Parada 33": {
        "cuisines": ["caf√©-bar", "tapas"],
        "platos_detectados": ["caf√©", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Casa Pedro": {
        "cuisines": ["tradicional", "espa√±ola", "gallega"],
        "platos_detectados": ["carne", "pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante la Perdiz": {
        "cuisines": ["tapas", "carnes", "pescados", "pulpo"],
        "platos_detectados": ["tapas", "carne", "pescado", "pulpo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pollo Crujiente El Corral": {
        "cuisines": ["comida r√°pida", "carnes", "bocatas", "hamburguesas"],
        "platos_detectados": ["pollo frito", "hamburguesa", "bocadillo"],
        "top_tags": ["cerveza"],
        "review_keywords": []
    },
    "Trivial": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Eduri": {
        "cuisines": ["caf√©-bar", "tapas", "men√∫ del d√≠a"],
        "platos_detectados": ["caf√©", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ous√° Montero Rios": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["pan", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Como En Casa, Comida Para Llevar": {
        "cuisines": ["para llevar", "espa√±ola", "gallega"],
        "platos_detectados": ["platos caseros", "empanada"],
        "top_tags": [],
        "review_keywords": []
    },
    "Alaska": {
        "cuisines": ["bar", "tapas", "bocadillos"],
        "platos_detectados": ["tapas", "bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Templo De Baco": {
        "cuisines": ["cervecer√≠a", "tapas"],
        "platos_detectados": ["tapas", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Malyan": {
        "cuisines": ["cervecer√≠a", "caf√©s", "tapas"],
        "platos_detectados": ["cerveza", "caf√©", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Rincon Enrique": {
        "cuisines": ["pizzer√≠a", "burgers", "bocatas", "tapas"],
        "platos_detectados": ["pizza", "hamburguesa", "bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Do√±a Bella Lugo": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bosforo Kebab Lugo": {
        "cuisines": ["kebab"],
        "platos_detectados": ["kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cuarenta Y Cuatro": {
        "cuisines": ["cervecer√≠a"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Morrigan Lugo": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Paseo Fonti√±as": {
        "cuisines": ["espa√±ola", "gallega"],
        "platos_detectados": ["pulpo", "carne", "pescado"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "La Postrer√≠a De Noe": {
        "cuisines": ["panader√≠a", "boller√≠a", "caf√©s"],
        "platos_detectados": ["pan", "boller√≠a", "caf√©"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "Carnicer√≠as Pesuma": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Kfc": {
        "cuisines": ["comida r√°pida", "hamburguesas", "pollo frito"],
        "platos_detectados": ["pollo frito", "hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pazo de Altamira Hotel Restaurant": {
        "cuisines": ["caf√©", "mariscos", "pescados", "carnes"],
        "platos_detectados": ["marisco", "pescado", "carne", "caf√©"],
        "top_tags": ["lujoso"],
        "review_keywords": []
    },
    "Las Colonias": {
        "cuisines": ["confiter√≠a"],
        "platos_detectados": ["dulces", "pasteles"],
        "top_tags": [],
        "review_keywords": []
    },
    "Con Culler": {
        "cuisines": ["pescados", "cerveza", "alta cocina"],
        "platos_detectados": ["pescado", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Monte do Gozo": {
        "cuisines": ["cervecer√≠a", "tapas", "bocadillos"],
        "platos_detectados": ["cerveza", "tapas", "bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Beach Solpor": {
        "cuisines": ["cafeter√≠a", "restaurante", "bocadillos", "platos combinados", "hamburguesas"],
        "platos_detectados": ["bocadillo", "plato combinado", "hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Opera Prima Santiago Compostela": {
        "cuisines": ["italiano"],
        "platos_detectados": ["pizza", "pasta"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Santa Isabel": {
        "cuisines": ["restaurante-cafeter√≠a", "men√∫ del d√≠a"],
        "platos_detectados": ["platos caseros"],
        "top_tags": ["barato"],
        "review_keywords": []
    },
    "La Pausa": {
        "cuisines": ["cafeter√≠a", "desayunos", "bocadillos", "espa√±ola", "gallega"],
        "platos_detectados": ["bocadillo", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Rinc√≥n siciliano": {
        "cuisines": ["pizzer√≠a"],
        "platos_detectados": ["pizza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Plaza De Abastos": {
        "cuisines": ["mercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "KFC": {
        "cuisines": ["comida r√°pida", "hamburguesas", "pollo frito"],
        "platos_detectados": ["pollo frito", "hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cocina Maria": {
        "cuisines": ["para llevar", "espa√±ola", "gallega", "variada"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Chida": {
        "cuisines": ["mexicana", "tacos", "cocktails"],
        "platos_detectados": ["tacos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Entre Fogones": {
        "cuisines": ["para llevar", "espa√±ola", "gallega", "mariscos", "carnes"],
        "platos_detectados": ["marisco", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Tr√©bol": {
        "cuisines": ["marisquer√≠a"],
        "platos_detectados": ["marisco", "pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "Elaine Tapas": {
        "cuisines": ["tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    }
})


# --- Nuevos completados ---
completados.update({
    "Adega Da Ancora": {
        "cuisines": ["tienda de bebidas alcoh√≥licas", "bar", "cocina casera"],
        "platos_detectados": ["cerveza", "pulpo", "pinchos", "carne", "vino", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Amigo Restobar": {
        "cuisines": ["peruano"],
        "platos_detectados": ["ceviche"],
        "top_tags": [],
        "review_keywords": []
    },
    "Barrio Do Cura": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["dulces", "croissant", "bollos", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bico De Grao Tenda Ecol√≥xica": {
        "cuisines": ["tienda de alimentos frescos"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Siboney ~ Caf√©s de Especialidad": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©s", "desayunos", "churros", "tortitas", "tostadas", "men√∫s", "tortilla", "tapas", "batidos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Carnes Parente": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Charcuteria La Trufa": {
        "cuisines": ["charcuter√≠a"],
        "platos_detectados": ["embutidos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Churrasquer√≠a San Amaro": {
        "cuisines": ["restaurante", "carnes", "brasa"],
        "platos_detectados": ["churrasco", "ternera", "calamares"],
        "top_tags": [],
        "review_keywords": []
    },
    "Clunia Panem": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["pan", "vegano", "sin gluten", "empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Coren Grill": {
        "cuisines": ["para llevar", "platos preparados"],
        "platos_detectados": ["cerdo", "croquetas", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Creperie La Terraza": {
        "cuisines": ["cafeter√≠a", "creper√≠a"],
        "platos_detectados": ["crepes", "caf√©", "s√°ndwich", "platos combinados", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Delijam√≥n": {
        "cuisines": ["tienda de delicatessen"],
        "platos_detectados": ["jam√≥n"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Cafet√≠n de Santiago (Santiago de Compostela)": {
        "cuisines": ["cafeter√≠a", "restaurante"],
        "platos_detectados": ["desayunos", "tostadas", "zumo", "tortilla", "men√∫ del d√≠a", "churros"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Gastro de Casona de Labrada, Aldea Rural Sostenible": {
        "cuisines": ["restaurante"],
        "platos_detectados": ["tataki", "croquetas", "cecina", "pescados", "mariscos", "carnes"],
        "top_tags": [],
        "review_keywords": []
    },
    "Gofres De Principe": {
        "cuisines": ["cafeter√≠a", "helader√≠a"],
        "platos_detectados": ["gofres", "helados"],
        "top_tags": [],
        "review_keywords": []
    },
    "Hipopotamus Gruponovastabernas": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos", "tapas", "desayuno"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Jamoneria El Pinar -Cuatro Caminos": {
        "cuisines": ["jamoner√≠a"],
        "platos_detectados": ["bocadillos", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    }
})

# --- Nuevos completados ---
completados.update({
    "Jamoneria Sabor Ib√©rico": {
        "cuisines": ["jamoner√≠a", "cafeter√≠a"],
        "platos_detectados": ["jam√≥n", "bocadillos", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Jam√≥n Jam√≥n Coru√±a": {
        "cuisines": ["jamoner√≠a"],
        "platos_detectados": ["jam√≥n", "quesos", "bocadillos", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Quadra Urban Food Court Pontevedra": {
        "cuisines": ["restaurante", "mexicana", "hamburguesas"],
        "platos_detectados": ["hamburguesas", "tacos", "nachos", "burrito", "costillas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Vienesa": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mipasteleria": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["dulces", "empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mi√±a Nai By Dosraices": {
        "cuisines": ["restaurante"],
        "platos_detectados": ["arroces", "croquetas", "marisco", "pescado", "pulpo", "pica√±a"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Sacho Pulpo Y Jamon": {
        "cuisines": ["restaurante"],
        "platos_detectados": ["pulpo", "tortilla", "pimientos de padr√≥n"],
        "top_tags": [],
        "review_keywords": []
    },
    "Panadar√≠a Pastelar√≠a Acu√±a": {
        "cuisines": ["cafeter√≠a", "panader√≠a"],
        "platos_detectados": ["pan", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Panadar√≠a S√°nchez Garc√≠a": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["dulces", "pan", "empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Andi√≥n": {
        "cuisines": ["restaurante", "paellas", "platos combinados", "tapas", "hamburguesas", "bocadillos"],
        "platos_detectados": ["paellas", "platos combinados", "men√∫s", "tapas", "hamburguesas", "bocadillos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Tr√©bol": {
        "cuisines": ["marisquer√≠a"],
        "platos_detectados": ["marisco", "chulet√≥n", "paella", "pimientos de padr√≥n", "zamburi√±as", "pulpo", "zorza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ribarteme": {
        "cuisines": ["restaurante gallego"],
        "platos_detectados": ["callos", "garbanzos", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tahonas Deb√©n": {
        "cuisines": ["cafeter√≠a", "panader√≠a"],
        "platos_detectados": ["caf√©", "boller√≠a", "empanadas", "pan"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Telepizza Ourense": {
        "cuisines": ["pizzer√≠a"],
        "platos_detectados": ["pizza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tosal Caf√©s Y Ca√±as": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tuypan Sl: Despacho F√°tima": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["bocadillos", "desayunos", "dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "Yant√©n Vegan Food": {
        "cuisines": ["vegano"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    }
})

completados.update({
    "Corner Hut": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Mui√±o Do Vento Alcabre": {
        "cuisines": ["tapas", "pulpo", "gallega", "carnes", "pescados"],
        "platos_detectados": ["pulpo", "carne", "pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "La via 109": {
        "cuisines": ["cafeter√≠a", "men√∫ del d√≠a"],
        "platos_detectados": ["men√∫ del d√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Kaba√±a": {
        "cuisines": ["bocater√≠a", "hamburguesas"],
        "platos_detectados": ["hamburguesa", "bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sacacorchos": {
        "cuisines": ["tapas", "bocadillos"],
        "platos_detectados": ["tapas", "calamares", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Casa Secunda": {
        "cuisines": ["parrilla", "brasa", "pescados", "carnes", "marisco"],
        "platos_detectados": ["carne", "pescado", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante El Cort√©s": {
        "cuisines": ["platos combinados", "pizzas", "hamburguesas", "men√∫ del d√≠a", "pulpo", "espa√±ola", "gallega"],
        "platos_detectados": ["pizza", "hamburguesa", "pulpo"],
        "top_tags": [],
        "review_keywords": []
    },
    "100 Montaditos": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["montaditos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bierhaus": {
        "cuisines": ["churrer√≠a"],
        "platos_detectados": ["churros", "chocolate"],
        "top_tags": [],
        "review_keywords": []
    },
    "Vigo Churros": {
        "cuisines": ["bar de tapas"],
        "platos_detectados": ["churros", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cen Gaiteiros": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Viptoria": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Pola": {
        "cuisines": ["comida casera"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Puentes": {
        "cuisines": ["hamburgueser√≠a"],
        "platos_detectados": ["hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bemposta": {
        "cuisines": ["cafeter√≠a", "men√∫s", "pinchos", "tapas", "bocadillos", "caf√©"],
        "platos_detectados": ["bocadillo", "pincho", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Reca Hollywood": {
        "cuisines": ["pub", "cerveza"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Perra Chica": {
        "cuisines": ["tapas", "bar"],
        "platos_detectados": ["vino", "cerveza", "men√∫ del d√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Camucha": {
        "cuisines": ["alta cocina", "pescados", "mariscos", "carnes"],
        "platos_detectados": ["empanada", "pescado", "marisco", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Roucos": {
        "cuisines": ["comida casera", "espa√±ola", "gallega"],
        "platos_detectados": ["postres", "platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Maispala": {
        "cuisines": ["bar musical", "cerveza"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pacha": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Queser√≠a De Vigo": {
        "cuisines": ["cerveza", "tostas", "queso"],
        "platos_detectados": ["queso", "tosta"],
        "top_tags": [],
        "review_keywords": []
    },
    "Froiz": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Sol Maya Galicia 2019 SL": {
        "cuisines": ["mexicana"],
        "platos_detectados": ["nachos", "tacos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Fruter√≠a Manzano": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Mafia se sienta a la mesa": {
        "cuisines": ["italiano"],
        "platos_detectados": ["pizza", "pasta"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Juakina": {
        "cuisines": ["bar", "cerveza", "pinchos", "vermut", "f√∫tbol"],
        "platos_detectados": ["pincho", "vermut"],
        "top_tags": [],
        "review_keywords": []
    },
    "O'mouri√±o": {
        "cuisines": ["cafeter√≠a", "tortilla", "espa√±ola", "gallega"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Vernal": {
        "cuisines": ["chino"],
        "platos_detectados": ["arroz frito", "pollo agridulce"],
        "top_tags": [],
        "review_keywords": []
    },
    "Porto Santo - Cuba": {
        "cuisines": ["hamburguesas", "tostas", "embutido", "tapas"],
        "platos_detectados": ["hamburguesa", "tosta", "embutido"],
        "top_tags": [],
        "review_keywords": []
    },
    "Chuli√±a Enxebre": {
        "cuisines": ["tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Colegiala": {
        "cuisines": ["bar de tapas", "tostas"],
        "platos_detectados": ["tosta", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Eroski Center": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Lisa Bruch": {
        "cuisines": ["brunch", "desayunos", "tostadas", "hamburguesas"],
        "platos_detectados": ["tostada", "hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "V Dieciocho": {
        "cuisines": ["cafeter√≠a", "pinchos"],
        "platos_detectados": ["pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "Supan": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Kassmir": {
        "cuisines": ["bar", "cafeter√≠a"],
        "platos_detectados": ["caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Laut√©ntica": {
        "cuisines": ["caf√©", "t√©", "zumos", "boller√≠a"],
        "platos_detectados": ["caf√©", "t√©", "zumo", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Frutas Seijo": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Despensa De Clara": {
        "cuisines": ["tienda de comida"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "La Pinta": {
        "cuisines": ["bocater√≠a"],
        "platos_detectados": ["bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Fussion Caribe√±o": {
        "cuisines": ["caribe√±a"],
        "platos_detectados": ["arepas", "platos caribe√±os"],
        "top_tags": [],
        "review_keywords": []
    },
    "Herborister√≠a Ra√≠ces": {
        "cuisines": ["herborister√≠a"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "La Esquina": {
        "cuisines": ["bar", "tabaco", "tapas"],
        "platos_detectados": ["tapas", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Montechico": {
        "cuisines": ["tienda delicatessen"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Santiveri Vigo": {
        "cuisines": ["tienda alimentaci√≥n", "cosm√©ticos"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Kebab Los Amigos": {
        "cuisines": ["kebab"],
        "platos_detectados": ["kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cbweed Shop - Vigo": {
        "cuisines": ["tienda cannabis"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "S√©mola": {
        "cuisines": ["panader√≠a", "pasteler√≠a"],
        "platos_detectados": ["pan", "dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Esquinita Vigo": {
        "cuisines": ["men√∫ del d√≠a", "pizza", "tapas", "hamburguesas"],
        "platos_detectados": ["pizza", "hamburguesa", "croquetas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tah√∫r": {
        "cuisines": ["bar", "terraza"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Caprichosa": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "Carnatur": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lareca": {
        "cuisines": ["bocater√≠a", "tapas"],
        "platos_detectados": ["bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Confieteria Martina": {
        "cuisines": ["confiter√≠a"],
        "platos_detectados": ["dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "Nova Macumba": {
        "cuisines": ["cafeter√≠a", "desayunos", "zumos", "tostadas", "vermut"],
        "platos_detectados": ["zumo", "tostada", "vermut"],
        "top_tags": [],
        "review_keywords": []
    },
    "C'alma Fruit": {
        "cuisines": ["tienda alimentos naturales"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "A Lideira": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["sin gluten"],
        "review_keywords": []
    },
    "Oolong&green": {
        "cuisines": ["tienda de t√©"],
        "platos_detectados": ["t√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "The City Vigo 2.0": {
        "cuisines": ["bar", "cervezas"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "JERUSALEM SHAWARMA & BISTRO": {
        "cuisines": ["shawarma", "kebab"],
        "platos_detectados": ["shawarma", "kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "Iogur!": {
        "cuisines": ["helader√≠a"],
        "platos_detectados": ["helado"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Tante": {
        "cuisines": ["espa√±ola", "gallega", "carnes", "mariscos"],
        "platos_detectados": ["entrecot", "zamburi√±as"],
        "top_tags": [],
        "review_keywords": []
    },
    "Grifone": {
        "cuisines": ["cervecer√≠a", "caf√©"],
        "platos_detectados": ["cerveza", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mezzo Napoletano": {
        "cuisines": ["italiano"],
        "platos_detectados": ["pizza", "pasta"],
        "top_tags": [],
        "review_keywords": []
    },
    "Panadel√≠a": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["pan"],
        "top_tags": [],
        "review_keywords": []
    },
    "Poller√≠a Sita": {
        "cuisines": ["poller√≠a", "carnicer√≠a"],
        "platos_detectados": ["pollo", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Frutas J.d Araujo": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Crazymeals": {
        "cuisines": ["para llevar", "empanadillas", "arepas", "pizzas", "ensaladas", "croquetas", "bocadillos", "bagels", "postres"],
        "platos_detectados": ["empanadillas", "arepas", "pizza", "ensalada", "tortilla", "croquetas", "bocadillo", "bagel", "postre"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mi Capricho": {
        "cuisines": ["cafeter√≠a", "taper√≠a"],
        "platos_detectados": ["empanadillas", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Fervella": {
        "cuisines": ["tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Airinos": {
        "cuisines": ["parrillada", "carnes"],
        "platos_detectados": ["churrasco", "ternera", "croca", "chipirones"],
        "top_tags": ["parking"],
        "review_keywords": []
    },
    "Chan da vella": {
        "cuisines": ["tapas", "espa√±ola", "gallega"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Gumer - Restaurante Pontevedra": {
        "cuisines": ["internacional", "alta cocina"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Pintxo Vi√±o": {
        "cuisines": ["tapas", "tostas"],
        "platos_detectados": ["tosta", "sartenes"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lua Chea Pontevedra": {
        "cuisines": ["cachopo", "croquetas", "jam√≥n"],
        "platos_detectados": ["cachopo", "croquetas", "jam√≥n"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ye Olde Basset": {
        "cuisines": ["cervecer√≠a"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Momu Facker": {
        "cuisines": ["hamburgueser√≠a"],
        "platos_detectados": ["hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Hiper Froiz Pontevedra": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "O Rueiro do Pereiro": {
        "cuisines": ["parrilla", "mariscos", "pescados", "carnes", "cocido", "tapas"],
        "platos_detectados": ["marisco", "pescado", "carne", "cocido"],
        "top_tags": [],
        "review_keywords": []
    },
    "Marna": {
        "cuisines": ["mariscos", "pescados", "internacional"],
        "platos_detectados": ["marisco", "pescado", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tienda de Clara": {
        "cuisines": ["bar", "cervecer√≠a"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Bodeguita": {
        "cuisines": ["bar de tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Fruter√≠a Sonia": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Scandale": {
        "cuisines": ["cerveza", "vinos"],
        "platos_detectados": ["cerveza", "vino"],
        "top_tags": [],
        "review_keywords": []
    }
})

completados.update({
    "Alimentaci√≥n Isabel": {
        "cuisines": ["tienda alimentaci√≥n natural"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "La Nata": {
        "cuisines": ["chocolater√≠a", "gofres", "churros", "batidos"],
        "platos_detectados": ["gofres", "churros", "batidos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Food N Bike": {
        "cuisines": ["comida a domicilio"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Glot√≥n": {
        "cuisines": ["para llevar", "bocadillos", "tapas", "lasagna", "vegano"],
        "platos_detectados": ["bocadillo", "tapas", "lasa√±a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Kfc": {
        "cuisines": ["comida r√°pida", "pollo frito"],
        "platos_detectados": ["pollo frito", "hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "American Tropical Burgu√©s": {
        "cuisines": ["hamburgueser√≠a"],
        "platos_detectados": ["hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "Central Terraza": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Andaluza Low Cost": {
        "cuisines": ["tapas", "bocadillos", "men√∫", "hamburguesas"],
        "platos_detectados": ["hamburguesa", "bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa do Americano": {
        "cuisines": ["furancho"],
        "platos_detectados": ["vino", "empanada"],
        "top_tags": [],
        "review_keywords": []
    },
    "Arriba la cuchara": {
        "cuisines": ["hamburgueser√≠a vegana", "crepes", "desayunos"],
        "platos_detectados": ["hamburguesa vegana", "crepe"],
        "top_tags": [],
        "review_keywords": []
    },
    "Galiur": {
        "cuisines": ["pizzas", "tostas", "sartenes", "pescados", "parrilla", "tapas"],
        "platos_detectados": ["pizza", "tosta", "revuelto"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ama Italiana Contemporanea": {
        "cuisines": ["italiano", "risotto", "tartar", "raviolis"],
        "platos_detectados": ["risotto", "tartar", "raviolis"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sion De Valais": {
        "cuisines": ["tapas", "gallego", "comida casera"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Fillo Do Medio": {
        "cuisines": ["pizzas", "cachopo", "tortilla"],
        "platos_detectados": ["pizza", "cachopo", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Santa Comba": {
        "cuisines": ["parrilla", "carnes", "pulpo", "mariscos"],
        "platos_detectados": ["carne", "pulpo", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Don Pollo": {
        "cuisines": ["pollo asado", "comida r√°pida"],
        "platos_detectados": ["pollo asado"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Nueva Estacion": {
        "cuisines": ["espa√±ola", "gallega", "carnes", "mariscos", "pescados"],
        "platos_detectados": ["carne", "pescado", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cast√∏": {
        "cuisines": ["bar", "pinchos", "tapas", "carnes", "mariscos"],
        "platos_detectados": ["carne", "marisco", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Kebab": {
        "cuisines": ["kebab"],
        "platos_detectados": ["kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "it's Green": {
        "cuisines": ["desayunos", "caf√©s", "tostadas", "bowls"],
        "platos_detectados": ["tostada", "bowl"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mafary": {
        "cuisines": ["bar", "pinchos", "bocadillos"],
        "platos_detectados": ["bocadillo", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Invernadero": {
        "cuisines": ["bar", "tapas", "caf√©s"],
        "platos_detectados": ["tapas", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Esquini√±a": {
        "cuisines": ["bar", "caf√©s", "tapas", "tortilla"],
        "platos_detectados": ["tortilla", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Botter": {
        "cuisines": ["cafeter√≠a", "bocadillos"],
        "platos_detectados": ["bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Pardo": {
        "cuisines": ["cervecer√≠a"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["estrella galicia"],
        "review_keywords": []
    },
    "La Campana": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Candil": {
        "cuisines": ["tapas", "hamburguesas", "postres"],
        "platos_detectados": ["tortilla", "hamburguesa", "postres caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Pedra Furada Coru√±a": {
        "cuisines": ["men√∫ degustaci√≥n", "alta cocina", "internacional"],
        "platos_detectados": ["carnes", "pescados", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "S11": {
        "cuisines": ["bar", "platos combinados", "hamburguesas", "tapas"],
        "platos_detectados": ["hamburguesa", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Fontenova": {
        "cuisines": ["espa√±ola", "gallega", "mariscos", "carnes", "pescados"],
        "platos_detectados": ["carne", "pescado", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Solpor": {
        "cuisines": ["vinos", "internacional", "carnes", "pescados"],
        "platos_detectados": ["vino", "carne", "pescado", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "KFC": {
        "cuisines": ["comida r√°pida", "pollo frito"],
        "platos_detectados": ["pollo frito"],
        "top_tags": [],
        "review_keywords": []
    },
    "Hotel Congreso": {
        "cuisines": ["espa√±ola", "gallega", "carnes", "pescados"],
        "platos_detectados": ["carne", "pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "Dksa": {
        "cuisines": ["tapas", "carnes", "vegetariano"],
        "platos_detectados": ["tapas", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pink's!!": {
        "cuisines": ["hamburgueser√≠a"],
        "platos_detectados": ["hamburguesa"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Cabo": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Patio": {
        "cuisines": ["bar de tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "100 Montaditos": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["montaditos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Le Cucu Petit Bistrot": {
        "cuisines": ["espa√±ola", "gallega", "internacional", "carnes", "pescados"],
        "platos_detectados": ["carne", "pescado", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "Don Pastel": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["pasteles"],
        "top_tags": [],
        "review_keywords": []
    },
    "Chafl√°n": {
        "cuisines": ["bar", "pinchos", "tortilla", "empanada"],
        "platos_detectados": ["tortilla", "empanada"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Cebreiro Calidade": {
        "cuisines": ["tienda alimentaci√≥n natural"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Portela": {
        "cuisines": ["gallego", "marisco", "pulpo", "pescado"],
        "platos_detectados": ["pulpo", "marisco", "pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "Eroski Center": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "O'norte": {
        "cuisines": ["bar", "restaurante", "tapas", "tortilla"],
        "platos_detectados": ["tortilla", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Las 2 Terrazas": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Lar De Mar√≠a": {
        "cuisines": ["bar", "tapas", "cerveza"],
        "platos_detectados": ["cerveza", "tortilla"],
        "top_tags": ["f√∫tbol"],
        "review_keywords": []
    },
    "Bocateria La Tradicional": {
        "cuisines": ["bocater√≠a"],
        "platos_detectados": ["bocadillo"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Mariola": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Martin": {
        "cuisines": ["tapas", "tortilla", "cerveza"],
        "platos_detectados": ["tortilla", "cerveza", "postres"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "O' Laxe": {
        "cuisines": ["bar restaurante", "tortilla", "tapas"],
        "platos_detectados": ["tortilla", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sahil Kebab Rua Sol Orzan": {
        "cuisines": ["kebab"],
        "platos_detectados": ["kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Cerralbo La Coru√±a": {
        "cuisines": ["tienda alimentaci√≥n", "embutidos"],
        "platos_detectados": ["embutido"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Sella": {
        "cuisines": ["tapas", "empanadas", "desayunos"],
        "platos_detectados": ["tapas", "empanada", "desayuno"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "Horno Sanbrandan": {
        "cuisines": ["panader√≠a", "desayunos", "empanadas"],
        "platos_detectados": ["pan", "empanada"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Mundo Do T√©": {
        "cuisines": ["tienda de t√©"],
        "platos_detectados": ["t√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Monty Ambig√∫": {
        "cuisines": ["cafeter√≠a", "pinchos", "desayunos"],
        "platos_detectados": ["caf√©", "pincho"],
        "top_tags": ["pet friendly"],
        "review_keywords": []
    },
    "Five": {
        "cuisines": ["cafeter√≠a", "desayunos", "boller√≠a"],
        "platos_detectados": ["caf√©", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Franxa": {
        "cuisines": ["cervecer√≠a"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sabaneque 3": {
        "cuisines": ["cocteler√≠a"],
        "platos_detectados": ["c√≥ctel"],
        "top_tags": [],
        "review_keywords": []
    },
    "Serramo": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Fanega E Cuartillos": {
        "cuisines": ["tienda alimentaci√≥n"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Asa De Cantaro": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["callos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Teagarden": {
        "cuisines": ["tienda de t√©"],
        "platos_detectados": ["t√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Deb√©n": {
        "cuisines": ["panader√≠a", "desayunos"],
        "platos_detectados": ["pan", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "T59": {
        "cuisines": ["cafeter√≠a", "tapas", "tortilla"],
        "platos_detectados": ["caf√©", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ca√±ahueca": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Abrego": {
        "cuisines": ["buffet"],
        "platos_detectados": ["platos variados"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Cova C√©ltica": {
        "cuisines": ["cervecer√≠a artesanal"],
        "platos_detectados": ["cerveza artesanal"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Cortijo - Orz√°n": {
        "cuisines": ["bar"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cazuza": {
        "cuisines": ["bar", "conciertos"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["dardos", "futbol√≠n"],
        "review_keywords": []
    },
    "Ocho_montealto": {
        "cuisines": ["caf√© bar", "hamburguesas", "s√°ndwiches", "tapas"],
        "platos_detectados": ["hamburguesa", "s√°ndwich", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Conde Alimentaci√≥n": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Tracio": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Echoqueai": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Herborister√≠a Paracelso": {
        "cuisines": ["herborister√≠a"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Tenda Ecol√≥xica Equilibrio": {
        "cuisines": ["tienda ecol√≥gica"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Blanroch": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["pasteles"],
        "top_tags": [],
        "review_keywords": []
    },
    "Benmesabes": {
        "cuisines": ["para llevar", "espa√±ola", "gallega"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Carrusco": {
        "cuisines": ["tapas", "comida casera"],
        "platos_detectados": ["tapas", "platos caseros"],
        "top_tags": ["men√∫s", "vino"],
        "review_keywords": []
    },
    "La Botella Azul": {
        "cuisines": ["cervecer√≠a", "tapas"],
        "platos_detectados": ["cerveza", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mamahizopan": {
        "cuisines": ["churrer√≠a", "tapas"],
        "platos_detectados": ["churros", "tapas"],
        "top_tags": ["comida variada"],
        "review_keywords": []
    },
    "Fuente √Ålamo Ii": {
        "cuisines": ["pizzer√≠a"],
        "platos_detectados": ["pizza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bocaos": {
        "cuisines": ["bar restaurante", "bocadillos", "tapas"],
        "platos_detectados": ["bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Popeyes Famous Louisiana Chicken": {
        "cuisines": ["comida r√°pida", "pollo frito"],
        "platos_detectados": ["pollo frito"],
        "top_tags": [],
        "review_keywords": []
    },
    "Todobio SL.": {
        "cuisines": ["vegetariano"],
        "platos_detectados": ["platos caseros"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    }
})

completados.update({
    "Gomimola Ourense": {
        "cuisines": ["tienda golosinas"],
        "platos_detectados": ["golosinas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Iogurlader√≠a Praza Maior": {
        "cuisines": ["cafeter√≠a", "yogurter√≠a", "helados"],
        "platos_detectados": ["yogur", "helado"],
        "top_tags": [],
        "review_keywords": []
    },
    "Novelty": {
        "cuisines": ["cafeter√≠a", "desayunos", "boller√≠a"],
        "platos_detectados": ["caf√©", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Kebab Ayman": {
        "cuisines": ["kebab"],
        "platos_detectados": ["kebab"],
        "top_tags": [],
        "review_keywords": []
    },
    "Luscofusco Ourense": {
        "cuisines": ["bar", "bocadillos", "cerveza"],
        "platos_detectados": ["bocadillo", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Os Caneyes": {
        "cuisines": ["bar de tapas", "tostas", "mariscos", "tortilla"],
        "platos_detectados": ["tosta", "tortilla", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Saborido": {
        "cuisines": ["bar de tapas", "pulpo", "carnes"],
        "platos_detectados": ["pulpo", "vino", "carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Confianza": {
        "cuisines": ["cafeter√≠a", "tapas", "cerveza"],
        "platos_detectados": ["tapas", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Paqui&palla": {
        "cuisines": ["cafeter√≠a", "tapas", "hamburguesas", "bocadillos"],
        "platos_detectados": ["hamburguesa", "bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Peto Maxico": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "O Piorno": {
        "cuisines": ["taberna", "men√∫ barato", "comida casera"],
        "platos_detectados": ["tapas", "platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lamizan": {
        "cuisines": ["kebab", "s√°ndwiches", "carnes"],
        "platos_detectados": ["kebab", "s√°ndwich"],
        "top_tags": ["men√∫s"],
        "review_keywords": []
    },
    "Escher Ourense": {
        "cuisines": ["cafeter√≠a", "billar", "pinchos", "helados", "batidos"],
        "platos_detectados": ["pincho", "helado", "batido"],
        "top_tags": [],
        "review_keywords": []
    },
    "O CABACEIRO": {
        "cuisines": ["tienda alimentos naturales"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Mundial": {
        "cuisines": ["internacional"],
        "platos_detectados": ["platos internacionales"],
        "top_tags": [],
        "review_keywords": []
    },
    "Queen": {
        "cuisines": ["hamburgueser√≠a"],
        "platos_detectados": ["hamburguesa", "patatas bravas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Eroski": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "C√°rnicas Milucho E Hijos Sl": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Alexandre Restaurante": {
        "cuisines": ["cafeter√≠a", "restaurante"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Gastroterra": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Vogue Ourense": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Rustic": {
        "cuisines": ["bar", "pinchos", "tapas"],
        "platos_detectados": ["pincho", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sarri": {
        "cuisines": ["bar", "comida casera"],
        "platos_detectados": ["platos caseros"],
        "top_tags": ["terraza", "ni√±os"],
        "review_keywords": []
    },
    "O Puxi√±o": {
        "cuisines": ["cafeter√≠a", "pinchos", "tapas", "vino"],
        "platos_detectados": ["pincho", "vino", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Dezanove": {
        "cuisines": ["cafeter√≠a", "caf√©", "tortilla", "men√∫ del d√≠a"],
        "platos_detectados": ["caf√©", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cash Record Lugo": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Chilandradas": {
        "cuisines": ["tienda alimentaci√≥n"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Lucus Carniceros": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante San Martin Pinario": {
        "cuisines": ["men√∫ del d√≠a", "cocina casera"],
        "platos_detectados": ["vino", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pan da Moa": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["pan"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lacarpanta SCQ": {
        "cuisines": ["restaurante"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Curruncho dos Titos": {
        "cuisines": ["bar", "bocadillos", "tapas"],
        "platos_detectados": ["bocadillo", "tapas"],
        "top_tags": ["f√∫tbol"],
        "review_keywords": []
    },
    "A Patela": {
        "cuisines": ["cafeter√≠a", "pinchos"],
        "platos_detectados": ["pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Molino": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "La mani√®re": {
        "cuisines": ["cafeter√≠a", "brunch", "smoothies", "vegano"],
        "platos_detectados": ["brunch", "smoothie"],
        "top_tags": [],
        "review_keywords": []
    },
    "Reca Rosal√≠a": {
        "cuisines": ["cafeter√≠a", "pinchos", "hamburguesas", "tortilla", "tapas"],
        "platos_detectados": ["hamburguesa", "bocadillo", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Buena Vida": {
        "cuisines": ["bar"],
        "platos_detectados": ["croquetas", "empanadillas"],
        "top_tags": [],
        "review_keywords": []
    },
    "20th Century Rock": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Havana 20": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Brillante": {
        "cuisines": ["cafeter√≠a", "men√∫ del d√≠a", "tortilla", "tapas"],
        "platos_detectados": ["tortilla", "jam√≥n asado", "croquetas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "O Almac√©n": {
        "cuisines": ["tienda vinos"],
        "platos_detectados": ["vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Sorte Da Meiga": {
        "cuisines": ["cafeter√≠a", "panader√≠a", "boller√≠a", "tapas"],
        "platos_detectados": ["pan", "boller√≠a", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "D'luna Avellaneda": {
        "cuisines": ["cafeter√≠a", "pulpo", "tapas"],
        "platos_detectados": ["pulpo", "tapas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Arco Da Vella": {
        "cuisines": ["bar", "cerveza"],
        "platos_detectados": ["cerveza", "tortilla"],
        "top_tags": ["ambiente Erasmus"],
        "review_keywords": []
    },
    "La Radio": {
        "cuisines": ["bar"],
        "platos_detectados": ["copas"],
        "top_tags": ["m√∫sica"],
        "review_keywords": []
    },
    "O Recuncho": {
        "cuisines": ["bar"],
        "platos_detectados": ["tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lembranza": {
        "cuisines": ["cocina casera"],
        "platos_detectados": ["pulpo", "tortilla", "pimientos de padr√≥n"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Praderita": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["bocadillo", "empanadillas"],
        "top_tags": ["futbol√≠n", "dardos", "terraza"],
        "review_keywords": []
    },
    "CUEVAS CASH": {
        "cuisines": ["supermercado"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "El Trotes": {
        "cuisines": ["bar", "pinchos", "embutido"],
        "platos_detectados": ["embutido", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "The Majestic": {
        "cuisines": ["cocteler√≠a"],
        "platos_detectados": ["c√≥ctel"],
        "top_tags": [],
        "review_keywords": []
    },
    "S√©mola": {
        "cuisines": ["panader√≠a", "pasteler√≠a"],
        "platos_detectados": ["pan", "dulces"],
        "top_tags": [],
        "review_keywords": []
    },
    "Don Gregorio": {
        "cuisines": ["cafeter√≠a", "bocadillos", "cerveza"],
        "platos_detectados": ["bocadillo", "caf√©", "churros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Las ramblas": {
        "cuisines": ["cafeter√≠a", "bocadillos", "pinchos"],
        "platos_detectados": ["bocadillo", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "Jukebox Vigo": {
        "cuisines": ["bar musical"],
        "platos_detectados": ["copas"],
        "top_tags": ["a√±os 80"],
        "review_keywords": []
    },
    "O Ovo": {
        "cuisines": ["bar"],
        "platos_detectados": ["chupito", "huevo cocido"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Ser√°n": {
        "cuisines": ["bar musical", "tapas", "cocteler√≠a"],
        "platos_detectados": ["tapas", "c√≥cteles"],
        "top_tags": ["opciones veganas"],
        "review_keywords": []
    },
    "Carpe Diem": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Chabely": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Movenpick": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["jam√≥n asado"],
        "top_tags": ["futbol√≠n", "billar", "dardos"],
        "review_keywords": []
    },
    "Boomerang": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["bocadillo", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Manon": {
        "cuisines": ["bar"],
        "platos_detectados": ["vino", "boller√≠a"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "Santa Claus": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["vino", "tortilla"],
        "top_tags": ["futbol√≠n", "billar"],
        "review_keywords": []
    },
    "Restaurante Aplomo": {
        "cuisines": ["arroces", "wok", "pulpo", "carrilleras"],
        "platos_detectados": ["arroz", "pulpo", "carrilleras"],
        "top_tags": ["men√∫ degustaci√≥n"],
        "review_keywords": []
    },
    "El huerto del cura": {
        "cuisines": ["bar", "brunch"],
        "platos_detectados": ["tortitas", "tostas", "bowls"],
        "top_tags": [],
        "review_keywords": []
    },
    "Don Sancho": {
        "cuisines": ["cafeter√≠a", "comida casera"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bachata": {
        "cuisines": ["cafeter√≠a", "cerveza", "bocatas", "hamburguesas"],
        "platos_detectados": ["bocadillo", "hamburguesa"],
        "top_tags": ["copas"],
        "review_keywords": []
    },
    "Sopa De Gallo": {
        "cuisines": ["cocteler√≠a"],
        "platos_detectados": ["c√≥ctel"],
        "top_tags": [],
        "review_keywords": []
    },
    "THE GROOVE HOUSE": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["futbol√≠n"],
        "review_keywords": []
    },
    "Pedra Furada": {
        "cuisines": ["men√∫ degustaci√≥n", "alta cocina"],
        "platos_detectados": ["carnes", "pescados"],
        "top_tags": [],
        "review_keywords": []
    },
    "Begamar": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tortilla", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Peton 3": {
        "cuisines": ["cafeter√≠a", "men√∫ del d√≠a"],
        "platos_detectados": ["lasa√±a", "postres", "churros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Orballo": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["dardos"],
        "review_keywords": []
    },
    "El Columpio": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pizza", "raciones"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "La Bodeguita del Coraz√≥n Chiflado": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Bajo de Amalia": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas", "tortilla"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "D'Jaime": {
        "cuisines": ["cafeter√≠a", "men√∫ del d√≠a"],
        "platos_detectados": ["bocadillo", "tapas"],
        "top_tags": ["pinchos"],
        "review_keywords": []
    },
    "La Meca": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tortilla", "bocata"],
        "top_tags": ["vino"],
        "review_keywords": []
    },
    "Friends": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["hamburguesa", "lasa√±a", "croquetas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Revent√≥n": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": ["familiar"],
        "review_keywords": []
    },
    "Campos": {
        "cuisines": ["carnicer√≠a"],
        "platos_detectados": ["carne"],
        "top_tags": [],
        "review_keywords": []
    },
    "Boulevard": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": ["billar", "dardos", "futbol√≠n", "terraza"],
        "review_keywords": []
    },
    "O Grelo": {
        "cuisines": ["bar", "hamburguesas", "bocadillos", "tapas"],
        "platos_detectados": ["hamburguesa", "bocadillo", "tapas"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "River": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tortilla", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "Faragullas": {
        "cuisines": ["panader√≠a", "boller√≠a"],
        "platos_detectados": ["pan", "boller√≠a"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "Teide": {
        "cuisines": ["bar", "comida casera"],
        "platos_detectados": ["platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Consuelo": {
        "cuisines": ["bar"],
        "platos_detectados": ["tostadas", "revuelto", "langostinos"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Recuncho do Granel": {
        "cuisines": ["tienda alimentaci√≥n natural"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Sector 7": {
        "cuisines": ["hamburguesas", "bar"],
        "platos_detectados": ["hamburguesa"],
        "top_tags": ["juegos", "ordenadores"],
        "review_keywords": []
    }
})

completados.update({
    "Hornos de Lamastelle": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["bizcochos", "empanadas", "boller√≠a", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Arte Cachito": {
        "cuisines": ["panader√≠a", "cafeter√≠a"],
        "platos_detectados": ["cachitos", "caf√©"],
        "top_tags": ["venezolana"],
        "review_keywords": []
    },
    "Aqu√≠ Mismo": {
        "cuisines": ["bar"],
        "platos_detectados": ["mojitos", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Travertino": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©", "infusiones"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Olivo": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas", "desayunos"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "DeRocha": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pesqueras Sotavento": {
        "cuisines": ["pescader√≠a"],
        "platos_detectados": ["pescado"],
        "top_tags": [],
        "review_keywords": []
    },
    "O'p√≠o": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Los Artesanos del Teque√±o": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["teque√±os"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lyss": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pinchos", "tapas", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "Kristel": {
        "cuisines": ["bar", "cafeter√≠a"],
        "platos_detectados": ["caf√©", "tortilla", "desayunos"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Cribo": {
        "cuisines": ["bar restaurante"],
        "platos_detectados": ["tortilla", "tapas", "churrasco", "callos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Forno Do Cumial": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["magdalenas", "bica", "empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Britania": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["empanadillas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "O Fillo do C√°ndido": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["men√∫ del d√≠a", "desayunos", "tapas"],
        "top_tags": ["parque infantil"],
        "review_keywords": []
    },
    "Arroutada": {
        "cuisines": ["restaurante", "para llevar"],
        "platos_detectados": ["hamburguesas", "gyozas", "alb√≥ndigas", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Tunel": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "CADOMOKI": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["hamburguesas", "chocolate", "tortilla"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Osi Gourmet": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["empanadas", "palmeritas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Horizonte": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Diverbolos": {
        "cuisines": ["ocio"],
        "platos_detectados": ["pizzas"],
        "top_tags": ["bolera", "karaoke", "billar", "futbol√≠n"],
        "review_keywords": []
    },
    "Fruter√≠a Cuatro Estaciones": {
        "cuisines": ["fruter√≠a"],
        "platos_detectados": ["frutas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Gale√≥n": {
        "cuisines": ["bar"],
        "platos_detectados": ["platos combinados", "cachopo"],
        "top_tags": ["centro comercial"],
        "review_keywords": []
    },
    "Restaurante Pizzbur": {
        "cuisines": ["pizzer√≠a", "hamburguesas", "pasta"],
        "platos_detectados": ["pizza", "hamburguesa", "pasta"],
        "top_tags": ["platos combinados"],
        "review_keywords": []
    },
    "Mar√≠a Teresa Prada Arias": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "A Volti√±a. Restaurante": {
        "cuisines": ["men√∫ del d√≠a"],
        "platos_detectados": ["tortilla", "caldo gallego", "merluza", "callos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Estudio 34 (Santiago de Compostela)": {
        "cuisines": ["bar"],
        "platos_detectados": ["tapas", "bocadillos"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Acendra Restauraci√≥n": {
        "cuisines": ["mariscos", "carnes"],
        "platos_detectados": ["cachopo", "tartar", "croquetas", "lubina", "postres"],
        "top_tags": ["apto cel√≠acos"],
        "review_keywords": []
    },
    "Junquito": {
        "cuisines": ["cafeter√≠a", "bocater√≠a"],
        "platos_detectados": ["hamburguesas", "tapas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "El Mono Vintage": {
        "cuisines": ["bar"],
        "platos_detectados": ["zumos"],
        "top_tags": ["bingo"],
        "review_keywords": []
    },
    "Le Monde": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pinchos", "tapas"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "O Papo Cheo": {
        "cuisines": ["bar de tapas"],
        "platos_detectados": ["ensaladas", "woks", "bocatas", "pulpo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Maui original": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos"],
        "top_tags": ["juegos de mesa", "terraza"],
        "review_keywords": []
    },
    "O Triskel": {
        "cuisines": ["arroces", "mariscos", "pescados"],
        "platos_detectados": ["paella", "fideu√°", "pulpo", "chipirones"],
        "top_tags": [],
        "review_keywords": []
    },
    "Bonilla": {
        "cuisines": ["churrer√≠a"],
        "platos_detectados": ["churros", "chocolate"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tienda 24 Horas": {
        "cuisines": ["tienda alimentaci√≥n"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Arroyo mar y tierra": {
        "cuisines": ["mariscos", "arroces"],
        "platos_detectados": ["pulpo", "croquetas", "vieiras", "arroz con marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Dolce Differenza": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["boller√≠a", "bocadillos"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "Amigui√±os Si": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["bocatas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Victoria": {
        "cuisines": ["men√∫ del d√≠a"],
        "platos_detectados": ["carnes", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "Zona Zero": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["bocatas", "tapas", "hamburguesas", "churros"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Pallota": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["croquetas", "bocadillos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Caff√® Moreno": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Paris": {
        "cuisines": ["men√∫ del d√≠a"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Australian": {
        "cuisines": ["restaurante"],
        "platos_detectados": ["tapas", "chipirones", "bogavante"],
        "top_tags": [],
        "review_keywords": []
    },
    "Piccadilly": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Miramar de Corme": {
        "cuisines": ["gallego", "arroces", "mariscos"],
        "platos_detectados": ["percebes", "centolla", "arroz"],
        "top_tags": [],
        "review_keywords": []
    },
    "RedRum Coru√±a": {
        "cuisines": ["cocteler√≠a"],
        "platos_detectados": ["c√≥ctel"],
        "top_tags": [],
        "review_keywords": []
    },
    "Hostal Boiro Miramar": {
        "cuisines": ["restaurante"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Degustaci√≥n": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["desayunos", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Casa Surrey": {
        "cuisines": ["bar de tapas"],
        "platos_detectados": ["calamares", "zorza", "raxo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Am√©rica": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tapas", "vermut", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "7„ÉªBOCADOS CASEROS„Éª7": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Confeitar√≠a Bambi": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["boller√≠a"],
        "top_tags": ["desayunos"],
        "review_keywords": []
    },
    "Terra Ib√©rica": {
        "cuisines": ["tienda alimentos naturales"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Bico": {
        "cuisines": ["cocteler√≠a"],
        "platos_detectados": ["desayunos", "tostadas"],
        "top_tags": ["vegano", "vegetariano"],
        "review_keywords": []
    },
    "El Toro": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Baul": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tapas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "La Pasarela": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pinchos", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "√Ånimas": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Zafire": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["pinchos", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Reloj": {
        "cuisines": ["caf√©", "cocteler√≠a"],
        "platos_detectados": ["caf√©", "c√≥ctel"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ch√≠f√°n": {
        "cuisines": ["peruano", "chifa"],
        "platos_detectados": ["tallarines", "want√°n", "pollo chijaukai"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sheraton": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Marifran": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cool": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tapas", "croissant", "s√°ndwich"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "Copet√≠n": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["cocido gallego"],
        "top_tags": [],
        "review_keywords": []
    },
    "Saraiba": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "California (A Coru√±a)": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tortilla", "hamburguesas"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "Eclipse": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "HOTEL NOIA": {
        "cuisines": ["restaurante"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Perla Negra": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["hamburguesas", "ensaladas", "tostas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cristean": {
        "cuisines": ["pasteler√≠a"],
        "platos_detectados": ["boller√≠a", "empanadas", "tarta milhojas"],
        "top_tags": [],
        "review_keywords": []
    },
    "C√≠cero": {
        "cuisines": ["internacional"],
        "platos_detectados": ["brunch", "hamburguesas", "hot dog"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "Ateneo": {
        "cuisines": ["bar"],
        "platos_detectados": ["copas"],
        "top_tags": ["m√∫sica", "mon√≥logos", "ajedrez", "trivial"],
        "review_keywords": []
    },
    "La Granja": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": ["futbol√≠n", "dardos"],
        "review_keywords": []
    },
    "Sanin": {
        "cuisines": ["bar"],
        "platos_detectados": ["tapas", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mokambo": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pasteles"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Tio Juan": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Rio de Janeiro": {
        "cuisines": ["bar restaurante", "latino"],
        "platos_detectados": ["platos bolivianos", "platos brasile√±os"],
        "top_tags": [],
        "review_keywords": []
    },
    "INOX": {
        "cuisines": ["bar"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Canela": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Copacabana": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["c√≥cteles", "helados", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Coco": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["alb√≥ndigas"],
        "top_tags": ["men√∫"],
        "review_keywords": []
    },
    "O Fidalgo": {
        "cuisines": ["bar"],
        "platos_detectados": ["pinchos", "picapollo"],
        "top_tags": [],
        "review_keywords": []
    },
    "As Tres Mar√≠as": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Black Widow": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Claro Boba!": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    }
})

completados.update({
    "T√≠o Ovidio": {
        "cuisines": ["cafeter√≠a", "bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": ["billar", "conciertos"],
        "review_keywords": []
    },
    "Cascabel": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["vino", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante La Mas√≠a": {
        "cuisines": ["espa√±ola", "gallega", "pescados", "mariscos"],
        "platos_detectados": ["pulpo", "pastel de cabracho"],
        "top_tags": [],
        "review_keywords": []
    },
    "Esgos": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["vino", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ferreiro": {
        "cuisines": ["tienda alimentaci√≥n"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "El De Manol√≠n (A Coru√±a)": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pizzas", "tostas", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "BOCALINO": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©", "tortilla"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Caribe": {
        "cuisines": ["bar"],
        "platos_detectados": ["cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "A tenda de Mari√°n": {
        "cuisines": ["tienda golosinas"],
        "platos_detectados": ["chuches"],
        "top_tags": [],
        "review_keywords": []
    },
    "Froitomar, S.L.": {
        "cuisines": ["tienda alimentaci√≥n org√°nica"],
        "platos_detectados": ["productos bio"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Rabuda": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["croquetas", "raxo", "huevos rotos"],
        "top_tags": [],
        "review_keywords": []
    },
    "TRAMPITAN - Ourense": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["pinchos", "vermut"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "La Chica de Ayer": {
        "cuisines": ["pub"],
        "platos_detectados": ["copas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Paraxe": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["bocatas", "tortilla", "tapas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Grandola": {
        "cuisines": ["bar", "teter√≠a"],
        "platos_detectados": ["cachimba"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Zapas": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["lac√≥n", "pinchos", "vermut"],
        "top_tags": [],
        "review_keywords": []
    },
    "Treboada": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": [],
        "review_keywords": []
    },
    "Faiscas": {
        "cuisines": ["bar"],
        "platos_detectados": [],
        "top_tags": ["juegos"],
        "review_keywords": []
    },
    "Nakar": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["caf√©", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pan Punto Negro (varios locales)": {
        "cuisines": ["panader√≠a"],
        "platos_detectados": ["pan"],
        "top_tags": [],
        "review_keywords": []
    }
})

# === Completados para los renombrados ===
completados.update({
    "Alameda": {
        "cuisines": ["men√∫ del d√≠a", "pulpo", "tapas", "empanada", "oreja", "mejillones", "pimientos de padr√≥n"],
        "platos_detectados": ["pulpo", "empanada", "oreja", "mejillones"],
        "top_tags": [],
        "review_keywords": []
    },
    "Cervecer√≠a O' Barullo": {
        "cuisines": ["cervecer√≠a", "tapas", "tortillas", "raxo", "croquetas"],
        "platos_detectados": ["tortilla", "raxo", "croquetas"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Lagareta": {
        "cuisines": ["bar de tapas", "tostas", "tortilla", "pinchos"],
        "platos_detectados": ["tosta", "tortilla", "pincho"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mes√≥n O Rico": {
        "cuisines": ["espa√±ola", "gallega", "pescados", "carnes", "vinos", "mariscos"],
        "platos_detectados": ["pescado", "carne", "marisco"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante La Mas√≠a": {
        "cuisines": ["espa√±ola", "gallega", "pulpo", "carnes", "mariscos", "postres"],
        "platos_detectados": ["pulpo", "carne", "marisco", "postre"],
        "top_tags": [],
        "review_keywords": []
    },
    "Esencia Restaurante Coru√±a": {
        "cuisines": ["men√∫ del d√≠a", "degustaci√≥n", "desayunos"],
        "platos_detectados": ["platos variados"],
        "top_tags": ["apto cel√≠acos"],
        "review_keywords": []
    },
    "Cervecer√≠a Ansu 2": {
        "cuisines": ["cervecer√≠a"],
        "platos_detectados": ["cerveza", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Churre": {
        "cuisines": ["tapas", "comida variada"],
        "platos_detectados": ["tapas", "platos caseros"],
        "top_tags": [],
        "review_keywords": []
    },
    "Taberna de √Ålvaro": {
        "cuisines": ["bocadillos", "tapas", "tortilla", "croquetas", "callos"],
        "platos_detectados": ["bocadillo", "tortilla", "croquetas", "callos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mediterranean Buffet": {
        "cuisines": ["buffet"],
        "platos_detectados": ["platos variados"],
        "top_tags": [],
        "review_keywords": []
    },
    "Taberna O Enxebre": {
        "cuisines": ["gallego", "pulpo", "vinos", "men√∫"],
        "platos_detectados": ["pulpo", "vino"],
        "top_tags": [],
        "review_keywords": []
    },
    "√Ågape Restaurante": {
        "cuisines": ["arroces", "croquetas", "tapas"],
        "platos_detectados": ["arroz", "croquetas", "tapas"],
        "top_tags": ["terraza"],
        "review_keywords": []
    },
    "Pulper√≠a y Churrasquer√≠a Peregrinus": {
        "cuisines": ["pulpo", "carnes", "vinos", "churrasco"],
        "platos_detectados": ["pulpo", "carne", "churrasco"],
        "top_tags": ["men√∫ del d√≠a"],
        "review_keywords": []
    },
    "Bar El Pasillo": {
        "cuisines": ["bar"],
        "platos_detectados": ["ca√±as"],
        "top_tags": [],
        "review_keywords": []
    }
})

completados.update({
    "Restaurante Andi√≥n": {
        "cuisines": ["cafeter√≠a", "paellas", "platos combinados", "men√∫", "hamburguesas", "tapas", "bocadillos", "cerveza"],
        "platos_detectados": ["paella", "plato combinado", "men√∫", "hamburguesa", "tapas", "bocadillo", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Tr√©bol": {
        "cuisines": ["marisquer√≠a", "pescados", "caldo gallego", "chulet√≥n", "paella", "zamburi√±as", "pulpo"],
        "platos_detectados": ["marisco", "pescado", "caldo gallego", "chulet√≥n", "paella", "zamburi√±as", "pulpo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Tu Bicada": {
        "cuisines": ["restaurante", "men√∫", "comida francesa", "pizzas", "caf√©"],
        "platos_detectados": ["men√∫", "comida francesa", "pizza", "caf√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "LATERAL": {
        "cuisines": ["cafeter√≠a", "caf√©", "pinchos", "tortilla"],
        "platos_detectados": ["caf√©", "pincho", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    }
})


# === Completados para renombrados iniciales ===
completados.update({
    "Restaurante Las Barricas": {
        "cuisines": ["bocadillos", "tapas", "para llevar"],
        "platos_detectados": ["bocadillo", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Taberna A Pedra": {
        "cuisines": ["tapas", "mariscos", "pescados"],
        "platos_detectados": ["pulpo", "marisco", "pescado"],
        "top_tags": [],
        "review_keywords": []
    }
})

# === Completados adicionales (faltantes detectados) ===
completados.update({
    "O Recuncho do Granel": {
        "cuisines": ["otros"],
        "platos_detectados": ["tienda alimentaci√≥n naturales"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante A de Maruja": {
        "cuisines": ["italiana", "tapas"],
        "platos_detectados": ["pizzas", "postres", "bocatas"],
        "top_tags": [],
        "review_keywords": []
    },
    "La Bola Coru√±a": {
        "cuisines": ["bar", "caf√©"],
        "platos_detectados": ["club billar", "dardos", "cervezas", "caf√©s"],
        "top_tags": [],
        "review_keywords": []
    },
    "The veggie Carmen": {
        "cuisines": ["vegano", "internacional"],
        "platos_detectados": ["tostas", "arroz", "wok", "nachos", "hamburguesas", "pasta"],
        "top_tags": [],
        "review_keywords": []
    },
    "Frutas Rey": {
        "cuisines": ["otros"],
        "platos_detectados": ["fruter√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Mazaricos 1": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["men√∫ del d√≠a", "hamburguesa", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Plaza Cai√≥n": {
        "cuisines": ["cafeter√≠a", "mariscos/pescados"],
        "platos_detectados": ["mariscos", "pescados", "tapas", "terraza"],
        "top_tags": [],
        "review_keywords": []
    },
    "Alimentaci√≥n de los Emigrantes": {
        "cuisines": ["otros"],
        "platos_detectados": ["tienda alimentaci√≥n"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Luis": {
        "cuisines": ["espa√±ola/gallega", "mariscos/pescados"],
        "platos_detectados": ["men√∫ del d√≠a", "mariscos", "caldo gallego", "pulpo", "almejas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Pallona": {
        "cuisines": ["bar"],
        "platos_detectados": ["vinos"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sombra": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["pizzas", "empanadillas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Paladium Hotel": {
        "cuisines": ["espa√±ola/gallega"],
        "platos_detectados": ["restaurante"],
        "top_tags": [],
        "review_keywords": []
    },
    "YUCA T√© y m√°s cosas ricas": {
        "cuisines": ["otros"],
        "platos_detectados": ["tienda de t√©"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Badulaque": {
        "cuisines": ["espa√±ola/gallega", "mariscos/pescados"],
        "platos_detectados": ["patatas bravas", "arroces", "pescados", "dorada a la brasa"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Pibe": {
        "cuisines": ["bar", "hamburguesas"],
        "platos_detectados": ["tapas", "f√∫tbol", "vinos", "hamburguesas", "perritos calientes"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ray": {
        "cuisines": ["otros"],
        "platos_detectados": ["tienda golosinas"],
        "top_tags": [],
        "review_keywords": []
    },
    "As Lolas / NUEVA GERENCIA desde mayo de 2025": {
        "cuisines": ["espa√±ola/gallega", "tapas"],
        "platos_detectados": ["tapas", "postres", "bao", "pulpo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Lest√≥n Artesanos": {
        "cuisines": ["pan/boller√≠a"],
        "platos_detectados": ["pasteler√≠a", "boller√≠a", "empanadas"],
        "top_tags": [],
        "review_keywords": []
    },
    "AMARELA CORU√ëA": {
        "cuisines": ["otros"],
        "platos_detectados": ["queser√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Ollo Canalla Kf√©": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["postres", "rollo canela", "tostadas", "vegano", "tarta de queso", "tarta de zanahoria", "desayunos"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Despensa do Saa": {
        "cuisines": ["vino/bar"],
        "platos_detectados": ["vinoteca"],
        "top_tags": [],
        "review_keywords": []
    },
    "Breoghan": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tortilla", "vinos", "terraza"],
        "top_tags": [],
        "review_keywords": []
    },
    "ADARVE": {
        "cuisines": ["cafeter√≠a", "internacional"],
        "platos_detectados": ["men√∫ del d√≠a", "tapas", "arepas", "bocatas/s√°ndwiches"],
        "top_tags": [],
        "review_keywords": []
    },
    "Torre De Andrade": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["cerveza", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "El Gandul": {
        "cuisines": ["pub"],
        "platos_detectados": ["pub"],
        "top_tags": [],
        "review_keywords": []
    },
    "Artesanos MR DEL RIO S.L.": {
        "cuisines": ["pan/boller√≠a"],
        "platos_detectados": ["panader√≠a", "boller√≠a"],
        "top_tags": [],
        "review_keywords": []
    },
    "Sancosa S.L.": {
        "cuisines": ["dulces"],
        "platos_detectados": ["pasteler√≠a", "tarta de Santiago"],
        "top_tags": [],
        "review_keywords": []
    },
    "Central Perk": {
        "cuisines": ["bar", "tapas"],
        "platos_detectados": ["dardos", "futbol√≠n", "tapas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Restaurante Pi√±eiro": {
        "cuisines": ["espa√±ola/gallega", "mariscos/pescados"],
        "platos_detectados": ["tapas", "mariscos", "pescados", "postres"],
        "top_tags": [],
        "review_keywords": []
    },
    "Vista O Mar": {
        "cuisines": ["cafeter√≠a"],
        "platos_detectados": ["tostadas", "verm√∫", "desayunos", "cervezas"],
        "top_tags": ["cerca del puerto"],
        "review_keywords": []
    },
    "Casa de la Queimada": {
        "cuisines": ["bar"],
        "platos_detectados": ["bar musical", "espect√°culo"],
        "top_tags": [],
        "review_keywords": []
    },
    "Planeta Vigo": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["pinchos", "bocatas/s√°ndwiches", "tapas", "tortilla"],
        "top_tags": ["fumar permitido"],
        "review_keywords": []
    },
    "Ad√°n y Eva": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tortilla", "tapas", "empanada"],
        "top_tags": [],
        "review_keywords": []
    },
    "A nosa leira": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["mes√≥n", "cerveza", "tapas", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "La teta enroscada": {
        "cuisines": ["cafeter√≠a", "bar"],
        "platos_detectados": ["cervezas"],
        "top_tags": [],
        "review_keywords": []
    },
    "Nevada": {
        "cuisines": ["cafeter√≠a", "tapas", "hamburguesas"],
        "platos_detectados": ["f√∫tbol", "bocatas/s√°ndwiches", "hamburguesas", "tapas", "boller√≠a", "churros"],
        "top_tags": [],
        "review_keywords": []
    },
    "A Ponte": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["caf√©", "pinchos", "tortilla"],
        "top_tags": [],
        "review_keywords": []
    },
    "Nolita": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["desayunos", "f√∫tbol", "terraza", "tortilla", "pinchos"],
        "top_tags": [],
        "review_keywords": []
    },
    "AGUAS BLANCAS": {
        "cuisines": ["cafeter√≠a", "tapas"],
        "platos_detectados": ["tapas", "pinchos", "cerveza"],
        "top_tags": [],
        "review_keywords": []
    }
})

"""### Correcci√≥n de claves en `completados`

Algunas claves de `completados` no coincid√≠an con la columna normalizada `nombre_final_norm`.  
Se crea un diccionario `mapeo_correccion` para alinear estas claves y asegurar que el enriquecimiento se aplique correctamente.

"""

# --- Correcci√≥n de claves de completados usando nombre_final_norm ---

# Diccionario de mapeo: original -> nombre_final_norm
mapeo_correccion = {
    "El Cafet√≠n de Santiago (Santiago de Compostela)": "el cafetin de santiago santiago de compostela",
    "Jamoneria El Pinar -Cuatro Caminos": "jamoneria el pinar cuatro caminos",
    "it's Green": "it s green",
    "Cast√∏": "cast",
    "Porto Santo - Cuba": "porto santo cuba",
    "Atlastapas_vigo": "atlastapas vigo",
    "O'mouri√±o": "o mourino",
    "Paqui&palla": "paqui palla",
    "O' Laxe": "o laxe",
    "Tuypan Sl: Despacho F√°tima": "tuypan sl despacho fatima",
    "C'alma Fruit": "c alma fruit",
    "Frutas J.d Araujo": "frutas j d araujo",
    "Ocho_montealto": "ocho montealto",
    "TRAMPITAN - Ourense": "trampitan ourense",
    "El Cortijo - Orz√°n": "el cortijo orzan",
    "Estudio 34 (Santiago de Compostela)": "estudio 34 santiago de compostela",
    "Claro Boba!": "claro boba",
    "The City Vigo 2.0": "the city vigo 2 0",
    "Oolong&green": "oolong green",
    "D'Jaime": "d jaime",
    "D'luna Avellaneda": "d luna avellaneda",
    "El De Manol√≠n (A Coru√±a)": "el de manolin a coruna",
    "A Volti√±a. Restaurante": "a voltina restaurante",
    "Cbweed Shop - Vigo": "cbweed shop vigo",
    "El Gastro de Casona de Labrada, Aldea Rural Sostenible": "el gastro de casona de labrada aldea rural sostenible",
    "Rico Rico -p√°gina": "rico rico pagina",
    "O'norte": "o norte",
    "O'p√≠o": "o pio",
    "7„ÉªBOCADOS CASEROS„Éª7": "7 bocados caseros 7",
    "California (A Coru√±a)": "california a coruna",
    "Artesanos MR DEL RIO S.L.": "artesanos mr del rio s l",
    "Sancosa S.L.": "sancosa s l",
    "Froitomar, S.L.": "froitomar s l",
    "Dayu's": "dayu s",
    "A Pipa Da L√∫a -actividades": "a pipa da lua actividades",
    "Caf√©s Campinas S. Paulo- Empresas Reunidas S.A.": "cafes campinas s paulo empresas reunidas s a",
    "D'Marino": "d marino",
    "Lolita¬¥s": "lolita s",
    "Iogur!": "iogur",
    "Gumer - Restaurante Pontevedra": "gumer restaurante pontevedra",
    "As Lolas / NUEVA GERENCIA desde mayo de 2025": "as lolas nueva gerencia desde mayo de 2025",
    "Pink's!!": "pink s",
    "Todobio SL.": "todobio sl",
    "Como En Casa, Comida Para Llevar": "como en casa comida para llevar",
}


# Crear un nuevo diccionario con claves corregidas
completados_corregido = {}

for original, data in completados.items():
    if original in mapeo_correccion:
        clave_norm = mapeo_correccion[original]
        completados_corregido[clave_norm] = data
    else:
        completados_corregido[original] = data

# Reemplazar completados por la versi√≥n corregida
completados = completados_corregido

import unicodedata

def normalize_name(x: str) -> str:
    if not isinstance(x, str):
        return ""
    # quitar acentos
    text = unicodedata.normalize("NFD", x)
    text = "".join(c for c in text if unicodedata.category(c) != "Mn")
    return text.lower().strip()

# Normalizar eliminados, renombrados y completados
eliminados_norm = [normalize_name(x) for x in eliminados]

renombrados_norm = {normalize_name(k): v for k, v in renombrados.items()}

completados_norm = {normalize_name(k): v for k, v in completados.items()}

"""### Aplicaci√≥n de reglas al dataset

Se aplican las tres fases sobre el DataFrame:  
1. Eliminaci√≥n de registros marcados.  
2. Renombrado de restaurantes.  
3. Enriquecimiento de informaci√≥n faltante con `completados`.

"""

df_resultado = df.copy()

# --- 1. Eliminar ---
df_eliminados = df_resultado[df_resultado['nombre_final_norm'].isin(eliminados_norm)].copy()
df_resultado = df_resultado[~df_resultado['nombre_final_norm'].isin(eliminados_norm)].copy()

# --- 2. Renombrar ---
for old_norm, new in renombrados_norm.items():
    mask = df_resultado['nombre_final_norm'] == old_norm
    df_resultado.loc[mask, 'nombre_final'] = new
    df_resultado.loc[mask, 'nombre_final_norm'] = normalize_name(new)

# --- 3. Completar ---
for nombre_norm, info in completados_norm.items():
    mask = df_resultado['nombre_final_norm'] == nombre_norm
    for campo, valor in info.items():
        if campo in df_resultado.columns:
            df_resultado.loc[mask, campo] = pd.Series([valor] * mask.sum(), index=df_resultado.index[mask])
        else:
            df_resultado[campo] = None
            df_resultado.loc[mask, campo] = pd.Series([valor] * mask.sum(), index=df_resultado.index[mask])

"""### Revisi√≥n final y eliminaci√≥n manual

Tras aplicar las reglas, se revisan los registros que siguen con todas las columnas vac√≠as.  
En caso de quedar algunos pendientes, se eliminan manualmente del dataset y se guardan para revisi√≥n.

"""

# Detectar filas con las 4 columnas vac√≠as
cols_check = ["cuisines", "platos_detectados", "top_tags", "review_keywords"]

def es_vacio(val):
    if val is None: return True
    if isinstance(val, (float, int)) and pd.isna(val): return True
    if isinstance(val, (list, np.ndarray)) and len(val) == 0: return True
    if isinstance(val, str) and val.strip().lower() in ("[]", "", "nan", "none", "null"): return True
    return False

mask_all_empty = df_resultado[cols_check].applymap(es_vacio).all(axis=1)
df_pendientes = df_resultado.loc[mask_all_empty].copy()

print("üóëÔ∏è Eliminados:", len(df_eliminados))
print("‚úÖ Dataset final:", df_resultado.shape)
print("üîé Pendientes con todo vac√≠o:", len(df_pendientes))
print(df_pendientes[['nombre_final']].head(30))

# --- Lista de pendientes a eliminar manualmente ---
pendientes_manual = [
    "a pipa da lua actividades",
    "rico rico pagina",
    "lolita s",
    "d marino",
    "cafes campinas s paulo empresas reunidas s a",
    "dayu s"
]

# 1. Extraer los eliminados para revisi√≥n
df_eliminados_manual = df_resultado[df_resultado['nombre_final_norm'].isin(pendientes_manual)].copy()

# 2. Eliminar del dataset final
df_resultado = df_resultado[~df_resultado['nombre_final_norm'].isin(pendientes_manual)].copy()


df_resultado.to_excel("/content/restaurantes_final.xlsx", index=False, engine="openpyxl")
df_resultado.to_json("/content/restaurantes_final.json", orient="records", force_ascii=False, indent=2)


print("‚úÖ Dataset final guardado en:")
print("- Excel: /content/restaurantes_final.xlsx")
print("- JSON:  /content/restaurantes_final.json")

print("üìä Resumen:")
print(f"- Eliminados manualmente: {df_eliminados_manual.shape[0]}")
print(f"- Dataset final: {df_resultado.shape}")

"""## Creaci√≥n de flags a partir de categor√≠as

En este paso se construyen **categor√≠as tem√°ticas de restaurantes** (pizza, hamburguesas, asi√°tica, etc.)
a partir de la informaci√≥n textual contenida en distintas columnas del dataset
(nombre, descripciones, platos detectados, etiquetas traducidas, etc.).

### Objetivo
- Detectar autom√°ticamente qu√© categor√≠as corresponden a cada restaurante.  
- Permitir b√∫squedas sencillas como *‚Äúrestaurantes italianos con opci√≥n vegana y terraza‚Äù*.  
- Unificar bajo una misma categor√≠a sin√≥nimos y variantes (ej. *pizza artesanal*, *domino‚Äôs*, *pizza napolitana* ‚Üí **Pizza**).

### Metodolog√≠a
1. **Cargar dataset** enriquecido con traducciones y limpieza previa.  
2. **Inspeccionar valores √∫nicos** de las columnas relevantes para identificar t√©rminos frecuentes.  
3. **Definir un diccionario `categories`** con categor√≠as principales y subcategor√≠as,
   cada una con una lista de palabras clave y sin√≥nimos.  
4. **Compilar expresiones regulares** para mejorar la detecci√≥n de t√©rminos y evitar falsos positivos.  
5. **Asignar categor√≠as a cada restaurante** concatenando todas las columnas relevantes y buscando coincidencias.  
6. **Generar una nueva columna `categorias_asignadas`** en el dataframe con todas las etiquetas detectadas.


"""

# 1. Cargar librer√≠as
import pandas as pd
import numpy as np
# ============================================
# 2. Cargar dataset enriquecido
# ============================================
df = pd.read_json("/content/restaurantes_final.json")
print("‚úÖ Dataset cargado:", df.shape)
df.head(3)

import pandas as pd
import ast

def valores_unicos(df, col):
    """
    Extrae los valores √∫nicos de una columna que contiene listas o strings con listas.
    Devuelve un conjunto de t√©rminos en min√∫sculas y sin espacios extra.
    """
    unicos = set()
    for val in df[col].dropna():
        if isinstance(val, list):
            items = val
        elif isinstance(val, str) and val.strip().startswith("[") and val.strip().endswith("]"):
            try:
                items = ast.literal_eval(val)
                if not isinstance(items, list):
                    items = [items]
            except:
                items = [val]
        else:
            items = [val]

        for i in items:
            if i and str(i).strip():
                unicos.add(str(i).lower().strip())
    return sorted(unicos)

# --- Generar listado por columna ---
cols_interes = [
    "cuisines_traducidas",
    "platos_detectados",
    "top_tags_es",
    "dining_options_es",
    "diets_es",
    "meal_types_es",
    "review_keywords_es"
]

for col in cols_interes:
    if col in df.columns:
        print(f"\nüîé Valores √∫nicos en {col}:")
        for v in valores_unicos(df, col):
            print("-", v)

"""### Diccionario de categor√≠as
Se define un diccionario jer√°rquico `categories` donde cada bloque (Pizza, Hamburguesas, Asi√°tica, etc.)
incluye palabras clave y sin√≥nimos detectados en distintas columnas.  

Ejemplo:
- `"Pizza" ‚Üí ["pizza", "pizzer√≠a", "domino's", "napolitana", "horno de le√±a"]`
- `"Hamburguesas" ‚Üí ["hamburguesa", "burger", "goiko", "mcdonald's"]`

"""

categories = {
    "Pizza": {
        "general": [
            "pizza", "pizzas", "pizzer√≠a",
            "domino's pizza", "telepizza", "pizza hut", "pizza m√≥vil",
            "pizzer√≠a carlos", "natural pizza", "naci√≥n pizza", "pizza tutto",
            "masa pizza", "marshall pizza", "pomodoro", "napolit", "sinuessa pizza",
            "grosso napoletano", "cambalache", "sicilia in bocca", "cosa nostra",
            "pizzer√≠a la bella napoli", "pizzer√≠a isabella", "pizzer√≠a chicote",
            "pizza artesanal", "pizza napolitana", "horno de le√±a"
        ]
    },
    "Hamburguesas": {
        "general": [
            "hamburguesa", "hamburguesas", "hamburgueser√≠a",
            "burger", "burgers", "burger shop", "smashburger",
            "burger king", "mcdonald's", "goiko", "la pepita", "foster's hollywood",
            "tgb", "vips", "baruta burger", "kapital burger", "devoto burger",
            "berty's burger", "la burguer√≠a", "carnivale burger", "hot cold burger",
            "nan burger", "101 smashburger", "leyenda burger", "mister burguer",
            "da ca hamburgueser√≠a", "ruta burger", "mrk istambul burger",
            "hamburguesa gourmet", "hamburguesa premium", "hamburguesa casera",
            "hamburguesa del norte"
        ]
    },
    "Turca": {
        "general": [
            "kebab", "turca", "d√∂ner", "doner", "shawarma",
            "ali baba kebab", "istambul doner kebab", "newroz doner",
            "telekebab", "king doner", "doner kebab turco", "doner baran kebap"
        ]
    },
    "Mexicana": {
        "general": [
            "mexicana", "mexicano", "tacos", "nachos", "quesadilla",
            "tex-mex", "fondue mexicana", "burrito", "burritos", "taquer√≠a",
            "chilaquiles", "flautas", "enchiladas",
            "taco bell", "la chilanga", "charro mex", "el taj√≠n", "la nacher√≠a",
            "masktacos", "platero's tex mex", "sol maya", "la catrina",
            "la fondue mexicana",
            "margaritas", "charro negro"
        ]
    },
    "Italiana": {
        "general": [
            "italiana", "italiano", "italiana del centro", "italiana del norte",
            "italiana del sur", "napolitana", "siciliana", "romana", "sarda",
            "lasa√±a", "lasagna", "pasta", "risotto", "raviolis", "ravioles",
            "campania", "toscana", "trattoria", "ginos", "il piccolo", "il piccolino",
            "damario trattoria", "semplice trattoria italiana", "terra mia",
            "arrabbiata", "la tagliatella", "cambalache",
            "cannoli", "arancini", "tiramis√∫", "fugazzeta"
        ]
    },
    "Asi√°tica": {
        "china": [
            "china", "chino", "cantonesa", "sichuanesa", "shanghainesa",
            "taiwanesa", "fujianesa", "wok", "want√°n", "arroz frito",
            "pollo agridulce", "pollo chijaukai", "hot pot", "fondue china",
            "gran muralla", "hong kong", "pekin", "palacio oriental", "simbo",
            "new bombay palace", "casa hong", "manzana roja", "soy sushi buffet"
        ],
        "japonesa": [
            "japonesa", "japon√©s", "gyozas", "sushi", "alga", "cocina de kyoto",
            "yakitori", "izakaya", "bento",
            "sibuya", "totoro", "hiroki", "art sushi", "oogway sushi", "sushi fafa",
            "takimi sushi", "sensei", "umami sushi", "wantai sushi", "sakura sushi",
            "tsuki", "ramen bar"
        ],
        "ramen": [
            "ramen", "buga ramen", "shifu ramen", "okaeri ramen",
            "dashi ramen", "omichi ramen", "umi ramen", "itadakimas"
        ],
        "india": [
            "india", "indio", "paquistan√≠", "balti", "banglades√≠",
            "tandoori", "curry", "taj mahal", "royal tandoori", "curry house",
            "sabor de la india"
        ],
        "general": [
            "asi√°tica", "asi√°tico", "asi√°tica central", "oriental", "asian",
            "filipina", "coreana", "indonesia", "tailandesa", "thai",
            "mongola", "malaya", "hawaiana", "chifa (peruana-china)",
            "padthaiwok", "thai bangkok cuisine", "asian street food"
        ]
    },
    "Espa√±ola": {
        "tapas": [
            "tapas", "taper√≠a", "bar de tapas", "furancho", "montaditos",
            "pincho", "pinchos", "raciones", "croquetas", "croquetas de jam√≥n",
            "pimientos de padr√≥n", "taberna", "alb√≥ndigas", "zorza", "oreja",
            "vermut", "verm√∫", "queso", "callos", "callos con garbanzos", "tortilla"
        ],
        "cocina gallega": [
            "gallega", "gallego", "cocido gallego", "lac√≥n", "raxo", "caldo gallego",
            "empanada", "empanadillas", "orixe", "fogar do santiso",
            "productos de temporada", "platos del mercado"
        ],
        "general": [
            "espa√±ola", "espa√±ol", "mes√≥n", "tradicional", "platos combinados",
            "plato combinado", "comida casera", "cocina casera", "men√∫ econ√≥mico"
        ],
        "arroces": ["arroces", "arroz", "paella", "paellas", "fideu√°", "arroz de langosta"],
        "mariscos": [
            "marisco", "mariscos", "ostras", "bivalvos", "berberechos",
            "gambas", "navajas", "bogavante", "centolla", "calamares",
            "chipirones"
        ],
        "pescados": [
            "pescado", "pescados", "lubina", "merluza", "bacalao", "at√∫n",
            "sardinas", "vieiras", "zamburi√±as", "caldeirada"
        ],
        "pulpo": [
            "pulpo", "pulpo a la parrilla", "pulpeira", "pulperia","pulpeiro","pulpo a feira"
        ],
        "men√∫ del d√≠a": ["men√∫", "men√∫ del d√≠a"]
    },
    "Carnes": {
        "general": [
            "carnes", "carne de res", "carne gallega", "ternera gallega",
            "parrilla", "parrillada", "parrilla/carnes", "asador", "brasa", "barbacoa",
            "chulet√≥n", "t-bone", "tomahawk", "cordero", "cordero asado",
            "costillas", "bife", "carrilleras", "pollo", "pollo asado",
            "pollo frito", "pollo a la brasa", "pollo crujiente", "ternera",
            "cachopo", "rotisserie", "ribs", "entrecot", "cerdo", "carnicer√≠a"
        ]
    },
    "Veggie": {
        "vegano": [
            "vegano", "opciones veganas", "basado en plantas", "seit√°n",
            "the veggie carmen", "la cuchara veggie", "yanten vegan food",
            "a seimeira vegano", "vegano creativo", "flexitarianos",
            "comida macrobi√≥tica", "bio healthy"
        ],
        "vegetariano": ["vegetariano", "apto para vegetarianos", "ensaladas", "vegetariano amigable"],
        "sin gluten": ["opciones sin gluten", "sin gluten", "apto cel√≠acos"],
        "general": ["saludable", "verduras", "ecol√≥gico", "org√°nico", "sin az√∫car", "comida medicinal", "herborister√≠a"]
    },
    "Desayunos": {
        "cafeter√≠a": [
            "cafeter√≠a", "caf√©", "caf√©s", "caf√© bar", "caf√©-bar", "restaurante-cafeter√≠a",
            "desayuno", "desayunos", "bagels", "panader√≠a", "pan/boller√≠a",
            "croissant", "bizcochos", "magdalenas", "tortitas", "t√©", "zumos",
            "zumo", "granier", "panaria", "empanadas", "empanada"
        ],
        "brunch": ["brunch", "bowls", "bowl", "smoothie", "frapp√©s", "batidos de prote√≠nas"],
        "general": []
    },
    "Dulces": {
        "helados": [
            "helader√≠a", "helados", "helado", "yogur", "yogurter√≠a", "helado artesanal",
            "haagen dazs", "bico de xeado"
        ],
        "pasteler√≠as": [
            "pasteler√≠a", "pasteles", "dulces", "postres", "donas", "gofres", "churros",
            "tarta de queso", "tarta de santiago", "tarta de zanahoria", "tarta milhojas",
            "rollo canela", "bizcochos", "bica", "confiter√≠a", "chocolater√≠a", "torta",
            "tiramis√∫", "sorbete", "tienda golosinas", "croqueta y presumida",
            "canela", "canelitos", "granizados"
        ]
    },
    "R√°pida": {
        "bocadillos": [
            "bocadillo", "bocadillos", "bocata", "bocatas", "bocater√≠a",
            "bocatas/s√°ndwiches", "s√°ndwich"
        ],
        "autoservicio": ["autoservicio"],
        "general": ["comida r√°pida", "comida callejera", "platos mixtos", "americana", "comida a domicilio"]
    },
    "Poke": {
        "general": ["poke", "poke bowl", "poke bar", "koa poke", "umami poke", "mahalo poke", "tasty poke"]
    },
    "Latina": {
        "argentina": ["argentina", "bife", "parrillada argentina", "cantina argentina"],
        "venezolana": [
            "venezolana", "teque√±os", "arepas", "ensalada de papa", "areper√≠a",
            "bendita arepa", "mas arepas"
        ],
        "brasile√±a": ["brasile√±a", "rodizio", "rodizio express", "brasayle√±a", "parrillada brasile√±a"],
        "boliviana": ["boliviana", "platos bolivianos"],
        "peruana": ["peruana", "ceviche", "anticuchos", "aj√≠", "fusi√≥n peruana"],
        "salvadore√±a": ["salvadore√±a"],
        "general": [
            "latina", "latino", "sudamericana", "caribe√±a", "cubana",
            "colombiana", "centroamericana", "chilena", "platos caribe√±os", "mestiza"
        ]
    },
    "√Årabe": {
        "general": ["√°rabe", "libanesa", "marroqu√≠", "oriente medio", "shawarma", "mezopotamya"]
    },
    "Africana": {
        "general": ["africana", "afgana", "nigeriana"]
    },
    "Gourmet": {
        "men√∫ degustaci√≥n": ["men√∫ degustaci√≥n", "degustaci√≥n"],
        "general": [
            "alta cocina", "estrella michel√≠n", "gastrobar", "fusi√≥n",
            "maridaje de vinos", "tartar", "tataki", "experiencia culinaria",
            "charcuter√≠a", "delicatessen"
        ]
    },
    "Europea": {
        "general": [
            "alemana", "alem√°n", "australiana", "austriaca", "belga", "brit√°nica",
            "francesa", "griega", "portuguesa", "centroeuropea", "este europea",
            "europea", "armenian", "albanesa", "mediterr√°nea"
        ]
    },
    "Otros": {
        "supermercados": [
            "supermercado", "tienda alimentaci√≥n", "tienda de comida", "tienda ecol√≥gica",
            "fruter√≠a", "queser√≠a", "productos bio", "mercadona", "eroski", "dia", "froiz", "el corte ingl√©s",
            "tienda cannabis", "tienda alimentos naturales", "tienda de alimentos frescos"
        ],
        "bares": [
            "bar", "bares", "bar musical", "barra de bar", "pub", "cervecer√≠a",
            "cerveza", "cervezas", "cervecer√≠as", "cervecer√≠a artesanal",
            "bares deportivos", "barra de jazz", "sidra", "cocktails", "cocteler√≠a",
            "mojitos", "copas", "terraza", "afterwork", "vinoteca", "tienda vinos","vino"
        ],
        "ocio": [
            "f√∫tbol", "karaoke", "billar", "bolera", "juegos", "juegos de mesa",
            "bingo", "conciertos", "espect√°culo", "futbol√≠n", "dardos",
            "m√∫sica en vivo", "wifi gratis", "afterwork"
        ],
        "miscel√°nea": [
            "internacional", "variada", "platos internacionales", "otros", "ocio",
            "restaurante", "mercado", "mercado de pescado", "nativo americana", "polinesia",
            "buffet", "comedor"
        ]
    }
}

"""### Compilaci√≥n de patrones
Para optimizar la b√∫squeda, se compila el diccionario de categor√≠as a expresiones regulares.  
Esto evita falsos positivos y acelera la detecci√≥n al aplicar reglas uniformes.

### Asignaci√≥n de categor√≠as
Cada fila se procesa concatenando nombre, descripciones y etiquetas (`platos_detectados`, `cuisines_traducidas`, etc.).  
El texto resultante se cruza con los patrones y se genera la nueva columna `categorias_asignadas`.

"""

import re

def compilar_diccionario(categories):
    patrones = []
    for categoria, subcats in categories.items():
        for subcat, palabras in subcats.items():
            for palabra in palabras:
                # Crear regex con bordes de palabra, escapando bien
                regex = re.compile(rf"\b{re.escape(palabra.lower())}\b")
                if subcat == "general":
                    patrones.append((regex, categoria))
                else:
                    patrones.append((regex, f"{categoria} > {subcat}"))
    return patrones

# Compilamos una vez fuera del loop
patrones = compilar_diccionario(categories)

def asignar_categorias(texto, patrones):
    texto = str(texto).lower()
    asignadas = []
    for regex, etiqueta in patrones:
        if regex.search(texto):
            asignadas.append(etiqueta)
    return list(set(asignadas))

def procesar_fila(fila, patrones):
    partes = []
    for col in [
        "nombre_final_norm", "platos_detectados", "cuisines_traducidas",
        "description_es", "review_keywords_es", "diets_es",
        "meal_types_es", "dining_options_es", "top_tags_es"
    ]:
        val = fila.get(col, "")
        if isinstance(val, list):
            partes.extend(val)
        elif isinstance(val, str):
            partes.append(val)
    texto = " ".join([str(x) for x in partes if x])
    return asignar_categorias(texto, patrones)

# Aplicar al dataframe
df["categorias_asignadas"] = df.apply(lambda fila: procesar_fila(fila, patrones), axis=1)

"""### Resultado obtenido
Cada fila del dataframe tiene un campo adicional con las categor√≠as detectadas, por ejemplo:

- **‚ÄúSibuya Urban Sushi Bar‚Äù** ‚Üí `["Asi√°tica > japonesa", "Sushi"]`  
- **‚ÄúLa Pepita Burger Bar‚Äù** ‚Üí `["Hamburguesas"]`  
- **‚ÄúCasa Marcelo‚Äù** ‚Üí `["Espa√±ola > cocina gallega", "Gourmet"]`  

Esto servir√° como base para la creaci√≥n de **flags binarios** y para futuros an√°lisis y recomendaciones.

"""

# Visualizar ejemplos de categor√≠as asignadas
cols_vista = ["nombre_final_norm", "categorias_asignadas"]

# Mostrar 10 ejemplos aleatorios
ejemplos = df[cols_vista].sample(10, random_state=42)
display(ejemplos)

# O bien ver ejemplos concretos filtrando por categor√≠as
ejemplo_pizza = df[df["categorias_asignadas"].apply(lambda x: "Pizza" in x if isinstance(x, list) else False)]
pd.set_option("display.max_colwidth", None)  # no cortar columnas de texto
print("üçï Ejemplo de restaurantes clasificados como Pizza:")
display(ejemplo_pizza[cols_vista].head(5))

ejemplo_sushi = df[df["categorias_asignadas"].apply(lambda x: any("Asi√°tica > japonesa" in i for i in x) if isinstance(x, list) else False)]
print("üç£ Ejemplo de restaurantes clasificados como Japonesa:")
display(ejemplo_sushi[cols_vista].head(5))

# Filtrar restaurantes sin categor√≠as
sin_cat = df[df["categorias_asignadas"].apply(lambda x: not x or len(x) == 0)]

print(f"‚ö†Ô∏è Restaurantes sin categor√≠as asignadas: {sin_cat.shape[0]}")

# Vista r√°pida de algunos ejemplos
display(sin_cat[[
    "nombre_final_norm", "platos_detectados", "cuisines_traducidas",
    "description_es", "review_keywords_es", "categorias_asignadas"
]].head(20))

# Guardar en Excel/CSV para revisi√≥n manual si son muchos
sin_cat.to_excel("/content/restaurantes_sin_categoria.xlsx", index=False)
print("üìÇ Archivo exportado con los restaurantes sin categor√≠a.")

df.to_excel("/content/restaurantes_final.xlsx", index=False, engine="openpyxl")
df.to_json("/content/restaurantes_final.json", orient="records", force_ascii=False, indent=2)

"""## üìä Resumen del Cap√≠tulo 3: Enriquecimiento Categ√≥rico y Creaci√≥n de Categor√≠as Maestras

Tras aplicar reglas autom√°ticas (detecci√≥n de platos y categor√≠as) y manuales (eliminaci√≥n, renombrado y completado), se obtuvo un **dataset homog√©neo en espa√±ol** con etiquetas limpias y jerarquizadas.

### ‚úÖ Resultados principales
- Creaci√≥n de las columnas **`platos_detectados`** y **`categorias_asignadas`**.  
- Normalizaci√≥n de nombres y unificaci√≥n de etiquetas duplicadas o inconsistentes.  
- Eliminaci√≥n de registros sin informaci√≥n relevante.  
- Exportaci√≥n de la versi√≥n final en **Excel** y **JSON**.  
- Base lista para la generaci√≥n de **flags binarios** y el sistema de recomendaci√≥n.

# Cap√≠tulo 4: Selecci√≥n de columnas finales, limpieza avanzada y creaci√≥n de flags

En este cap√≠tulo se construye la primera versi√≥n definitiva del dataset, preparada para an√°lisis y algoritmos de recomendaci√≥n.  

## üéØ Objetivos
- Seleccionar solo las columnas relevantes para el modelo.  
- Eliminar duplicados y registros espurios aplicando reglas de consistencia.  
- Corregir manualmente outliers en ratings y rese√±as.  
- Transformar categor√≠as en variables binarias (**flags**) para facilitar consultas y an√°lisis estad√≠sticos.  
- Validar gr√°ficamente la coherencia del dataset tras la limpieza.  

## üìë Columnas seleccionadas
Se conservan √∫nicamente las variables clave:  

- **Identificaci√≥n** ‚Üí `id`, `nombre_final`, `nombre_final_norm`, `ciudad`, `maps_url_final`.  
- **Valoraci√≥n** ‚Üí `rating_final`, `reviews_final`, `price_num`, `score`.  
- **Categor√≠as** ‚Üí `categorias_asignadas`, `cuisines_traducidas`, `platos_detectados`.  
- **Opciones** ‚Üí `diets_es`, `meal_types_es`, `dining_options_es`, `top_tags_es`.  
- **Localizaci√≥n** ‚Üí `latitude`, `longitude`, `address_norm`.  
- **Servicios** ‚Üí `delivery`, `reservable`.  

Otras columnas (contacto, im√°genes, enlaces) se conservan en un dataset auxiliar de **informaci√≥n extra**, mientras que el resto se descartan por redundancia o falta de utilidad.  

---

### Creaci√≥n de flags binarios
A partir de `categorias_asignadas` se generan columnas dummy (0/1) que permiten filtrar f√°cilmente:  

- `flag_pizza`  
- `flag_mexicana`  
- `flag_vegano`  
- `flag_hamburguesas`  
- `flag_espa√±ola`  
- ‚Ä¶  

Estos indicadores facilitan el filtrado r√°pido por categor√≠as maestras y subcategor√≠as espec√≠ficas, habilitando consultas como *‚Äúhamburgueser√≠as veganas en Santiago con rating ‚â• 4.0‚Äù* o *‚Äúpizzer√≠as baratas en Vigo‚Äù*.  

---


## üìä Validaci√≥n gr√°fica

Como paso de verificaci√≥n adicional, se elaborarn varios **gr√°ficos exploratorios** con el objetivo de comprobar la coherencia del dataset:

1. **Ratings vs rese√±as** ‚Üí para detectar posibles outliers, como locales con 5‚≠ê y un n√∫mero muy reducido de rese√±as.  
2. **Top categor√≠as generales** ‚Üí para visualizar la distribuci√≥n principal de tipos de cocina en Galicia y confirmar qu√© bloques son m√°s frecuentes.  
3. **Distribuci√≥n de ratings por categor√≠a** ‚Üí para analizar si las valoraciones se concentran en torno a ciertos rangos y si existen diferencias significativas entre cocinas.  
4. **Precio medio por categor√≠a** ‚Üí para comparar el nivel de precios de cada categor√≠a y validar su consistencia.  
5. **Valoraci√≥n promedio por rango de precio** ‚Üí para explorar la relaci√≥n entre nivel de precios y valoraci√≥n media.  
6. **Delivery y reservas** ‚Üí para estimar la proporci√≥n de restaurantes que ofrecen servicios de entrega a domicilio o reserva anticipada.  
7. **Valoraci√≥n media por ciudad** ‚Üí para comprobar si existen diferencias relevantes en la satisfacci√≥n promedio entre las principales ciudades gallegas.  

Estos gr√°ficos servir√°n como herramienta de validaci√≥n final antes de dar por consolidado el dataset.


---

## ‚úÖ Resultado
El resultado de este cap√≠tulo es un **dataset limpio, consistente y enriquecido**.

| Columna                               | Estado   | Motivo |
|---------------------------------------|----------|--------|
| id                                    | ‚úÖ Core  | Identificador √∫nico del registro |
| name                                  | ‚ùå       | Sustituida por `nombre_final` |
| link                                  | ‚ùå       | URL redundante, usamos `maps_url_final` |
| rating                                | ‚ùå       | Normalizado en `rating_final` |
| reviews                               | ‚ùå       | Normalizado en `reviews_final` |
| price_range_usd                       | ‚ùå       | Normalizado en `price_num` |
| latitude                              | ‚úÖ Core  | Coordenadas para mapa y filtros |
| longitude                             | ‚úÖ Core  | Coordenadas para mapa y filtros |
| cuisines                              | ‚ùå       | Reemplazada por `cuisines_traducidas` |
| is_sponsored                          | ‚ùå       | Meta de TripAdvisor, no aporta al modelo |
| is_premium                            | ‚ùå       | Igual que anterior |
| menu_link                             | ‚ÑπÔ∏è Extra | Informaci√≥n √∫til en vista detallada |
| reservation_link                      | ‚ÑπÔ∏è Extra | Igual que anterior |
| featured_image                        | ‚ÑπÔ∏è Extra | Imagen del local, solo en detalle |
| ciudad                                | ‚úÖ Core  | Filtro geogr√°fico |
| address                               | ‚ùå       | Sustituida por `address_norm` |
| phone                                 | ‚ÑπÔ∏è Extra | Contacto en detalle del restaurante |
| international_phone                   | ‚ÑπÔ∏è Extra | Igual que anterior |
| website                               | ‚ÑπÔ∏è Extra | Contacto online |
| rating_google                         | ‚ùå       | Ya consolidado en `rating_final` |
| reviews_count_google                  | ‚ùå       | Igual que anterior |
| price_level_google                    | ‚ùå       | Normalizado en `price_num` |
| maps_url                              | ‚ùå       | Sustituida por `maps_url_final` |
| reservable                            | ‚úÖ Core  | Filtro de servicios |
| delivery                              | ‚úÖ Core  | Filtro de servicios |
| open_hours_google                     | ‚ùå       | Sustituido por `horarios_norm` |
| business_status                       | ‚ùå       | Info de origen, no √∫til en modelo |
| description                           | ‚ùå       | Sustituida por `description_es` |
| email                                 | ‚ÑπÔ∏è Extra | Contacto en ficha |
| review_keywords                       | ‚ùå       | Sustituida por `review_keywords_es` |
| diets                                 | ‚ùå       | Sustituida por `diets_es` |
| meal_types                            | ‚ùå       | Sustituida por `meal_types_es` |
| dining_options                        | ‚ùå       | Sustituida por `dining_options_es` |
| owner_types                           | ‚ùå       | Poco √∫til, info de origen |
| top_tags                              | ‚ùå       | Sustituida por `top_tags_es` |
| ranking.current_rank                  | ‚ùå       | Datos incompletos |
| ranking.total                         | ‚ùå       | Igual que anterior |
| detailed_address.street                | ‚ùå       | Redundante, usamos `address_norm` |
| detailed_address.city                  | ‚ùå       | Igual que anterior |
| detailed_address.postal_code           | ‚ùå       | Redundante |
| detailed_address.state                 | ‚ùå       | Vac√≠a |
| detailed_address.country_code          | ‚ùå       | Galicia ya contextualizado |
| reviews_per_rating.1                   | ‚ùå       | Detalle innecesario |
| reviews_per_rating.2                   | ‚ùå       | Igual que anterior |
| reviews_per_rating.3                   | ‚ùå       | Igual que anterior |
| reviews_per_rating.4                   | ‚ùå       | Igual que anterior |
| reviews_per_rating.5                   | ‚ùå       | Igual que anterior |
| open_hours.sun                         | ‚ùå       | Sustituido por `horarios_norm` |
| open_hours.mon                         | ‚ùå       | Igual que anterior |
| open_hours.tue                         | ‚ùå       | Igual que anterior |
| open_hours.wed                         | ‚ùå       | Igual que anterior |
| open_hours.thu                         | ‚ùå       | Igual que anterior |
| open_hours.fri                         | ‚ùå       | Igual que anterior |
| open_hours.sat                         | ‚ùå       | Igual que anterior |
| detalle_completo                       | ‚ùå       | Flag auxiliar sin valor |
| ranking                                | ‚ùå       | Siempre vac√≠o |
| google_maps_url                        | ‚ùå       | Sustituida por `maps_url_final` |
| name_google                            | ‚ùå       | Consolidado en `nombre_final` |
| open_hours_google.openNow              | ‚ùå       | Se calcular√° din√°micamente |
| open_hours_google.periods              | ‚ùå       | Sustituido por `horarios_norm` |
| open_hours_google.weekdayDescriptions  | ‚ùå       | Redundante |
| open_hours_google.nextCloseTime        | ‚ùå       | No necesario, se calcula |
| open_hours_google.nextOpenTime         | ‚ùå       | No necesario, se calcula |
| open_hours_dict                        | ‚ùå       | Sustituido por `horarios_norm` |
| nombre_final                           | ‚úÖ Core  | Nombre limpio final |
| rating_final                           | ‚úÖ Core  | Rating consolidado |
| reviews_final                          | ‚úÖ Core  | N¬∫ rese√±as consolidado |
| maps_url_final                         | ‚úÖ Core  | Enlace principal a Maps |
| horarios_final                         | ‚ùå       | Sustituido por `horarios_norm` |
| horarios_norm                          | ‚úÖ Core  | Horarios normalizados |
| nombre_final_norm                      | ‚úÖ Core  | Nombre normalizado para matching |
| description_clean                      | ‚ùå       | Sustituida por `description_es` |
| price_google_num                       | ‚ùå       | Redundante, consolidado en `price_num` |
| price_usd_num                          | ‚ùå       | No aporta en contexto local |
| price_range_num                        | ‚ùå       | Redundante |
| price_num                              | ‚úÖ Core  | Precio consolidado |
| score                                  | ‚úÖ Core  | M√©trica consolidada |
| row_score                              | ‚ùå       | Redundante con `score` |
| cuisines_traducidas                    | ‚úÖ Core  | Categor√≠as de cocina en espa√±ol |
| description_es                         | ‚úÖ Core  | Descripci√≥n en espa√±ol |
| review_keywords_es                     | ‚úÖ Core  | Palabras clave en espa√±ol |
| diets_es                               | ‚úÖ Core  | Dietas traducidas |
| meal_types_es                          | ‚úÖ Core  | Tipos de comida traducidos |
| dining_options_es                      | ‚úÖ Core  | Opciones de servicio traducidas |
| top_tags_es                            | ‚úÖ Core  | Tags destacados en espa√±ol |
| address_norm                           | ‚úÖ Core  | Direcci√≥n estandarizada |
| lat_round                              | ‚ùå       | Redondeo, innecesario |
| lon_round                              | ‚ùå       | Redondeo, innecesario |
| platos_detectados                      | ‚úÖ Core  | Platos detectados autom√°ticamente |
| categorias_asignadas                   | ‚úÖ Core  | Categor√≠as maestras homog√©neas |
"""

df.shape

import pandas as pd

# --- Cargar dataset desde JSON ---
df = pd.read_json("restaurantes_final.json")

# === Clasificaci√≥n de columnas ===
core_cols = [
    "id", "nombre_final", "nombre_final_norm", "ciudad",
    "latitude", "longitude",
    "rating_final", "reviews_final", "price_num", "score",
    "horarios_norm", "maps_url_final",
    "cuisines_traducidas", "platos_detectados", "categorias_asignadas",
    "description_es", "review_keywords_es", "diets_es", "meal_types_es",
    "dining_options_es", "top_tags_es",
    "address_norm", "delivery", "reservable"
]

extra_cols = [
    "id","phone", "international_phone",
    "website", "email",
    "featured_image",
    "menu_link", "reservation_link"
]

# El resto se consideran descartadas
descartar_cols = [c for c in df.columns if c not in core_cols + extra_cols]

# === Crear DataFrames ===
df_core = df[core_cols].copy()
df_extra = df[extra_cols].copy()
df_descartar = df[descartar_cols].copy()

print("‚úÖ DataFrames creados:")
print(f"- Core: {df_core.shape}")
print(f"- Extra info: {df_extra.shape}")
print(f"- Descartadas: {df_descartar.shape}")

# (Opcional) Guardar a JSON para seguir trabajando
df_core.to_json("restaurantes_core.json", orient="records", force_ascii=False, indent=2)
df_extra.to_json("restaurantes_extra.json", orient="records", force_ascii=False, indent=2)

"""## Revisi√≥n final de anomal√≠as

### Detecci√≥n de duplicados

Se analizan duplicados bajo los dos criterios usados anteriormente:
- **Nombre + direcci√≥n** ‚Üí posible repetici√≥n por la misma sede.  
- **Nombre + coordenadas** ‚Üí duplicados exactos en lat/lon.  

Se muestran ejemplos de los registros afectados para revisi√≥n.
"""

import pandas as pd
import numpy as np

df = pd.read_json("restaurantes_core.json")

# --- Duplicados por nombre + direcci√≥n ---
dup_name_address = df.duplicated(["nombre_final_norm", "address_norm"], keep=False)
df_dup_name_address = df[dup_name_address].sort_values(["nombre_final_norm", "address_norm"])

print("Duplicados por nombre + direcci√≥n:", df_dup_name_address.shape[0])
print("\nEjemplos (nombre + direcci√≥n):")
print(df_dup_name_address[["nombre_final","ciudad","address_norm","rating_final","reviews_final"]].head(20))

# --- Duplicados por nombre + coordenadas ---
dup_name_coords = df.duplicated(["nombre_final_norm", "latitude", "longitude"], keep=False)
df_dup_name_coords = df[dup_name_coords].sort_values(["nombre_final_norm", "latitude", "longitude"])

print("\nDuplicados por nombre + coordenadas:", df_dup_name_coords.shape[0])
print("\nEjemplos (nombre + coordenadas):")
print(df_dup_name_coords[["nombre_final","ciudad","latitude","longitude","rating_final","reviews_final"]].head(20))

# Copia de trabajo
df_limpio = df.copy()

# --- 1. Eliminar duplicados por nombre + direcci√≥n ---
before = df_limpio.shape[0]

df_limpio = df_limpio.sort_values("score", ascending=False)
df_limpio = df_limpio.drop_duplicates(subset=["nombre_final_norm", "address_norm"], keep="first")

after = df_limpio.shape[0]
print(f"üóëÔ∏è Eliminados por nombre+direcci√≥n: {before - after}")
print(f"‚úÖ Total tras limpieza nombre+direcci√≥n: {after}")

"""### Detecci√≥n de outliers en valoraciones

Se identifican restaurantes con:
- **Rating = 5‚≠ê pero con ‚â§ 2 rese√±as**, considerados sospechosos.  
Estos se corrigen manualmente (actualizaci√≥n de datos reales) o se eliminan si se trata de ruido.

"""

# Outliers sospechosos
sospechosos_rating = df[(df["rating_final"] == 5) & (df["reviews_final"] <= 2)]

print("5‚≠ê con pocas rese√±as:", sospechosos_rating.shape[0])
print("\nEjemplos de sospechosos:")
print(sospechosos_rating[["nombre_final", "ciudad","address_norm", "rating_final", "reviews_final"]].head(20))

# Copia de trabajo
df_corr = df_limpio.copy()

# --- 1. Actualizar valores ---
updates = {
    "cafeteria y restaurante al jaima": {"rating_final": 4.6, "reviews_final": 448},
    "o bocado iberico": {"rating_final": 4.8, "reviews_final": 5},
}

for nombre, cambios in updates.items():
    mask = df_corr["nombre_final_norm"] == nombre
    for col, val in cambios.items():
        df_corr.loc[mask, col] = val

# --- 2. Eliminar registros ---
eliminar = [
    "noe s boutique",
    "punto de encuentro",
    "waffle bubble company",
    "ponteareas",
    "lourenza",
    "sportium en bar cunqueiro",
]

before = df_corr.shape[0]
df_corr = df_corr[~df_corr["nombre_final_norm"].isin(eliminar)].copy()
after = df_corr.shape[0]

print(f"üóëÔ∏è Eliminados manualmente: {before - after}")
print(f"‚úÖ Dataset corregido: {df_corr.shape}")

"""### Ratings fuera de rango y rese√±as negativas

Se verifican anomal√≠as:
- `rating_final` fuera del rango [1, 5].  
- `reviews_final` < 0.  

Se listan ejemplos y se procede a corregirlos manualmente.

"""

# --- 3. Valoraciones ---
ratings_fuera = df[~df["rating_final"].between(1,5)]
print("Ratings fuera de rango:", ratings_fuera.shape[0])

if not ratings_fuera.empty:
    print("\nEjemplos de ratings fuera de rango:")
    print(ratings_fuera[["nombre_final","ciudad","address_norm","rating_final","reviews_final"]].head(20))


reviews_neg = df[df["reviews_final"] < 0]
print("\nReviews negativas:", reviews_neg.shape[0])

if not reviews_neg.empty:
    print("\nEjemplos de reviews negativas:")
    print(reviews_neg[["nombre_final","ciudad", "address_norm","rating_final","reviews_final"]].head(20))

# Copia de trabajo
df_corr2 = df_corr.copy()

# --- 1. Actualizar valores ---
updates = {
    "restaurante paradise": {"rating_final": 4.6, "reviews_final": 179},
    "o cruceiro de vilar terraza": {"rating_final": 4.4, "reviews_final": 127},
}

for nombre, cambios in updates.items():
    mask = df_corr2["nombre_final_norm"] == nombre
    for col, val in cambios.items():
        df_corr2.loc[mask, col] = val

# --- 2. Eliminar registros ---
eliminar = [
    "alexandre restaurante",
    "ipanema pub",
    "metropolis pub",
    "a queimada",
    "acceso parking",
    "bar xxele",
]

before = df_corr2.shape[0]
df_corr2 = df_corr2[~df_corr2["nombre_final_norm"].isin(eliminar)].copy()
after = df_corr2.shape[0]

print(f"üóëÔ∏è Eliminados manualmente: {before - after}")
print(f"‚úÖ Dataset corregido: {df_corr2.shape}")

"""### Validaci√≥n geogr√°fica y de categor√≠as
- Se revisan coordenadas fuera de rango, excluyendo nulos.  
- Se confirman los valores √∫nicos de `price_num`.  
- Se asegura que **todos los restaurantes tienen al menos una categor√≠a asignada**.  
- Se validan los flags de servicios (`delivery`, `reservable`).
"""

# --- 2. Geograf√≠a (sin contar NaN como error) ---
lat_ok = df_corr2["latitude"].between(41.8, 44.2) | df_corr2["latitude"].isna()
lon_ok = df_corr2["longitude"].between(-9.3, -6.5) | df_corr2["longitude"].isna()

fuera_rango = df_corr2[~(lat_ok & lon_ok)]
print("Coordenadas fuera de rango:", fuera_rango.shape[0])

# --- 3. Valoraciones ---
print("Ratings fuera de rango:", df_corr2[~df_corr2["rating_final"].between(1,5)].shape[0])
print("Reviews negativas:", (df_corr2["reviews_final"] < 0).sum())

# --- 4. Precio ---
print("Valores √∫nicos en price_num:", df["price_num"].dropna().unique())

# --- 5. Categor√≠as ---
sin_categorias = df_corr2[df_corr2["categorias_asignadas"].isna() | (df_corr2["categorias_asignadas"].apply(len) == 0)]
print("Restaurantes sin categor√≠as:", sin_categorias.shape[0])

# --- 6. Flags servicios ---
print("Valores √∫nicos delivery:", df_corr2["delivery"].dropna().unique())
print("Valores √∫nicos reservable:", df_corr2["reservable"].dropna().unique())

"""## Creaci√≥n de flags binarios

Para facilitar consultas y an√°lisis, se generaron **variables dummy (0/1)** a partir de las **categor√≠as maestras** (`categorias_asignadas`).  
Se excluy√≥ la categor√≠a **‚ÄúOtros‚Äù** al no aportar valor anal√≠tico.  

#### Categor√≠as incluidas como flags:
- Pizza  
- Hamburguesas  
- Turca  
- Mexicana  
- Italiana  
- Asi√°tica  
- Espa√±ola  
- Carnes  
- Veggie  
- Desayunos  
- Dulces  
- R√°pida  
- Poke  
- Latina  
- √Årabe  
- Africana  
- Gourmet  
- Europea  

Cada restaurante recibe un `1` en la columna correspondiente si pertenece a esa categor√≠a, `0` en caso contrario.  
Ejemplos de columnas generadas: `flag_pizza`, `flag_mexicana`, `flag_vegano`, etc.  

"""

flags = [
    "Pizza", "Hamburguesas", "Turca", "Mexicana", "Italiana",
    "Asi√°tica", "Espa√±ola", "Carnes", "Veggie", "Desayunos",
    "Dulces", "R√°pida", "Poke", "Latina", "√Årabe",
    "Africana", "Gourmet", "Europea"
]

df_flags = df_corr2.copy()

# Crear flags binarios
for cat in flags:
    col = f"flag_{cat.lower()}"
    df_flags[col] = df_flags["categorias_asignadas"].apply(
        lambda x: int(any(cat in str(i) for i in x)) if isinstance(x, list) else 0
    )

# Revisi√≥n r√°pida
print("‚úÖ Flags creados:")
for col in [c for c in df_flags.columns if c.startswith("flag_")]:
    print(f"{col}: {df_flags[col].sum()} restaurantes")

df_flags.to_excel("/content/restaurantes_modelo.xlsx", index=False, engine="openpyxl")
df_flags.to_json("/content/restaurantes_modelo.json", orient="records", force_ascii=False, indent=2)

"""# üö® Cap√≠tulo 5: Resoluci√≥n de inconsistencias en el dataset

Durante el desarrollo del **modelo de recomendaci√≥n** se detectaron inconsistencias en el dataset que no se hab√≠an resuelto en las fases previas de limpieza.  

Aunque en los cap√≠tulos anteriores ya se eliminaron locales cerrados, se normalizaron horarios y direcciones, y se aplicaron rutinas de deduplicaci√≥n, al poner en pr√°ctica el sistema aparecieron problemas que requer√≠an una **segunda limpieza m√°s exhaustiva**.

## üîé Problemas detectados
1. **Duplicados persistentes**  
   - Restaurantes con la misma direcci√≥n (`address_norm`).  
   - Registros con las mismas coordenadas (`latitude`, `longitude`).  
   - Casos con la misma direcci√≥n y URL de Google Maps.  
   - Locales en la misma direcci√≥n pero con variantes m√≠nimas en el nombre (tildes, abreviaturas).

2. **Errores de geolocalizaci√≥n**  
   - Restaurantes distintos compartiendo coordenadas.  
   - Coordenadas asociadas a nombres incorrectos.

3. **Desajustes ciudad ‚Üî coordenadas**  
   - Locales con coordenadas dentro de Vigo, A Coru√±a, Santiago, Lugo, Ourense o Pontevedra pero etiquetados con otra ciudad distinta.  

## üõ†Ô∏è Acciones correctivas
Para solucionar estos problemas se aplicaron varias estrategias:

- **Detecci√≥n y resoluci√≥n de duplicados**  
  Se identificaron duplicados por direcci√≥n, coordenadas, direcci√≥n+nombre simplificado y direcci√≥n+URL. En cada grupo se conserv√≥ el registro con mayor `score`.  

- **Uso de Google Places**  
  Los duplicados por direcci√≥n se contrastaron contra la API de Google Places, lo que permiti√≥ enriquecer los registros con informaci√≥n actualizada (nombre, direcci√≥n, horarios, rating, rese√±as y precio).  

- **Normalizaci√≥n de datos**  
  Los campos clave (horarios, ratings, rese√±as, precios y direcciones) se volvieron a normalizar con los datos m√°s recientes de Google.  

- **Reintegraci√≥n en el dataset maestro**  
  Se sustituyeron los valores antiguos por los normalizados, manteniendo la misma estructura de dataset.  

- **Correcciones manuales**  
  Eliminaci√≥n de IDs duplicados, correcci√≥n de coordenadas err√≥neas y actualizaci√≥n de nombres especiales.  

- **Validaci√≥n ciudad‚Äìcoordenadas**  
  Se utilizaron *bounding boxes* de las principales ciudades gallegas para confirmar que la columna `ciudad` coincid√≠a con la localizaci√≥n geogr√°fica del restaurante.  

## ‚úÖ Resultado
El proceso dio lugar a un dataset:  
- Sin duplicados residuales (ni por direcci√≥n, ni por URL, ni por coordenadas).  
- Con informaci√≥n m√°s precisa y actualizada desde Google Places.  
- Con coordenadas consistentes y ciudades verificadas.  

Los archivos finales generados fueron:  
- `restaurantes_modelo_final.json`  
- `restaurantes_modelo_final.xlsx`  

De este modo, el dataset queda completamente depurado y preparado para alimentar al **modelo de recomendaci√≥n** sin inconsistencias.

### Detecci√≥n de Duplicados

A pesar de haber limpiado los datos se encontr√≥ un error en los datos respecto a los duplicados. Por este motivo, en esta secci√≥n se analizan las **direcciones** y **coordenadas** para identificar restaurantes
que aparecen m√°s de una vez en el dataset.

- **Duplicados por direcci√≥n** ‚Üí mismos valores en `address_norm`.  
- **Duplicados por coordenadas** ‚Üí mismos valores en `latitude` y `longitude`.  
- Se guardan reportes en `.json` y `.xlsx` para revisi√≥n manual.
"""

# --- Cargar dataset desde JSON ---
df = pd.read_json("restaurantes_modelo.json")

# --- Direcciones repetidas (excluyendo nulas) ---
duplicados_direccion = (
    df[df["address_norm"].notna()]
    .loc[df["address_norm"].notna()]
    .loc[lambda d: d.duplicated("address_norm", keep=False)]
)

print("üìç Restaurantes con la misma direcci√≥n (excluyendo nulas):", duplicados_direccion.shape[0])
if not duplicados_direccion.empty:
    display(
        duplicados_direccion
        .sort_values("address_norm")[["nombre_final", "ciudad", "address_norm","rating_final", "reviews_final", "maps_url_final"]]
    )

# --- Coordenadas repetidas (excluyendo nulas) ---
duplicados_coords = (
    df[df["latitude"].notna() & df["longitude"].notna()]
    .loc[lambda d: d.duplicated(["latitude", "longitude"], keep=False)]
)

print("üåç Restaurantes con las mismas coordenadas (excluyendo nulas):", duplicados_coords.shape[0])
if not duplicados_coords.empty:
    display(
        duplicados_coords
        .sort_values(["latitude", "longitude"])[["nombre_final_norm", "ciudad", "latitude", "longitude", "rating_final", "reviews_final", "maps_url_final"]]
    )

    # Guardar a JSON
    duplicados_direccion.to_json("restaurantes_direcciones_duplicadas.json", orient="records", force_ascii=False, indent=2)

    # Guardar a Excel
    duplicados_direccion.to_excel("restaurantes_direcciones_duplicadas.xlsx", index=False)

    print("‚úÖ Archivos guardados: restaurantes_direcciones_duplicadas.json y .xlsx")

"""### Resoluci√≥n de duplicados por direcci√≥n

Este bloque de c√≥digo utiliza la **API de Google Places** para validar y actualizar restaurantes detectados con la misma direcci√≥n (`address_norm`).

#### Objetivo
- Resolver conflictos de **duplicados por direcci√≥n** escogiendo la fuente m√°s fiable.  
- Enriquecer los registros con datos de Google: nombre, direcci√≥n, coordenadas, horarios, rating, rese√±as y precios.  

#### Proceso
1. Cargar los restaurantes duplicados desde `restaurantes_direcciones_duplicadas.json`.  
2. Buscar cada restaurante en **Google Places** (nombre + ciudad).  
3. Seleccionar el **mejor candidato** seg√∫n similitud de nombre y proximidad de coordenadas.  
4. Actualizar el registro con la informaci√≥n m√°s completa.  
5. Guardar los resultados en:  
   - `restaurantes_actualizados.json / .xlsx` (actualizados).  
   - `restaurantes_no_encontrados.json` (sin match v√°lido).  

#### Resultado
Se obtiene un dataset **sin duplicados por direcci√≥n**, con datos m√°s precisos y consistentes.

"""

!pip install unidecode

import requests
import json
import time
import re
import unidecode
import difflib
import pandas as pd
from math import radians, sin, cos, sqrt, atan2

# ======= CONFIG =======
API_KEY = "AIzaSyDpGcr99hoMCu8pc58JDae5x8fglA_3Chs"  # ‚Üê tu API key
INPUT_FILE = "restaurantes_direcciones_duplicadas.json"
OUTPUT_JSON = "restaurantes_actualizados.json"
OUTPUT_XLSX = "restaurantes_actualizados.xlsx"
NO_ENCONTRADOS_FILE = "restaurantes_no_encontrados.json"

LANG = "es"
MAX_LLAMADAS = 50000
contador_llamadas = 0

# Bounding box Galicia
BOUNDING_BOX_GALICIA = {
    "min_lat": 41.80,
    "max_lat": 43.85,
    "min_lng": -9.35,
    "max_lng": -6.50
}

# ======= UTILIDADES =======
def normalizar_texto(s: str) -> str:
    if not s:
        return ""
    s = unidecode.unidecode(s.lower())
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def normalizar_nombre(nombre: str) -> str:
    return normalizar_texto(nombre)

def normalizar_ciudad(c: str) -> str:
    return unidecode.unidecode((c or "").lower()).strip()

def sim(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, (a or ""), (b or "")).ratio()

def en_bbox_galicia(lat, lng) -> bool:
    if lat is None or lng is None:
        return False
    return (BOUNDING_BOX_GALICIA["min_lat"] <= lat <= BOUNDING_BOX_GALICIA["max_lat"] and
            BOUNDING_BOX_GALICIA["min_lng"] <= lng <= BOUNDING_BOX_GALICIA["max_lng"])

def haversine(lat1, lon1, lat2, lon2):
    """Distancia en metros entre dos coordenadas."""
    R = 6371000  # radio tierra en m
    phi1, phi2 = radians(lat1), radians(lat2)
    dphi = radians(lat2 - lat1)
    dlambda = radians(lon2 - lon1)

    a = sin(dphi/2)**2 + cos(phi1)*cos(phi2)*sin(dlambda/2)**2
    return 2*R*atan2(sqrt(a), sqrt(1-a))

# ======= GOOGLE PLACES =======
def search_text(name, ciudad=None):
    """B√∫squeda por texto con sesgo a Galicia."""
    global contador_llamadas
    query = f"{name} {ciudad}" if ciudad else name
    url = "https://places.googleapis.com/v1/places:searchText"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": API_KEY,
        "X-Goog-FieldMask": "places.id,places.displayName,places.formattedAddress,places.location"
    }
    data = {
        "textQuery": query,
        "languageCode": LANG,
        "regionCode": "ES",
        "locationBias": {
            "rectangle": {
                "low": {"latitude": BOUNDING_BOX_GALICIA["min_lat"], "longitude": BOUNDING_BOX_GALICIA["min_lng"]},
                "high": {"latitude": BOUNDING_BOX_GALICIA["max_lat"], "longitude": BOUNDING_BOX_GALICIA["max_lng"]}
            }
        }
    }
    r = requests.post(url, headers=headers, json=data)
    contador_llamadas += 1
    if r.status_code != 200:
        print(f"‚ö†Ô∏è Error searchText: {r.status_code} ‚Üí {r.text}")
        return []
    return r.json().get("places", []) or []

def obtener_detalles_place(place_id):
    global contador_llamadas
    url = f"https://places.googleapis.com/v1/places/{place_id}"
    headers = {
        "Content-Type": "application/json",
        "X-Goog-Api-Key": API_KEY,
        "X-Goog-FieldMask": ",".join([
            "displayName","formattedAddress","googleMapsUri","regularOpeningHours",
            "rating","userRatingCount","priceLevel","location"
        ])
    }
    r = requests.get(url, headers=headers)
    contador_llamadas += 1
    if r.status_code != 200:
        return {}
    return r.json()

# ======= MATCHING =======
def elegir_mejor_candidato(nombre, ciudad, candidatos, rest_lat=None, rest_lon=None):
    """Elige candidato usando nombre y, si hay empate, coordenadas como desempate."""
    n1 = normalizar_nombre(nombre or "")
    c1 = normalizar_ciudad(ciudad)

    mejor = None
    mejor_score = -1
    tipo = None

    for p in candidatos:
        dn = (p.get("displayName") or {}).get("text", "")
        dn_norm = normalizar_nombre(dn)
        loc = p.get("location") or {}
        lat, lng = loc.get("latitude"), loc.get("longitude")

        # Filtro: solo Galicia
        if not en_bbox_galicia(lat, lng):
            continue

        # 1) Coincidencia exacta ‚Üí gana directo
        if dn_norm == n1:
            return p, "perfecto"

        # 2) Score por similitud de nombre
        s = sim(dn_norm, n1)

        # 3) Bonus por proximidad si coords conocidas (solo como desempate)
        if rest_lat and rest_lon and lat and lng:
            dist = haversine(rest_lat, rest_lon, lat, lng)
            if dist < 100:   # solo suma si est√° razonablemente cerca
                s += 0.2

        if s > mejor_score:
            mejor = p
            mejor_score = s
            if s >= 0.86:
                tipo = "fuerte"
            elif s >= 0.40:
                tipo = "d√©bil"

    # ‚ö†Ô∏è Si la similitud es muy baja (<0.4), descartamos para evitar falsos positivos
    if mejor_score < 0.40:
        return None, None

    return mejor, tipo

# ======= PROCESO =======
with open(INPUT_FILE, "r", encoding="utf-8") as f:
    restaurantes = json.load(f)

actualizados = []
no_encontrados = []

for idx, rest in enumerate(restaurantes):
    if contador_llamadas >= MAX_LLAMADAS:
        break

    nombre = rest.get("nombre_final")
    ciudad = rest.get("ciudad")
    rest_lat = rest.get("latitude")
    rest_lon = rest.get("longitude")

    print(f"[{idx+1}/{len(restaurantes)}] üîé {nombre} ({ciudad})")

    # Buscar siempre por texto (nombre + ciudad)
    candidatos = search_text(nombre, ciudad)
    if not candidatos:
        print(f"‚ùå No encontrado: {nombre}")
        no_encontrados.append(rest)
        continue

    mejor, tipo = elegir_mejor_candidato(nombre, ciudad, candidatos, rest_lat, rest_lon)
    if not mejor:
        print(f"‚ùå Sin buen candidato: {nombre}")
        no_encontrados.append(rest)
        continue

    detalles = obtener_detalles_place(mejor["id"])
    if not detalles:
        no_encontrados.append(rest)
        continue

    # ‚úÖ Actualizar con los datos de Google
    rest["name_google"] = (detalles.get("displayName") or {}).get("text", "")
    rest["address"] = detalles.get("formattedAddress", rest.get("address"))
    rest["google_maps_url"] = detalles.get("googleMapsUri")
    rest["open_hours_google"] = (detalles.get("regularOpeningHours") or {}).get("weekdayDescriptions")
    rest["rating_google"] = detalles.get("rating")
    rest["reviews_count_google"] = detalles.get("userRatingCount")
    rest["price_level_google"] = detalles.get("priceLevel")
    rest["match_type"] = tipo

    print(f"‚úÖ Encontrado: {nombre} (Google: {rest['name_google']}) "
          f"‚Üí {rest['address']} (‚≠ê {rest['rating_google']} | Rese√±as: {rest['reviews_count_google']} | Precio: {rest['price_level_google']})")

    actualizados.append(rest)
    time.sleep(0.2)  # evitar l√≠mite API

# ======= SALIDA =======
with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
    json.dump(actualizados, f, ensure_ascii=False, indent=2)

with open(NO_ENCONTRADOS_FILE, "w", encoding="utf-8") as f:
    json.dump(no_encontrados, f, ensure_ascii=False, indent=2)

df = pd.DataFrame(actualizados)
df.to_excel(OUTPUT_XLSX, index=False)

print(f"\n‚úÖ Guardados {len(actualizados)} en {OUTPUT_JSON} y {OUTPUT_XLSX}")
print(f"‚ùå No encontrados: {len(no_encontrados)} ‚Üí {NO_ENCONTRADOS_FILE}")
print(f"‚ÑπÔ∏è Llamadas usadas: {contador_llamadas}")

# 2. Cargar JSON inicial
file_path = "/content/restaurantes_actualizados.json"
with open(file_path, "r", encoding="utf-8") as f:
    data = json.load(f)

df = pd.json_normalize(data)

print("‚úÖ Dataset cargado")
print("Dimensiones:", df.shape)
print("Columnas iniciales:", len(df.columns))

# Vista previa
df.head(3)

"""### Normalizaci√≥n de datos nuevos

En este bloque se **reproduce el proceso de normalizaci√≥n** ya aplicado en el Cap√≠tulo 1, pero ahora sobre los **datos actualizados obtenidos del scraping de Google Places**.  

Se normalizan de nuevo los campos clave:  
- **Horarios** ‚Üí convertidos a formato homog√©neo en espa√±ol.  
- **Ratings y rese√±as** ‚Üí convertidos a valores num√©ricos consistentes.  
- **Precios** ‚Üí mapeados a la escala unificada 1‚Äì4.  
- **Direcciones** ‚Üí limpiadas y sobrescritas en `address_norm`.  

El resultado es un dataset con los **datos m√°s recientes y consistentes**, guardado en `restaurantes_actualizados_final.xlsx / .json`.

"""

import pandas as pd
import re
import ast
from datetime import datetime

# ==================================================
# 1. Normalizaci√≥n de horarios
# ==================================================
DIAS_EN = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
DIAS_ES = ["lunes","martes","mi√©rcoles","jueves","viernes","s√°bado","domingo"]
MAP_DIAS = dict(zip(DIAS_EN, DIAS_ES))

def to_24h(hora_str):
    try:
        return datetime.strptime(hora_str.strip(), "%I:%M %p").strftime("%H:%M")
    except:
        try:
            return datetime.strptime(hora_str.strip(), "%I %p").strftime("%H:%M")
        except:
            return hora_str.strip()

def normalizar_horarios(val):
    if val is None or (isinstance(val, float) and pd.isna(val)):
        return ["Horario no disponible"]

    if isinstance(val, list):
        lista = val
    elif isinstance(val, str) and val.strip().startswith("[") and val.strip().endswith("]"):
        try:
            lista = ast.literal_eval(val)
        except:
            return ["Horario no disponible"]
    else:
        return ["Horario no disponible"]

    salida = []
    for item in lista:
        if not isinstance(item, str) or ":" not in item:
            continue

        dia_en, horas = item.split(":", 1)
        dia_es = MAP_DIAS.get(dia_en.strip(), dia_en.strip())
        horas = horas.strip()

        if "Closed" in horas:
            salida.append(f"{dia_es}: cerrado")
            continue
        if "Open 24 hours" in horas:
            salida.append(f"{dia_es}: abierto 24h")
            continue

        rangos = []
        for rango in horas.split(","):
            match = re.split(r"‚Äì|-", rango)
            if len(match) == 2:
                start, end = match
                start_24 = to_24h(start)
                end_24 = to_24h(end)
                rangos.append(f"{start_24}‚Äì{end_24}")

        if rangos:
            salida.append(f"{dia_es}: {', '.join(rangos)}")

    return salida if salida else ["Horario no disponible"]

# ==================================================
# 2. Cargar dataset del scraping actualizado
# ==================================================
df_new = pd.read_excel("restaurantes_actualizados.xlsx")

# Normalizar horarios
df_new["horarios_norm"] = df_new["open_hours_google"].apply(normalizar_horarios)

# ==================================================
# 3. Normalizaci√≥n de ratings y rese√±as
# ==================================================
df_new["rating_google"] = pd.to_numeric(df_new["rating_google"], errors="coerce")
df_new["reviews_count_google"] = (
    pd.to_numeric(df_new["reviews_count_google"], errors="coerce")
    .fillna(0)
    .astype(int)
)

# ==================================================
# 4. Normalizaci√≥n de precios
# ==================================================
MAP_GOOGLE_PRICE = {
    "PRICE_LEVEL_INEXPENSIVE": 1,
    "PRICE_LEVEL_MODERATE": 2,
    "PRICE_LEVEL_EXPENSIVE": 3,
    "PRICE_LEVEL_VERY_EXPENSIVE": 4,
    0: 1, "0": 1,
    1: 1, "1": 1,
    2: 2, "2": 2,
    3: 3, "3": 3,
    4: 4, "4": 4
}

df_new["price_level_google"] = df_new["price_level_google"].map(MAP_GOOGLE_PRICE)

# ==================================================
# 5. Unificaci√≥n de campos finales (los nuevos pisan a los antiguos)
# ==================================================
df_new["rating_final"] = df_new["rating_google"].fillna(df_new["rating_final"])
df_new["reviews_final"] = df_new["reviews_count_google"].fillna(df_new["reviews_final"])
df_new["price_num"] = df_new["price_level_google"].fillna(df_new["price_num"])
df_new["maps_url_final"] = df_new["google_maps_url"].fillna(df_new["maps_url_final"])

# ==================================================
# üëâ Direcci√≥n: sobrescribimos siempre con la nueva si existe, pero normalizada
# ==================================================
df_new["address_norm"] = df_new.apply(
    lambda row: clean_text(row["address"], mode="address") if pd.notna(row["address"]) else row["address_norm"],
    axis=1
)

# üîé Mostrar ejemplos de normalizaci√≥n de direcciones
print("\nüîé Ejemplo de direcciones normalizadas:")
muestra = df_new[["nombre_final", "address", "address_norm"]].dropna().sample(5, random_state=42)
print(muestra.to_string(index=False))



# ==================================================
# 6. Guardar resultados
# ==================================================
df_new.to_excel("restaurantes_actualizados_final.xlsx", index=False)
df_new.to_json("restaurantes_actualizados_final.json",
               orient="records", force_ascii=False, indent=2)

print("‚úÖ Archivos guardados con todos los campos normalizados (incluyendo direcci√≥n)")

"""### Sustituci√≥n en el dataset maestro

En el bloque siguiente, los datos normalizados se **fusionan con el dataset maestro (`restaurantes_modelo.json`)**.  

- Se realiza un **merge por `id`** para alinear registros.  
- Las columnas clave (`horarios_norm`, `rating_final`, `reviews_final`, `price_num`, `maps_url_final`, `address_norm`) se **sobrescriben con los valores nuevos**.  
- El resultado es un √∫nico dataset actualizado:  
  - `restaurantes_modelo_actualizado.xlsx`  
  - `restaurantes_modelo_actualizado.json`  

De esta forma, el dataset final mantiene la estructura original, pero con la informaci√≥n m√°s reciente y fiable.

"""

import pandas as pd

# ==================================================
# 1. Cargar datasets
# ==================================================
df_modelo = pd.read_json("restaurantes_modelo.json")               # Dataset maestro
df_actualizados = pd.read_json("restaurantes_actualizados_final.json")  # Dataset con datos normalizados

# ==================================================
# 2. Selecci√≥n de columnas nuevas/actualizadas
# ==================================================
cols_update = [
    "id",
    "horarios_norm",
    "rating_final",
    "reviews_final",
    "price_num",
    "maps_url_final",
    "address_norm"
]
df_actualizados = df_actualizados[cols_update]

# ==================================================
# 3. Merge con el dataset maestro
# ==================================================
df_merged = df_modelo.merge(df_actualizados, on="id", how="left", suffixes=("", "_nuevo"))

# ==================================================
# 4. Sustituir valores antiguos por los nuevos (los nuevos pisan a los viejos)
# ==================================================
for col in ["horarios_norm", "rating_final", "reviews_final", "price_num", "maps_url_final", "address_norm"]:
    if f"{col}_nuevo" in df_merged.columns:
        df_merged[col] = df_merged[f"{col}_nuevo"].fillna(df_merged[col])
        df_merged = df_merged.drop(columns=[f"{col}_nuevo"])

# ==================================================
# 5. Guardar resultados
# ==================================================
df_merged.to_json("restaurantes_modelo_actualizado.json", orient="records", force_ascii=False, indent=2)
df_merged.to_excel("restaurantes_modelo_actualizado.xlsx", index=False)

print("‚úÖ Dataset maestro actualizado con horarios, ratings, rese√±as, precios, URL y direcciones")

"""### Carga del dataset enriquecido y detecci√≥n de duplicados de nuevo
Se carga el dataset actualizado y se identifican de nuevo los restaurantes con la **misma direcci√≥n** o con la misma **direcci√≥n + nombre simplificado**.  
Estos duplicados se exportan a Excel y JSON para su revisi√≥n.

"""

# 2. Cargar dataset enriquecido
# ============================================
import numpy as np
import pandas as pd
df = pd.read_json("restaurantes_modelo_actualizado.json")
print("‚úÖ Dataset cargado:", df.shape)
df.head(3)

import pandas as pd

# --- Direcciones repetidas (excluyendo nulas) ---
duplicados_direccion = (
    df[df["address_norm"].notna()]
    .loc[lambda d: d.duplicated("address_norm", keep=False)]
)

print("üìç Restaurantes con la misma direcci√≥n (excluyendo nulas):", duplicados_direccion.shape[0])

if not duplicados_direccion.empty:
    # Mostrar en notebook (si procede)
    display(
        duplicados_direccion
        .sort_values("address_norm")[[
            "nombre_final",
            "ciudad",
            "address_norm",
            "rating_final",
            "reviews_final",
            "maps_url_final"
        ]]
    )

    # Guardar a JSON
    duplicados_direccion.to_json("restaurantes_direcciones_duplicadas.json", orient="records", force_ascii=False, indent=2)

    # Guardar a Excel
    duplicados_direccion.to_excel("restaurantes_direcciones_duplicadas.xlsx", index=False)

    print("‚úÖ Archivos guardados: restaurantes_direcciones_duplicadas.json y .xlsx")

"""### Deduplicaci√≥n por direcci√≥n + nombre simplificado

Gracias al **scraping con la API de Google Places**, los duplicados por **direcciones coincidentes** se redujeron aproximadamente a la mitad.  
Sin embargo, a√∫n persisten casos donde la misma direcci√≥n aparece con **nombres escritos de forma diferente** (ej. tildes, abreviaturas o palabras accesorias).  

Para detectar estos **duplicados ocultos**, se genera un campo de **nombre simplificado** y se combina con la direcci√≥n normalizada.  
De cada grupo se conserva la fila con **mayor score**, eliminando las dem√°s.  

El resultado se guarda en:  
- `restaurantes_deduplicados.xlsx`  
- `restaurantes_deduplicados.json`  

"""

!pip install unidecode

import pandas as pd
import numpy as np
import unidecode, re

# --- 1. Simplificar nombres ---
STOPWORDS_NOMBRES = {
    "bar","restaurante","meson","mes√≥n","parrillada","asador","taperia","taper√≠a",
    "pulperia","pulper√≠a","vinoteca","taberna","cerveceria","cervecer√≠a","cafeteria",
    "cafeter√≠a","churrasqueria","churrasquer√≠a","casa","de","comidas","o","el","la",
    "los","las","del","dos","da","do","das","a","en","y","e"
}

def simplificar_nombre(nombre: str) -> str:
    if not isinstance(nombre, str):
        return ""
    n = unidecode.unidecode(nombre.lower())
    n = re.sub(r"[^a-z0-9\s]", " ", n)
    tokens = [t for t in n.split() if t not in STOPWORDS_NOMBRES]
    return " ".join(tokens).strip()

df["nombre_simplificado"] = df["nombre_final_norm"].apply(simplificar_nombre)

# --- 2. Detectar duplicados por direcci√≥n + nombre_simplificado ---
duplicados = (
    df[df["address_norm"].notna()]
    .loc[lambda d: d.duplicated(["address_norm", "nombre_simplificado"], keep=False)]
)

print("üìç Posibles duplicados por direcci√≥n + nombre simplificado:", duplicados.shape[0])


# --- 4. Deduplicaci√≥n: quedarnos con la fila de mayor score ---
df_dedup = (
    df.sort_values("score", ascending=False)
      .drop_duplicates(subset=["address_norm", "nombre_simplificado"], keep="first")
)

print(f"‚úÖ Antes: {df.shape[0]} registros")
print(f"‚úÖ Despu√©s de deduplicar: {df_dedup.shape[0]} registros")
print(f"üóëÔ∏è Eliminados: {df.shape[0] - df_dedup.shape[0]} duplicados")

# --- 5. Guardar resultados ---
df_dedup.to_excel("restaurantes_deduplicados.xlsx", index=False)
df_dedup.to_json("restaurantes_deduplicados.json", orient="records", force_ascii=False, indent=2)

print("‚úÖ Archivos guardados con duplicados eliminados (por direcci√≥n+nombre simplificado)")

# --- Chequeo de duplicados SOLO por direcci√≥n en el dataset deduplicado ---
duplicados_solo_dir = (
    df_dedup[df_dedup["address_norm"].notna()]
    .loc[lambda d: d.duplicated("address_norm", keep=False)]
)

print("üìç Duplicados encontrados solo por direcci√≥n (post-deduplicaci√≥n):", duplicados_solo_dir.shape[0])

if duplicados_solo_dir.empty:
    print("‚úÖ No quedan duplicados por direcci√≥n")
else:
    display(
        duplicados_solo_dir
        .sort_values("address_norm")[
            ["id","nombre_final", "nombre_final_norm", "nombre_simplificado",
             "ciudad", "address_norm", "rating_final", "reviews_final", "maps_url_final"]
        ]
    )

if not duplicados_solo_dir.empty:
    # Ordenamos por direcci√≥n y nombre
    duplicados_solo_dir_sorted = duplicados_solo_dir.sort_values(
        ["address_norm", "nombre_final_norm"]
    )

    # Reordenamos columnas: ponemos address_norm justo despu√©s de nombre_final_norm
    cols = [
        "id",
        "nombre_final",
        "nombre_final_norm",
        "address_norm",   # üëà aqu√≠ movemos address_norm
        "nombre_simplificado",
        "ciudad",
        "rating_final",
        "reviews_final",
        "maps_url_final"
    ]

    # Nos aseguramos de que no falte ninguna columna extra
    cols = [c for c in cols if c in duplicados_solo_dir_sorted.columns]
    duplicados_solo_dir_sorted = duplicados_solo_dir_sorted[cols]

    # Guardar en Excel y JSON
    duplicados_solo_dir_sorted.to_excel("restaurantes_duplicados_solo_direccion.xlsx", index=False)
    duplicados_solo_dir_sorted.to_json(
        "restaurantes_duplicados_solo_direccion.json",
        orient="records", force_ascii=False, indent=2
    )

    print("‚úÖ Archivos guardados con address_norm al lado de nombre_final_norm")

"""###  Eliminaci√≥n de duplicados por direcci√≥n + URL
Se identifican restaurantes con la **misma direcci√≥n y la misma URL de Google Maps**.  
De cada grupo, se conserva √∫nicamente el de **mayor score** (m√°s informaci√≥n).  
El resultado se guarda en `restaurantes_deduplicados_final.json / .xlsx`.

"""

# --- 3. Deduplicaci√≥n adicional por direcci√≥n + url---
df_dedup2 = (
    df_dedup.sort_values("score", ascending=False)
            .drop_duplicates(
                subset=["address_norm", "maps_url_final"],
                keep="first"
            )
)

print(f"‚úÖ Antes (dedup direcci√≥n+nombre simplificado): {df_dedup.shape[0]} registros")
print(f"‚úÖ Despu√©s (dedup direcci√≥n+url): {df_dedup2.shape[0]} registros")
print(f"üóëÔ∏è Eliminados: {df_dedup.shape[0] - df_dedup2.shape[0]} duplicados")

# --- Guardar resultados finales ---
df_dedup2.to_excel("restaurantes_deduplicados_final.xlsx", index=False)
df_dedup2.to_json("restaurantes_deduplicados_final.json", orient="records", force_ascii=False, indent=2)

print("‚úÖ Archivos guardados con duplicados eliminados (direcci√≥n + url, quedando el de mayor score)")

"""### Comprobaci√≥n de duplicados por coordenadas
A continuaci√≥n se buscan restaurantes con **coordenadas id√©nticas**.  
Estos casos suelen ser errores de carga o variaciones m√≠nimas.  
Se exporta un listado para **revisi√≥n manual**, donde se eliminan duplicados y se corrigen coordenadas err√≥neas.

"""

import pandas as pd

# --- Cargar dataset final ---
df_final = pd.read_json("restaurantes_deduplicados_final.json")

# --- Chequear duplicados por coordenadas ---
duplicados_coords = (
    df_final[df_final["latitude"].notna() & df_final["longitude"].notna()]
    .loc[lambda d: d.duplicated(subset=["latitude", "longitude"], keep=False)]
)

print("üìç Restaurantes con las mismas coordenadas:", duplicados_coords.shape[0])

if duplicados_coords.empty:
    print("‚úÖ No quedan duplicados por coordenadas")
else:
    display(
        duplicados_coords.sort_values(["latitude", "longitude"])[
            ["id", "nombre_final", "ciudad", "latitude", "longitude",
             "address_norm", "maps_url_final", "reviews_final", "rating_final", "score"]
        ]
    )
    # Exportar reporte si quieres revisar manualmente
    duplicados_coords.to_excel("restaurantes_duplicados_coordenadas.xlsx", index=False)
    duplicados_coords.to_json(
        "restaurantes_duplicados_coordenadas.json",
        orient="records", force_ascii=False, indent=2
    )
    print("‚úÖ Reporte guardado en .xlsx y .json")

"""### Correcciones manuales de coordenadas y nombres
En la revisi√≥n manual:
- Se eliminaron registros redundantes.  
- Se ajustaron coordenadas copiadas de Google Maps.  
- Se corrigieron nombres en casos especiales (ej. `GuoC`).  
El dataset corregido se guarda como `restaurantes_corregido.json / .xlsx`.

"""

import pandas as pd
# --- IDs a eliminar ---
ids_eliminar = [26458289, 23955060, 21296736]

df_final = df_final[~df_final["id"].isin(ids_eliminar)]

# --- Correcciones de coordenadas y nombre ---
correcciones = {
    22998331: (42.20043082149737, -8.763368050246518),
    23384643: (42.236794751507304, -8.71192846188267),
    23635106: (42.235939214264796, -8.710488873521271),
    24977532: (42.23464157727288, -8.713784306297807),
    25215208: (42.22524294657433, -8.754397260030393),
    25265214: (42.22264229329068, -8.729944033047449),  # adem√°s renombrar
    26241271: (42.23960134514809, -8.7253473348993),
    24040086: (42.213530211636105, -8.73916761770346),
    24969080: (42.13228230107539, -8.797939146545058),
    26877302: (42.35132191792576, -7.876107146530363),
    26356036: (42.28560056587428, -7.812455524357113),
    26828916: (42.337734874285125, -7.8631460042035926),
    25009470: (42.37421503558472, -7.881828461873403),
    32790543: (42.237154077445524, -8.721838990718771),
    28107368: (42.22512173661106, -8.741037888866556),
    32849263: (42.433860422664296, -8.646032190705542),
    27290808: (42.43311658724745, -8.639562548377878),
    25172601: (42.87609796933484, -8.543760390675637),
    24049216: (42.87785293880093, -8.54559073300324),
    27170115: (42.87304717331995, -8.546953019511967),
    28034645: (43.01065972082039, -7.558811019502618),
    27714391: (43.01145341520436, -7.551494634847178),
    27984015: (43.34497748147332, -8.427825607841244),
    30224114: (43.344782421761074, -8.427696861807513),
    12736330: (43.345928202769244, -8.40750008879058),
    27365561: (43.345957914048256, -8.407468946462867),
    22945303: (43.35304998455128, -8.401226105987664),
    21402455: (43.35306338992878, -8.401238061806959),
    19701877: (42.969961776120584, -8.439803475324654),
    13867294: (42.83858480594931, -8.900842957672495),
    27332048: (43.35604479135903, -8.403721361806738),
    23773963: (43.36553916597146, -8.407422448314533),
    23998380: (43.335806845985516, -8.37816199064426),
    24149986: (43.36971595132097, -8.40036099064197),
    26791745: (43.37462074970299, -8.397398746460889),
    24186082: (43.36893616412605, -8.403403432969696),
    26917457: (43.368361562392174, -8.403901004133594),
    12861645: (43.129600683919975, -9.183763833666676),
    14783458: (43.37696732629671, -8.406068886935415),
}

# Aplicar correcciones
for rid, (lat, lon) in correcciones.items():
    df_final.loc[df_final["id"] == rid, ["latitude", "longitude"]] = (lat, lon)

# Cambiar nombre GuoC (en ambas columnas)
df_final.loc[df_final["id"] == 25265214, "nombre_final"] = "GuoC"
df_final.loc[df_final["id"] == 25265214, "nombre_final_norm"] = "guoc"


# --- Guardar dataset corregido ---
df_final.to_json("restaurantes_corregido.json", orient="records", force_ascii=False, indent=2)
df_final.to_excel("restaurantes_corregido.xlsx", index=False)

print("‚úÖ Dataset actualizado con coordenadas corregidas, eliminados y renombrados")

"""### Validaci√≥n ciudad‚Äìcoordenadas
Se valid√≥ que las **coordenadas coincidan con la ciudad declarada**.  
Esto se comprob√≥ mediante bounding boxes aproximados por ciudad (A Coru√±a, Vigo, Santiago, Lugo, Ourense, Pontevedra).  
Los registros inconsistentes se detectaron, exportaron y **se corrigieron manualmente**, quedando guardados en `restaurantes_corregido_ciudad.json / .xlsx`.

"""

import pandas as pd

# --- Cargar dataset corregido ---
df_corr = pd.read_json("restaurantes_corregido.json")

# --- Bounding boxes aproximados por ciudad ---
ciudades_bbox = {
    "a coru√±a":     (43.32, 43.39, -8.45, -8.35),
    "vigo":         (42.18, 42.27, -8.80, -8.65),
    "santiago":     (42.84, 42.89, -8.58, -8.52),
    "lugo":         (42.98, 43.05, -7.60, -7.52),
    "ourense":      (42.31, 42.36, -7.91, -7.83),
    "pontevedra":   (42.41, 42.45, -8.66, -8.62),
}

# --- Funci√≥n para asignar ciudad desde coordenadas ---
def ciudad_bbox(lat, lon):
    for ciudad, (lat_min, lat_max, lon_min, lon_max) in ciudades_bbox.items():
        if lat_min <= lat <= lat_max and lon_min <= lon <= lon_max:
            return ciudad
    return None

# --- Detectar ciudad aproximada ---
df_corr["ciudad_detectada"] = df_corr.apply(
    lambda row: ciudad_bbox(row["latitude"], row["longitude"])
    if pd.notnull(row["latitude"]) and pd.notnull(row["longitude"]) else None,
    axis=1
)

# --- Restaurantes con discrepancia ---
df_mismatch = df_corr[
    (df_corr["ciudad_detectada"].notna()) &
    (df_corr["ciudad"].str.lower() != df_corr["ciudad_detectada"].str.lower())
]

print("üìç Restaurantes con discrepancia entre ciudad y coordenadas:", df_mismatch.shape[0])

if df_mismatch.empty:
    print("‚úÖ Todas las coordenadas coinciden con la ciudad declarada (seg√∫n bounding box)")
else:
    display(
        df_mismatch[
            ["id", "nombre_final", "ciudad", "ciudad_detectada",
             "latitude", "longitude", "address_norm", "maps_url_final"]
        ]
    )
    df_mismatch.to_excel("restaurantes_ciudad_incorrecta_bbox.xlsx", index=False)
    df_mismatch.to_json(
        "restaurantes_ciudad_incorrecta_bbox.json", orient="records", force_ascii=False, indent=2
    )
    print("‚úÖ Reporte guardado en .xlsx y .json")

import pandas as pd

# --- Cargar dataset corregido ---
df = pd.read_json("restaurantes_corregido.json")

# === 1. IDs a eliminar (los largos reales) ===
ids_eliminar = [27370831, 23840920, 27971134, 32790543, 21214147, 21242100]
df = df[~df["id"].isin(ids_eliminar)]

# === 2. Actualizaciones de ciudad / direcci√≥n / coordenadas ===
cambios = {
    5970183:  {"ciudad": "santiago", "address_norm": "R√∫a de Vista Alegre, 57, 15705 Santiago de Compostela, A Coru√±a"},
    7302780:  {"ciudad": "vigo"},  # La Mafia se sienta a la mesa
    2662876:  {"ciudad": "a coru√±a"},  # El Olivo
    4259033:  {"ciudad": "a coru√±a"},  # 100 Montaditos
    3444108:  {"ciudad": "santiago"},  # Bar Piccolo
    29009772: {"ciudad": "vigo", "address_norm": "R√∫a do Doutor Cadaval, 32, Santiago de Vigo, 36202 Vigo, Pontevedra"},
    13009067: {"latitude": 42.873767841799996, "longitude": -8.54167054232771},
    26865382: {"ciudad": "vigo"},
    27338806: {"ciudad": "a coru√±a"},
    21243828: {"ciudad": "santiago", "address_norm": "R√∫a de Santa Marta de Arriba, 1, 15706 Santiago de Compostela, A Coru√±a"},
    10356721: {"ciudad": "vigo"},
    14001350: {"ciudad": "santiago"},
    6613337:  {"ciudad": "vigo"},
    27533613: {"ciudad": "vigo"},
    11712341: {"ciudad": "santiago"},
    5971636:  {"ciudad": "a coru√±a"},
    10465755: {"ciudad": "vigo"},
    14010000: {"ciudad": "a coru√±a"},
    10792420: {"ciudad": "vigo"},
    13831693: {"ciudad": "ourense"},
    27119132: {"ciudad": "santiago", "latitude": 42.84629301642788, "longitude": -8.57675006235653},
    13355100: {"ciudad": "pontevedra"},
    28107368: {"ciudad": "vigo"},
    19981751: {"ciudad": "vigo"},
    7644196:  {"ciudad": "santiago"},
    27001153: {"ciudad": "lugo"},
    27270505: {"ciudad": "ourense"},
}

# Aplicar los cambios
for rid, cambios_r in cambios.items():
    for campo, valor in cambios_r.items():
        df.loc[df["id"] == rid, campo] = valor

# === 3. Guardar resultados corregidos ===
df.to_json("restaurantes_modelo_final.json", orient="records", force_ascii=False, indent=2)
df.to_excel("restaurantes_modelo final.xlsx", index=False)

print("‚úÖ Dataset actualizado y guardado como restaurantes_modelo_final.json / .xlsx")

# --- Direcciones repetidas (excluyendo nulas) ---
duplicados_direccion = (
    df[df["address_norm"].notna()]
    .loc[df["address_norm"].notna()]
    .loc[lambda d: d.duplicated("address_norm", keep=False)]
)

print("üìç Restaurantes con la misma direcci√≥n (excluyendo nulas):", duplicados_direccion.shape[0])
if not duplicados_direccion.empty:
    display(
        duplicados_direccion
        .sort_values("address_norm")[["nombre_final", "ciudad", "address_norm","rating_final", "reviews_final", "maps_url_final"]]
    )

# --- Coordenadas repetidas (excluyendo nulas) ---
duplicados_coords = (
    df[df["latitude"].notna() & df["longitude"].notna()]
    .loc[lambda d: d.duplicated(["latitude", "longitude"], keep=False)]
)

print("üåç Restaurantes con las mismas coordenadas (excluyendo nulas):", duplicados_coords.shape[0])
if not duplicados_coords.empty:
    display(
        duplicados_coords
        .sort_values(["latitude", "longitude"])[["nombre_final_norm", "ciudad", "latitude", "longitude", "rating_final", "reviews_final", "maps_url_final"]]
    )

"""## Gr√°ficos"""

import pandas as pd
# --- Cargar dataset final ---
df = pd.read_json("restaurantes_modelo_final.json")

"""### Gr√°fica 1: Ratings vs Rese√±as (validaci√≥n de outliers)

Este gr√°fico de dispersi√≥n muestra la relaci√≥n entre el n√∫mero de rese√±as (en escala logar√≠tmica) y el rating final de cada restaurante.   

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(df["reviews_final"], df["rating_final"], alpha=0.4, s=20, color="teal")

plt.xscale("log")
plt.xlabel("N¬∫ rese√±as (log)")
plt.ylabel("Rating final")
plt.title("Ratings vs Rese√±as (dataset corregido)")

plt.show()

"""- La mayor√≠a de restaurantes se concentran entre **3.5 y 4.5‚≠ê** con decenas o cientos de rese√±as.

### Gr√°fica 2: Top categor√≠as generales de restaurantes

Esta gr√°fica muestra las **categor√≠as maestras m√°s frecuentes** en el dataset, excluyendo el bloque `Otros` y cualquier subcategor√≠a.  

De este modo se obtiene una visi√≥n clara de la **distribuci√≥n principal de tipos de cocina** en Galicia:  
- Destacan categor√≠as como *Espa√±ola*, *Italiana*, *Hamburguesas* y *Pizza*.  
- El bloque `Otros`, (supermercados, ocio, bares gen√©ricos), queda excluido para no distorsionar el an√°lisis.
"""

import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter

# --- Funci√≥n auxiliar: contar categor√≠as generales ---
def contar_categorias_generales(categorias):
    if not isinstance(categorias, list):
        return []
    generales = []
    for cat in categorias:
        if ">" in cat:
            base = cat.split(">")[0].strip()
        else:
            base = cat.strip()
        if base != "Otros":  # excluir bloque Otros
            generales.append(base)
    return generales

# Expandir todas las categor√≠as maestras en una lista
todas_categorias = df["categorias_asignadas"].dropna().apply(contar_categorias_generales)
todas_categorias_flat = [cat for sublist in todas_categorias for cat in sublist]

# Contar frecuencia
conteo = Counter(todas_categorias_flat).most_common(15)

# Pasar a DataFrame
df_top = pd.DataFrame(conteo, columns=["Categoria", "Frecuencia"])

# --- Gr√°fico ---
plt.figure(figsize=(10,6))
plt.barh(df_top["Categoria"], df_top["Frecuencia"], color="skyblue")
plt.gca().invert_yaxis()

plt.title("Top 15 categor√≠as generales de restaurantes")
plt.xlabel("N√∫mero de restaurantes")
plt.ylabel("Categor√≠a general")

plt.show()

"""- Predomina la categor√≠a **Espa√±ola**, seguida de **Veggie** y **Desayunos**.  
- Otras categor√≠as relevantes en Galicia son **Europea**, **Carnes** y **R√°pida**.  
- Las categor√≠as minoritarias como **Mexicana** o **Turca** muestran una presencia muy limitada.

### Gr√°fica 3: Distribuci√≥n de ratings por categor√≠a general

Este boxplot compara la **distribuci√≥n de ratings** entre las principales categor√≠as de restaurantes.  

- Se utilizan solo las **categor√≠as generales** (sin subcategor√≠as).  
- El bloque `Otros` queda excluido.
- El gr√°fico permite identificar cocinas con mayor dispersi√≥n en satisfacci√≥n, as√≠ como aquellas que tienden a concentrarse en puntuaciones m√°s altas o bajas.
"""

import seaborn as sns

# --- Preparar datos ---
rows = []
for _, fila in df.iterrows():
    cats = contar_categorias_generales(fila["categorias_asignadas"])
    for cat in cats:
        rows.append({"Categoria": cat, "Rating": fila["rating_final"]})

df_box = pd.DataFrame(rows)

# Filtrar categor√≠as con un n¬∫ m√≠nimo de restaurantes (ej. 30)
df_box_filtrado = df_box.groupby("Categoria").filter(lambda x: len(x) >= 30)

# --- Gr√°fico ---
plt.figure(figsize=(12,6))
sns.boxplot(data=df_box_filtrado, x="Categoria", y="Rating", palette="Set3")
plt.xticks(rotation=45, ha="right")

plt.title("Distribuci√≥n de ratings por categor√≠a general (>=30 restaurantes)")
plt.ylabel("Rating final")
plt.xlabel("Categor√≠a general")

plt.show()

"""- La mayor√≠a de categor√≠as presentan distribuciones centradas en torno a **4.2‚Äì4.5‚≠ê**.  
- Destacan **Veggie, Gourmet y Latina** con medianas m√°s altas.  
- La categor√≠a **Hamburguesas** muestra mayor dispersi√≥n y algunos valores bajos, lo que indica calidad desigual.

### Gr√°fica 4: Precio medio por categor√≠a general

Este gr√°fico muestra el **precio medio (`price_num`)** de los restaurantes en cada categor√≠a general.  

- Se utilizan √∫nicamente las **categor√≠as maestras** (sin subcategor√≠as).  
- El bloque `Otros` se excluye para evitar ruido.  
- Se incluyen categor√≠as con al menos `30 restaurantes` para asegurar representatividad.  
- Encima de cada barra se indica el valor medio del rango de precios (1 = barato, 4 = caro).
"""

# --- Preparar datos ---
rows = []
for _, fila in df.iterrows():
    cats = contar_categorias_generales(fila["categorias_asignadas"])
    for cat in cats:
        rows.append({"Categoria": cat, "Precio": fila["price_num"]})

df_precios = pd.DataFrame(rows)

# Agrupar y calcular promedio (filtrando categor√≠as con al menos 30 locales)
df_precios_mean = (
    df_precios.groupby("Categoria")
    .agg({"Precio":"mean", "Categoria":"count"})
    .rename(columns={"Categoria":"N"})
    .query("N >= 30")
    .sort_values("Precio", ascending=False)
)

# --- Gr√°fico ---
plt.figure(figsize=(12,3))
ax = sns.barplot(
    data=df_precios_mean.reset_index(),
    x="Categoria", y="Precio", palette="plasma"
)

# A√±adir etiquetas encima de las barras
for i, row in df_precios_mean.reset_index().iterrows():
    ax.text(
        i, row["Precio"] + 0.02,
        f"{row['Precio']:.2f}",
        ha="center", va="bottom", fontsize=9, fontweight="bold"
    )

plt.xticks(rotation=45, ha="right")
plt.title("Precio medio por categor√≠a general (>=30 restaurantes)")
plt.ylabel("Precio medio (1=barato, 4=caro)")
plt.xlabel("Categor√≠a general")
plt.ylim(1, 4.2)

plt.show()

"""- La categor√≠a **Gourmet (2.07)** se posiciona como la m√°s cara, seguida por **Veggie (1.83)** y **Europea (1.75)**.  
- Las cocinas **Espa√±ola (1.58)**, **Carnes (1.58)** y **Mexicana (1.56)** se ubican en un nivel intermedio.  
- Las categor√≠as **Pizza (1.28)**, **R√°pida (1.27)** y **Hamburguesas (1.25)** destacan como las m√°s econ√≥micas.  
- **Turca (1.00)** es la categor√≠a con menor nivel de precios promedio, reforzando su posici√≥n como opci√≥n de bajo coste.  
- En general, el rango de precios se concentra entre **1.0 y 2.1**, lo que refleja un mercado mayoritariamente asequible.

### Gr√°fica 5: Valoraci√≥n promedio seg√∫n rango de precio

Este gr√°fico muestra c√≥mo var√≠a la **valoraci√≥n media (`rating_final`)** en funci√≥n del rango de precios (`price_num`).  

- Se incluyen √∫nicamente los restaurantes con `price_num` no nulo.  
- La escala de precios va de **1 (m√°s barato) a 4 (m√°s caro)**.  
- Encima de cada barra se indica el valor promedio exacto.
"""

# --- Promedio de rating por rango de precio ---
df_price_rating = (
    df.dropna(subset=["price_num"])
    .groupby("price_num")["rating_final"]
    .mean()
    .reset_index()
)

# --- Gr√°fico ---
plt.figure(figsize=(8,5))
ax = sns.barplot(data=df_price_rating, x="price_num", y="rating_final", palette="mako")

# Etiquetas encima de las barras
for i, row in df_price_rating.iterrows():
    ax.text(i, row["rating_final"] + 0.02, f"{row['rating_final']:.2f}",
            ha="center", va="bottom", fontsize=9, fontweight="bold")

plt.title("Valoraci√≥n promedio seg√∫n rango de precio")
plt.xlabel("Rango de precio (1=barato, 4=caro)")
plt.ylabel("Promedio rating_final")
plt.ylim(3.5, 5.0)
plt.show()

"""- Existe una tendencia positiva: a mayor rango de precio, mayor valoraci√≥n media.  
- Los locales de precio **4 (caros)** alcanzan una media de **4.57‚≠ê**, frente a **4.25‚≠ê** en los m√°s econ√≥micos.  
- Esto sugiere que la percepci√≥n de calidad est√° asociada al rango de precios.

### Gr√°fica 6: Disponibilidad de delivery y reservas

Este gr√°fico compara la **proporci√≥n de restaurantes** que ofrecen:  

- **Delivery (a domicilio)**  
- **Reservable (reserva anticipada)**  

El objetivo es analizar c√≥mo est√°n distribuidos estos servicios en Galicia.
"""

# --- Preparar datos ---
servicios = {
    "Delivery": df["delivery"].sum(),
    "Reservable": df["reservable"].sum()
}
total = len(df)

df_servicios = pd.DataFrame({
    "Servicio": servicios.keys(),
    "Porcentaje": [v/total*100 for v in servicios.values()]
})

# --- Gr√°fico ---
plt.figure(figsize=(7,5))
ax = sns.barplot(data=df_servicios, x="Servicio", y="Porcentaje", palette="crest")

# Etiquetas encima de las barras
for i, row in df_servicios.iterrows():
    ax.text(i, row["Porcentaje"] + 0.5, f"{row['Porcentaje']:.1f}%",
            ha="center", va="bottom", fontsize=9, fontweight="bold")

plt.title("Disponibilidad de delivery y reservas (%)")
plt.ylabel("Porcentaje de restaurantes")
plt.ylim(0, 100)
plt.show()

"""- Aproximadamente **30% de los restaurantes ofrecen delivery**, mientras que un **65% son reservables**.  
- El delivery todav√≠a no est√° ampliamente extendido, mientras que la posibilidad de reserva es ya un est√°ndar en muchos locales.

### Gr√°fica 7: Promedio de valoraci√≥n final por ciudad

Este gr√°fico muestra la **valoraci√≥n promedio (`rating_final`)** de los restaurantes agrupados por ciudad.  

- Se consideran √∫nicamente las ciudades con al menos `30 restaurantes`, para asegurar representatividad.  
- El gr√°fico permite comparar percepciones de calidad gastron√≥mica entre las principales ciudades gallegas.
"""

# --- Promedio de ratings por ciudad ---
df_city_ratings = (
    df.groupby("ciudad")["rating_final"]
    .agg(["mean","count"])
    .reset_index()
    .sort_values("mean", ascending=False)
)

# --- Gr√°fico ---
plt.figure(figsize=(10,6))
ax = sns.barplot(data=df_city_ratings, x="ciudad", y="mean", palette="coolwarm")

# A√±adir etiquetas encima de cada barra (posici√≥n real de las barras)
for p in ax.patches:
    altura = p.get_height()  # valor (rating medio)
    ax.text(
        p.get_x() + p.get_width() / 2,  # centro horizontal de la barra
        altura + 0.02,                  # un poco por encima de la barra
        f"{altura:.2f}",                # formato num√©rico
        ha="center", va="bottom", fontsize=10, fontweight="bold")


plt.title("Promedio de valoraci√≥n final por ciudad")
plt.ylabel("Promedio rating_final")
plt.xlabel("Ciudad")
plt.xticks(rotation=45, ha="right")
plt.ylim(3.5, 5.0)
plt.show()

"""- Las valoraciones son bastante homog√©neas en todas las ciudades gallegas, situ√°ndose entre **4.26 y 4.30‚≠ê**.  
- **Ourense**, **Pontevedra**, **Santiago** y **A Coru√±a** (todas con 4.30‚≠ê)  se sit√∫an como las ciudades con puntuaciones ligeramente m√°s altas, lo que sugiere una percepci√≥n positiva de su oferta gastron√≥mica.  
- **Lugo (4.26‚≠ê)** y **Vigo (4.26‚≠ê)** est√°n en la parte baja, aunque la diferencia con el resto es m√≠nima.  
- En general, no hay grandes diferencias de calidad percibida entre ciudades, lo que refleja un ecosistema gastron√≥mico consistente en toda Galicia.

"""

import pandas as pd
from collections import defaultdict
import ast

# --- Cargar dataset final ---
df = pd.read_json("restaurantes_modelo_final.json")

# --- Asegurar que 'categorias_asignadas' es lista ---
def safe_eval(val):
    if isinstance(val, str):
        try:
            return ast.literal_eval(val)
        except:
            return []
    elif isinstance(val, list):
        return val
    return []

df["categorias_asignadas"] = df["categorias_asignadas"].apply(safe_eval)

# --- Detectar todas las subcategor√≠as ---
subcats = defaultdict(set)
for categorias in df["categorias_asignadas"].dropna():
    if isinstance(categorias, list):
        for c in categorias:
            if ">" in c:
                maestro, sub = [p.strip().lower() for p in c.split(">", 1)]
                subcats[maestro].add(sub)

# --- Crear columnas binarias subcat_* ---
for maestro, subs in subcats.items():
    for sub in subs:
        col = f"subcat_{maestro}_{sub}".replace(" ", "_")
        df[col] = df["categorias_asignadas"].apply(
            lambda cats: int(any((">" in c and c.strip().lower() == f"{maestro} > {sub}") for c in cats))
            if isinstance(cats, list) else 0
        )

print("‚úÖ Subcategor√≠as a√±adidas:", [c for c in df.columns if c.startswith("subcat_")])

# --- Guardar ---
df.to_json("restaurantes_con_subcategorias.json", orient="records", force_ascii=False, indent=2)
df.to_excel("restaurantes_con_subcategorias.xlsx", index=False)

print("üìÇ Archivos exportados: JSON + XLSX")

df.info()

"""# üìä Cap√≠tulo 6: Modelo de Recomendaci√≥n H√≠brido

## ¬øEn qu√© consiste este modelo?

En este cap√≠tulo desarrollamos un **sistema h√≠brido de recomendaci√≥n** basado en el contenido de los restaurantes.  
Dado que no disponemos de datos de usuarios individuales (necesarios para un filtrado colaborativo cl√°sico), nos centramos en un h√≠brido **dentro del enfoque content-based**, que combina varios mecanismos:

1. **üîé B√∫squeda por texto libre**  
   El usuario puede introducir consultas (*ej: ‚Äúpulpo gallego barato‚Äù*) y encontrar restaurantes relevantes a partir de descripciones, rese√±as y categor√≠as.

2. **üçî Filtros por categor√≠as, flags y subcategor√≠as**  
   El sistema permite activar filtros como *vegano, hamburguesas, italiana, gourmet*, etc. (`flag_*`), as√≠ como subcategor√≠as m√°s espec√≠ficas (*ej: `subcat_espa√±ola_pulpo`, `subcat_veggie_sin_gluten`*).

3. **üìä Ordenaci√≥n personalizada**  
   Los resultados pueden ordenarse por criterios como `rating_final`, `reviews_final`, `price_num` o la m√©trica interna `score`.

4. **üìç Proximidad geogr√°fica y horarios**  
   Adem√°s, se incorporan filtros por distancia (Haversine o rutas reales con ORS) y disponibilidad horaria (`horarios_norm`), para que el usuario vea solo restaurantes cercanos y abiertos.

Este **enfoque mixto** combina b√∫squeda sem√°ntica, filtrado, ordenaci√≥n y proximidad, ofreciendo una experiencia flexible y realista de exploraci√≥n gastron√≥mica.


"""

# --- Paso 1: Cargar dataset y librer√≠as ---

# Librer√≠as base
import pandas as pd
import numpy as np
import ast

# NLP / Similaridad
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Visualizaci√≥n opcional
import matplotlib.pyplot as plt

# Cargar dataset final
df = pd.read_json("restaurantes_enriquecido.json")

print("‚úÖ Dataset cargado:", df.shape)
df.head(3)
df.info()

# --- Normalizar columnas que mezclan listas y strings ---
def normalizar_listas(df, cols):
    for col in cols:
        if col in df.columns:
            df[col] = df[col].apply(lambda x:
                ", ".join(ast.literal_eval(x)) if isinstance(x, str) and x.startswith("[") and x.endswith("]")
                else (", ".join(x) if isinstance(x, list) else str(x))
            )
    return df

cols_listas = [
    "cuisines_traducidas", "platos_detectados", "categorias_asignadas",
    "review_keywords_es", "diets_es", "meal_types_es",
    "dining_options_es", "top_tags_es"
]

df = normalizar_listas(df, cols_listas)

"""## Preprocesamiento para b√∫squeda por texto libre (TF-IDF)

El objetivo es permitir al usuario buscar restaurantes introduciendo un texto libre (*ej: ‚Äúpulpo gallego barato‚Äù*).  

Para ello:  
1. Creamos un **campo combinado de texto** con varias columnas descriptivas (`nombre_final`, `description_es`, `review_keywords_es`, `cuisines_traducidas`, etc.).  
2. Aplicamos **TF-IDF (Term Frequency - Inverse Document Frequency)** para vectorizar ese texto.  
   - Cada restaurante se convierte en un vector num√©rico que representa sus palabras clave.  
3. Calculamos similitudes de coseno entre la consulta del usuario y los restaurantes.  
   - Cuanto m√°s alto el valor, m√°s relevante es el restaurante respecto a la b√∫squeda.  

"""

# 1. Crear un campo combinado de texto m√°s completo
def combine_text(row):
    parts = []
    for col in [
        "nombre_final", "description_es", "review_keywords_es",
        "cuisines_traducidas", "categorias_asignadas", "diets_es",
        "top_tags_es", "meal_types_es", "platos_detectados"
    ]:
        val = row.get(col, None)

        # Evitar valores nulos, None o "None"
        if val is None or str(val).lower() in ["none", "nan", "null", ""]:
            continue

        if isinstance(val, list):
            parts.extend([str(x).strip() for x in val if x and str(x).lower() not in ["none", "nan", "null"]])
        elif isinstance(val, str):
            val_clean = val.strip()
            if val_clean.lower() not in ["none", "nan", "null", ""]:
                parts.append(val_clean)

    return " ".join(parts)

df["texto_combinado"] = df.apply(combine_text, axis=1)
df["texto_combinado"] = df["texto_combinado"].fillna("").astype(str)


print("üìë Ejemplo de texto combinado:")
print(df["texto_combinado"].iloc[0][:500])

# 2. Vectorizaci√≥n TF-IDF
import nltk
from nltk.corpus import stopwords

# Descargar lista de palabras vac√≠as en espa√±ol
nltk.download("stopwords")
stop_words_es = stopwords.words("spanish")

# Crear el vectorizador con stopwords en espa√±ol
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words=stop_words_es)

# Crear matriz TF-IDF de todos los restaurantes
tfidf_matrix = vectorizer.fit_transform(df["texto_combinado"])

print("‚úÖ Matriz TF-IDF creada:", tfidf_matrix.shape)


# 3. Funci√≥n de b√∫squeda
from sklearn.metrics.pairwise import cosine_similarity

def buscar_texto(query, top_n=5):
    # Convertir la consulta del usuario en vector
    query_vec = vectorizer.transform([query])

    # Calcular similitud de coseno entre la consulta y todos los restaurantes
    sim = cosine_similarity(query_vec, tfidf_matrix).flatten()

    # Ordenar de mayor a menor similitud
    indices = sim.argsort()[::-1][:top_n]

    # Recuperar resultados
    resultados = df.iloc[indices][["nombre_final", "ciudad", "rating_final", "price_num", "maps_url_final"]].copy()
    resultados["similaridad"] = sim[indices]

    return resultados

# Ejemplo de prueba
buscar_texto("pulpo barato ", top_n=5)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

def explicar_resultados_tfidf(query, top_n=5):
    """
    Explica por qu√© aparecen los restaurantes en los resultados TF-IDF:
    - Muestra coincidencias literales con la query
    - Indica los scores de similitud
    - Visualiza las palabras m√°s relevantes del top-1
    """
    # Vectorizar la consulta
    query_vec = vectorizer.transform([query])
    sim = cosine_similarity(query_vec, tfidf_matrix).flatten()

    # Obtener los √≠ndices de los restaurantes m√°s similares
    indices = sim.argsort()[::-1][:top_n]
    resultados = df.iloc[indices][["nombre_final", "ciudad", "rating_final"]].copy()
    resultados["similaridad"] = sim[indices]

    print(f"\nüîé **Consulta:** {query}")
    print("üìä Resultados m√°s similares por TF-IDF:\n")

    for i, idx in enumerate(indices):
        nombre = df.iloc[idx]["nombre_final"]
        ciudad = df.iloc[idx]["ciudad"]
        score = sim[idx]
        texto = df.iloc[idx]["texto_combinado"].lower()
        coincidencias = [w for w in query.lower().split() if w in texto]
        print(f"{i+1}. {nombre} ({ciudad}) ‚Üí similitud: {score:.4f}")
        if coincidencias:
            print(f"   üí¨ Coincidencias literales: {', '.join(coincidencias)}")
        else:
            print("   ‚öôÔ∏è Coincidencia contextual (no literal)")
        print()

    # --- Visualizaci√≥n de palabras m√°s relevantes del top-1 ---
    idx_top = indices[0]
    feature_names = np.array(vectorizer.get_feature_names_out())
    tfidf_vector = tfidf_matrix[idx_top].toarray().flatten()
    top_indices = tfidf_vector.argsort()[::-1][:10]
    words = feature_names[top_indices]
    scores = tfidf_vector[top_indices]

    plt.figure(figsize=(7,4))
    plt.barh(words[::-1], scores[::-1])
    plt.title(f"üî† Palabras con mayor peso TF-IDF en '{df.iloc[idx_top]['nombre_final']}'")
    plt.xlabel("Peso TF-IDF")
    plt.tight_layout()
    plt.show()

    return resultados

# Ejemplo: b√∫squeda por texto libre
resultados_tfidf = explicar_resultados_tfidf("pulpo barato", top_n=5)
display(resultados_tfidf)

"""## B√∫squeda h√≠brida: TF-IDF + Embeddings sem√°nticos

En este paso integramos dos enfoques de b√∫squeda:

1. **TF-IDF**  
   - Basado en frecuencia de palabras.  
   - Muy r√°pido y explicable.  

2. **SentenceTransformer (paraphrase-multilingual-MiniLM-L12-v2)**  
   - Convierte frases en embeddings vectoriales.  
   - Captura sin√≥nimos y similitud sem√°ntica.  

3. **H√≠brido**  
   - Calculamos similitud por ambos m√©todos.  
   - Combinamos los scores con un peso configurable (ej. 0.5 cada uno).  
   - Devolvemos los resultados ordenados por score h√≠brido.

"""

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# --- Cargar modelo SBERT (puede tardar un poco la primera vez) ---
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# --- Precalcular embeddings de todos los restaurantes ---
embeddings = model.encode(df["texto_combinado"].tolist(), show_progress_bar=True)
embeddings = np.array(embeddings)

def buscar_hibrido(query, top_n=10, alpha=0.5, ciudad=None, df_base=None):
    """
    B√∫squeda h√≠brida TF-IDF + embeddings sem√°nticos (E5 o SBERT).
    Si se pasa df_base, solo busca dentro de ese subconjunto filtrado.
    """

    # --- Seleccionar base de datos ---
    if df_base is not None:
        # Mantener los √≠ndices del df global para alinear los scores
        df_local = df_base.copy()
        idx_local = df.index.isin(df_base.index)
    else:
        df_local = df.copy()
        idx_local = np.ones(len(df), dtype=bool)

    # --- TF-IDF ---
    query_vec_tfidf = vectorizer.transform([query])
    sim_tfidf_global = cosine_similarity(query_vec_tfidf, tfidf_matrix).flatten()

    # --- Embeddings sem√°nticos ---
    query_vec_sbert = model.encode([query])
    sim_sbert_global = cosine_similarity(query_vec_sbert, embeddings).flatten()

    # --- Combinar scores globales ---
    sim_total_global = alpha * sim_tfidf_global + (1 - alpha) * sim_sbert_global

    # --- Filtrar solo los √≠ndices locales ---
    sim_tfidf = sim_tfidf_global[idx_local]
    sim_sbert = sim_sbert_global[idx_local]
    sim_total = sim_total_global[idx_local]

    # --- Asignar scores al df_local ---
    df_local = df_local.copy()
    df_local["score_tfidf"] = sim_tfidf
    df_local["score_sbert"] = sim_sbert
    df_local["score_hibrido"] = sim_total

    # --- Filtro adicional por ciudad si aplica ---
    if ciudad:
        df_local = df_local[df_local["ciudad"].str.lower() == ciudad.lower()]

    # --- Orden final ---
    df_local = df_local.sort_values("score_hibrido", ascending=False)

    return df_local[[
        "nombre_final", "ciudad",
        "score_hibrido", "score_tfidf", "score_sbert","reviews_final",
         "latitude", "longitude", "horarios_norm"
    ]].head(top_n)

# --- Ejemplo de uso ---
print("üîé Buscando 'cachopo y sidra' en todo el dataset con modelo MiniLM:")
resultados = buscar_hibrido("cachopo y sidra ", top_n=10, alpha=0.65)
display(resultados)

from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# --- Cargar modelo multiling√ºe avanzado ---
model = SentenceTransformer("BAAI/bge-m3")

# --- Normalizaci√≥n opcional (recomendado para bge-m3) ---
def normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / np.clip(norms, a_min=1e-10, a_max=None)

# --- Precalcular embeddings normalizados de todos los restaurantes ---
print("‚öôÔ∏è Calculando embeddings con BAAI/bge-m3 (puede tardar unos minutos la primera vez)...")
embeddings = model.encode(
    df["texto_combinado"].tolist(),
    normalize_embeddings=True,  # bge-m3 soporta normalizaci√≥n interna
    show_progress_bar=True
)
embeddings = np.array(embeddings)

# --- Nueva funci√≥n h√≠brida adaptada ---
def buscar_hibrido_avanzado(query, top_n=10, alpha=0.5, ciudad=None):
    # TF-IDF
    query_vec_tfidf = vectorizer.transform([query])
    sim_tfidf = cosine_similarity(query_vec_tfidf, tfidf_matrix).flatten()

    # Embeddings con bge-m3
    query_vec_sbert = model.encode([query], normalize_embeddings=True)
    sim_sbert = cosine_similarity(query_vec_sbert, embeddings).flatten()

    # Combinar scores (ajustando peso: bge-m3 suele ser m√°s fuerte)
    sim_total = alpha * sim_tfidf + (1 - alpha) * sim_sbert

    resultados = df.copy()
    resultados["score_tfidf"] = sim_tfidf
    resultados["score_sbert"] = sim_sbert
    resultados["score_hibrido"] = sim_total

    # Filtro por ciudad si aplica
    if ciudad:
        resultados = resultados[resultados["ciudad"].str.lower() == ciudad.lower()]

    # Orden por score h√≠brido final
    resultados = resultados.sort_values("score_hibrido", ascending=False)

    return resultados[[
        "nombre_final", "ciudad",
        "score_hibrido", "score_tfidf", "score_sbert",
        "reviews_final", "maps_url_final", "latitude", "longitude"
    ]].head(top_n)

# --- Nueva funci√≥n h√≠brida adaptada ---
def buscar_hibrido_avanzado(query, top_n=10, alpha=0.5, ciudad=None):
    # TF-IDF
    query_vec_tfidf = vectorizer.transform([query])
    sim_tfidf = cosine_similarity(query_vec_tfidf, tfidf_matrix).flatten()

    # Embeddings con bge-m3
    query_vec_sbert = model.encode([query], normalize_embeddings=True)
    sim_sbert = cosine_similarity(query_vec_sbert, embeddings).flatten()

    # Combinar scores (ajustando peso: bge-m3 suele ser m√°s fuerte)
    sim_total = alpha * sim_tfidf + (1 - alpha) * sim_sbert

    resultados = df.copy()
    resultados["score_tfidf"] = sim_tfidf
    resultados["score_sbert"] = sim_sbert
    resultados["score_hibrido"] = sim_total

    # Filtro por ciudad si aplica
    if ciudad:
        resultados = resultados[resultados["ciudad"].str.lower() == ciudad.lower()]

    # Orden por score h√≠brido final
    resultados = resultados.sort_values("score_hibrido", ascending=False)

    return resultados[[
        "nombre_final", "ciudad",
        "score_hibrido", "score_tfidf", "score_sbert",
        "reviews_final", "maps_url_final", "latitude", "longitude"
    ]].head(top_n)

# --- Ejemplo de uso ---
print("üîé Buscando 'buenas vistas' en todo el dataset con modelo BGE-M3:")
resultados = buscar_hibrido_avanzado("buenas vistas", top_n=10, alpha=0)
display(resultados)

# --- BLOQUE: B√∫squeda h√≠brida con modelo multilingual-e5-base ---

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- Cargar el modelo E5 ---
# Es multiling√ºe y entiende frases completas tipo "comida gallega con sidra"
print("‚öôÔ∏è Cargando modelo multilingual-e5-base (puede tardar unos segundos)...")
model = SentenceTransformer("intfloat/multilingual-e5-base")

# --- Preparar embeddings del corpus (solo se hace una vez) ---
# A√±adimos el prefijo "passage: " como recomienda el modelo
corpus = ["passage: " + t for t in df["texto_combinado"].astype(str).tolist()]
embeddings = model.encode(corpus, normalize_embeddings=True, show_progress_bar=True)
embeddings = np.array(embeddings)
print(f"‚úÖ Embeddings calculados: {embeddings.shape}")

def buscar_hibrido_e5(query, top_n=10, alpha=0.65, ciudad=None, df_base=None):
    """
    B√∫squeda h√≠brida TF-IDF + multilingual-E5-base.
    Combina coincidencia literal (TF-IDF) y sem√°ntica (E5),
    y permite limitar la b√∫squeda a un subconjunto filtrado (df_base).
    """

    # --- Seleccionar base de datos ---
    if df_base is not None:
        df_local = df_base.copy()
        idx_local = df.index.isin(df_base.index)
    else:
        df_local = df.copy()
        idx_local = np.ones(len(df), dtype=bool)

    # --- TF-IDF ---
    query_vec_tfidf = vectorizer.transform([query])
    sim_tfidf_global = cosine_similarity(query_vec_tfidf, tfidf_matrix).flatten()

    # --- Embeddings sem√°nticos (E5) ---
    query_vec_e5 = model.encode(["query: " + query], normalize_embeddings=True)
    sim_e5_global = cosine_similarity(query_vec_e5, embeddings).flatten()

    # --- Combinar pesos ---
    sim_total_global = alpha * sim_tfidf_global + (1 - alpha) * sim_e5_global

    # --- Filtrar los √≠ndices locales (si df_base est√° definido) ---
    sim_tfidf = sim_tfidf_global[idx_local]
    sim_e5 = sim_e5_global[idx_local]
    sim_total = sim_total_global[idx_local]

    # --- Asignar scores al df_local ---
    df_local = df_local.copy()
    df_local["score_tfidf"] = sim_tfidf
    df_local["score_sbert"] = sim_e5
    df_local["score_hibrido"] = sim_total

    # --- Filtro por ciudad si aplica ---
    if ciudad:
        df_local = df_local[df_local["ciudad"].str.lower() == ciudad.lower()]

    # --- Orden final ---
    df_local = df_local.sort_values("score_hibrido", ascending=False)

    return df_local[[
        "nombre_final", "ciudad", "rating_final", "price_num",
        "score_hibrido", "score_tfidf", "score_sbert",
        "reviews_final", "maps_url_final", "latitude", "longitude"
    ]].head(top_n)

# --- Ejemplo de uso ---
print("üîé Buscando 'buenas vistas' en todo el dataset con multilingual-e5-base:")
resultados = buscar_hibrido_e5("buenas vistas", top_n=10, alpha=0)
display(resultados)

# --- Utilidades comunes ---

def limpiar_duplicados(df_in):
    """Elimina duplicados seg√∫n 'id' o (nombre_final, ciudad)."""
    return df_in.drop_duplicates(
        subset=["id"] if "id" in df_in.columns else ["nombre_final", "ciudad"]
    )

def seleccionar_columnas(df):
    """Selecciona columnas esenciales + extras si existen."""
    base = [
        "nombre_final", "ciudad", "rating_final", "price_num",
        "maps_url_final", "reviews_final", "criterio","criterio_orden"
    ]
    extras = [
        "score_hibrido", "horarios_norm", "horarios_lista",
        "latitude", "longitude", "phone", "international_phone",
        "website", "email", "featured_image", "menu_link",
        "reservation_link", "delivery", "reservable"
    ]
    for extra in extras:
        if extra in df.columns:
            base.append(extra)
    return df[base]


# --- Prioridades globales coherentes ---
PRIORIDAD_MAP = {
    "üìå Coincidencia exacta en texto": 1,     # texto + flags/subcats
    "üìå Coincide con": 2,                     # solo flags/subcats
    "üîé Coincidencia exacta en texto": 3,     # texto solo
    "‚ú® B√∫squeda sem√°ntica": 4,               # b√∫squeda sem√°ntica h√≠brida
}

def asignar_prioridad(criterio):
    """Asigna prioridad seg√∫n el tipo de coincidencia textual (parcial)."""
    for clave, valor in PRIORIDAD_MAP.items():
        if clave in str(criterio):  # <-- coincide aunque haya flag_pizza, subcat_asiatica_ramen, etc.
            return valor
    return 99

"""## B√∫squeda h√≠brida + flags/subcategor√≠as

Hasta ahora el sistema combinaba directamente TF-IDF y embeddings sem√°nticos.  
Sin embargo, en muchos casos el usuario espera que las coincidencias **m√°s literales** y con **categor√≠as activas** aparezcan primero.

Por ello se implementa una b√∫squeda jer√°rquica con niveles de prioridad:

1. **Coincidencia exacta + flags/subcategor√≠as**  
   - Se devuelven primero los restaurantes que contienen literalmente la palabra buscada en su texto (`texto_combinado`) **y** cumplen los filtros activos (`flag_*` o `subcat_*`).  

2. **Solo flags/subcategor√≠as**  
   - Si no se llena el `top_n`, se a√±aden restaurantes que cumplen los filtros activos aunque no mencionen la palabra exacta.  

3. **Coincidencia exacta en texto (sin filtros)**  
   - Si a√∫n faltan resultados, se incluyen los que contienen la palabra en `texto_combinado` aunque no tengan flag/subcategor√≠a activo.  

4. **B√∫squeda sem√°ntica h√≠brida**  
   - Como √∫ltimo recurso se completa la lista con la b√∫squeda h√≠brida TF-IDF + SBERT, que captura sin√≥nimos y expresiones similares.  

De esta manera se garantiza que los resultados m√°s relevantes para el usuario aparecen al principio, y solo si no hay suficientes se ampl√≠a el abanico hacia coincidencias m√°s difusas.



"""

def buscar_prioritario(query, flags=None, top_n=50, df_base=None):
    """
    B√∫squeda jer√°rquica prioritaria dentro del DataFrame indicado (df_base).
    Si no se pasa, usa el DataFrame global df.
    """
    if df_base is None:
        df_base = df  # fallback al global si no se pasa nada

    resultados = pd.DataFrame()
    faltan = top_n

    # --- Separar flags y subcategor√≠as ---
    subcats = [f for f in flags if f.startswith("subcat_")] if flags else []
    main_flags = [f for f in flags if f.startswith("flag_")] if flags else []

    # --- Nivel 1: Coincidencia exacta en texto + flags/subcats ---
    if query.strip() and (main_flags or subcats):
        subset = df_base.copy()
        for f in main_flags + subcats:
            if f in subset.columns:
                subset = subset[subset[f] == 1]
        patron = rf"\b{query.rstrip('s')}\w*\b"
        subset = subset[subset["texto_combinado"].astype(str).str.contains(
            patron, case=False, na=False, regex=True
        )]
        if not subset.empty:
            subset["criterio"] = f"üìå Coincidencia exacta en texto ('{query}') + {' + '.join(main_flags + subcats)}"
            resultados = pd.concat([resultados, subset])
        resultados = limpiar_duplicados(resultados)
        faltan = top_n - len(resultados)

    # --- Nivel 2: Coincidencia por flags/subcats (sin texto) ---
    if faltan > 0 and (main_flags or subcats):
        subset = df_base.copy()
        for f in main_flags + subcats:
            if f in subset.columns:
                subset = subset[subset[f] == 1]
        if not subset.empty:
            subset["criterio"] = f"üìå Coincide con {' + '.join(main_flags + subcats)}"
            resultados = pd.concat([resultados, subset])
        resultados = limpiar_duplicados(resultados)
        faltan = top_n - len(resultados)

    # --- Nivel 3: Coincidencia exacta solo en texto ---
    if faltan > 0 and query.strip():
        patron = rf"\b{query.rstrip('s')}\w*\b"
        subset = df_base[df_base["texto_combinado"].astype(str).str.contains(
            patron, case=False, na=False, regex=True
        )].copy()
        if not subset.empty:
            subset["criterio"] = f"üîé Coincidencia exacta en texto ('{query}')"
            resultados = pd.concat([resultados, subset])
        resultados = limpiar_duplicados(resultados)
        faltan = top_n - len(resultados)

    # --- Nivel 4: B√∫squeda sem√°ntica h√≠brida ---
    if faltan > 0 and query.strip():
        semanticos = buscar_hibrido(query, top_n=faltan, df_base=df_base, ciudad=None).copy()
        semanticos["criterio"] = f"‚ú® B√∫squeda sem√°ntica h√≠brida ('{query}')"
        resultados = pd.concat([resultados, semanticos])
        resultados = limpiar_duplicados(resultados)

    # --- Asignar prioridad √∫nica ---
    resultados["criterio_orden"] = resultados["criterio"].apply(asignar_prioridad)

    # --- Ordenar jer√°rquicamente ---
    resultados = resultados.sort_values(
        by=["criterio_orden", "rating_final", "reviews_final"],
        ascending=[True, False, False],
        kind="mergesort"
    )

    return seleccionar_columnas(resultados)

"""## B√∫squeda √∫nica por flags/subcategor√≠as

Adem√°s de la b√∫squeda por texto o proximidad, el sistema permite filtrar restaurantes por categor√≠as:

1. **Flags (`flag_*`)**  
   Variables binarias que representan las categor√≠as principales:  
   - `flag_pizza`  
   - `flag_hamburguesas`  
   - `flag_veggie`  
   - `flag_mexicana`, etc.  

2. **Subcategor√≠as (`subcat_*`)**  
   Algunas categor√≠as maestras tienen subdivisiones m√°s espec√≠ficas, por ejemplo:  
   - `subcat_espa√±ola_tapas`  
   - `subcat_espa√±ola_pulpo`  
   - `subcat_latina_peruana`  
   - `subcat_veggie_sin_gluten`  

### L√≥gica de filtrado
- Los filtros se aplican en **modo AND**: si se seleccionan varias categor√≠as/subcategor√≠as, el restaurante debe cumplirlas todas.    
- Si no se selecciona ning√∫n flag ni subcategor√≠a, esta capa no elimina resultados.  



"""

def filtrar_flags(df, flags_activos=None, top_n=50):
    """Filtra el DataFrame por flags y subcategor√≠as (modo AND)."""
    df_filtrado = df.copy()
    resultados = pd.DataFrame()

    # --- Separar flags principales y subcategor√≠as ---
    subcats = [f for f in flags_activos if f.startswith("subcat_")] if flags_activos else []
    main_flags = [f for f in flags_activos if f.startswith("flag_")] if flags_activos else []

    # --- Aplicar flags/subcats (modo AND) ---
    if main_flags or subcats:
        for f in main_flags + subcats:
            if f in df_filtrado.columns:
                df_filtrado = df_filtrado[df_filtrado[f] == 1]
        if not df_filtrado.empty:
            df_filtrado["criterio"] = "üìå Coincide con " + " + ".join(main_flags + subcats)
            resultados = pd.concat([resultados, df_filtrado])

    # --- Si no hay coincidencias ---
    if resultados.empty:
        resultados["criterio"] = "‚ùå Sin resultados"

    resultados = limpiar_duplicados(resultados)

    # --- Asignar prioridad coherente ---
    resultados["criterio_orden"] = resultados["criterio"].apply(asignar_prioridad)

    # --- Ordenar estable ---
    resultados = resultados.sort_values(
        by=["criterio_orden", "rating_final", "reviews_final"],
        ascending=[True, False, False],
        kind="mergesort"
    )

    return seleccionar_columnas(resultados).head(top_n)

"""## Funci√≥n unificada de b√∫squeda + filtrado

Inicialmente se usaban dos funciones distintas:

- **`buscar_prioritario`** ‚Üí cuando el usuario introduc√≠a una *query* de texto libre.  
- **`filtrar_flags`** ‚Üí cuando el usuario solo seleccionaba *flags* o *subcategor√≠as*.  

Para simplificar el flujo se crea una funci√≥n √∫nica **`buscar_y_filtrar`** que unifica ambas:

1. Si el usuario introduce una **query** ‚Üí se ejecuta la l√≥gica de `buscar_prioritario` (con un pre-top de 500 resultados).  
2. Si no hay query ‚Üí se ejecuta la l√≥gica de `filtrar_flags`.  
3. En ambos casos se aplican filtros adicionales de **ciudad**, **precio m√°ximo** (`price_num`) y **rating m√≠nimo** (`rating_final`).  
4. Finalmente se devuelven los resultados ordenados y limitados al `top_n` solicitado.  

De esta forma, el asistente solo necesita invocar **una funci√≥n central** para obtener resultados consistentes, independientemente de si la b√∫squeda se hace por texto libre o solo por categor√≠as.

"""

def buscar_y_filtrar(query="", flags=None, ciudad=None, precio_max=None, rating_min=None, top_n=40):
    """
    Unifica b√∫squeda textual y filtrado por flags/subcategor√≠as,
    aplicando primero los filtros globales (ciudad, precio, rating)
    y luego la b√∫squeda jer√°rquica dentro del subconjunto filtrado.
    """

    df_filtrado = df.copy()

    # --- Filtros previos: ciudad, precio, rating ---
    if ciudad:
        ciudad_norm = quitar_acentos(ciudad.lower().strip())
        df_filtrado = df_filtrado[
            df_filtrado["ciudad"].str.lower().str.strip().apply(quitar_acentos) == ciudad_norm
        ]

    if precio_max is not None:
        df_filtrado = df_filtrado[
            (df_filtrado["price_num"].isna()) | (df_filtrado["price_num"] <= precio_max)
        ]

    if rating_min is not None:
        df_filtrado = df_filtrado[df_filtrado["rating_final"] >= rating_min]

    # --- Si tras los filtros no queda nada, salir ---
    if df_filtrado.empty:
        return pd.DataFrame()

    # --- B√∫squeda prioritaria dentro del subconjunto filtrado ---
    if query.strip():
        resultados = buscar_prioritario(query, flags=flags, top_n=40, df_base=df_filtrado)
    else:
        resultados = filtrar_flags(df_filtrado, flags_activos=flags, top_n=40)

    # --- Orden final coherente y truncado ---
    if not resultados.empty:
        resultados = resultados.sort_values(
            by=["criterio_orden", "rating_final", "reviews_final"],
            ascending=[True, False, False],
            kind="mergesort"
        )

    return resultados.head(top_n)

"""## Filtro por proximidad con OpenRouteService

Adem√°s de los filtros por texto y categor√≠as, el sistema permite restringir los resultados a restaurantes **cercanos a la ubicaci√≥n del usuario**.

En lugar de calcular solo la distancia en l√≠nea recta (*Haversine*), utilizamos la API de **OpenRouteService (ORS)**, que devuelve la **distancia real** y el **tiempo estimado de trayecto** en distintos modos de transporte (*coche, andando, bici, etc.*).

### Ventajas
- Distancia real seg√∫n la red de carreteras o caminos.  
- Tiempo estimado de trayecto en minutos.  
- Geocodificaci√≥n de direcciones ‚Üí conversi√≥n a coordenadas (lat, lon).  

### Flujo de c√°lculo
1. Si el usuario proporciona una direcci√≥n ‚Üí se convierte en coordenadas con ORS.  
2. Se hace un **prefiltrado r√°pido** con Haversine para descartar restaurantes demasiado lejanos.  
3. Para los candidatos cercanos se consulta ORS ‚Üí obteniendo distancia real y tiempo de trayecto.  
4. Se ordenan los resultados por proximidad o se filtran por radio/tiempo m√°ximo.


"""

!pip install openrouteservice

# --- Paso 3: Proximidad geogr√°fica con OpenRouteService ---

import openrouteservice
from openrouteservice import convert

# ‚ö†Ô∏è Necesitas tu propia API KEY de ORS
api_key = "eyJvcmciOiI1YjNjZTM1OTc4NTExMTAwMDFjZjYyNDgiLCJpZCI6ImFmNmI5ZmEwNzQ2YjRmNWY4MjgyMDRlZDlhYzIwNDdlIiwiaCI6Im11cm11cjY0In0="
client = openrouteservice.Client(key=api_key)

# --- Funci√≥n: geocodificar direcci√≥n a coordenadas ---
def direccion_a_coordenadas(direccion, solo_espana=True):
    """
    Devuelve las coordenadas (lat, lon) de una direcci√≥n mediante ORS Pelias.
    Si solo_espana=True, limita la b√∫squeda al territorio espa√±ol
    (previene falsos positivos en direcciones hom√≥nimas).
    """
    try:
        extra = {}
        if solo_espana:
            extra = {
                "boundary.country": "ES",
                "boundary.rect.min_lon": -9.30,  # l√≠mite oeste
                "boundary.rect.min_lat": 41.80,  # l√≠mite sur
                "boundary.rect.max_lon": -6.50,  # l√≠mite este
                "boundary.rect.max_lat": 43.80   # l√≠mite norte
            }

        geocode = client.pelias_search(direccion, extra_params=extra)
        if geocode and "features" in geocode and geocode["features"]:
            coords = geocode["features"][0]["geometry"]["coordinates"]  # [lon, lat]
            return coords[1], coords[0]  # devolver (lat, lon)
        else:
            print(f"‚ö†Ô∏è No se encontraron coordenadas para '{direccion}'")
    except Exception as e:
        print(f"‚ö†Ô∏è Error geocodificando '{direccion}': {e}")
    return None, None


# --- Funci√≥n: calcular distancia real entre 2 puntos ---
def distancia_real(lat1, lon1, lat2, lon2, modo="driving-car"):
    try:
        coords = [(lon1, lat1), (lon2, lat2)]  # ORS usa (lon, lat)
        ruta = client.directions(coords, profile=modo, format="geojson")
        dist_m = ruta["features"][0]["properties"]["segments"][0]["distance"]
        dur_s = ruta["features"][0]["properties"]["segments"][0]["duration"]
        return dist_m/1000, dur_s/60  # km y minutos
    except Exception as e:
        print(f"‚ö†Ô∏è Error en distancia_real con modo {modo}: {e}")
        return None, None


from math import radians, sin, cos, sqrt, atan2
import time

# --- Funci√≥n Haversine (distancia en l√≠nea recta en km) ---
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # radio de la tierra en km
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    return R * c

def filtrar_proximidad_hibrido(lat, lon, radio_km=5, top_n=5, modo="driving-car"):
    # 1. Preselecci√≥n con Haversine (r√°pido, sin API)
    distancias = df.apply(
        lambda row: haversine(lat, lon, row["latitude"], row["longitude"]), axis=1
    )
    df_temp = df.copy()
    df_temp["distancia_haversine"] = distancias
    candidatos = df_temp[df_temp["distancia_haversine"] <= radio_km].head(20)

    # 2. Calcular distancia real solo para esos candidatos
    resultados = []
    for i, row in candidatos.iterrows():
        try:
            d_km, d_min = distancia_real(lat, lon, row["latitude"], row["longitude"], modo=modo)
            resultados.append((row["nombre_final"], row["ciudad"], d_km, d_min, row["maps_url_final"]))
            time.sleep(1)  # evitar saturar API
        except:
            pass

    # 3. Ordenar por distancia real
    df_res = pd.DataFrame(resultados, columns=["nombre_final", "ciudad", "distancia_km", "tiempo_min", "maps_url_final"])
    return df_res.sort_values("distancia_km").head(top_n)

# --- Completar coordenadas faltantes a partir de address_norm ---
def rellenar_coordenadas(df):
    for i, row in df.iterrows():
        if pd.isna(row["latitude"]) or pd.isna(row["longitude"]):
            if "address_norm" in row and pd.notna(row["address_norm"]):
                lat, lon = direccion_a_coordenadas(row["address_norm"])
                if lat and lon:
                    df.at[i, "latitude"] = lat
                    df.at[i, "longitude"] = lon
                    print(f"‚úÖ Coordenadas obtenidas para {row['nombre_final']}: {lat:.5f}, {lon:.5f}")
                else:
                    print(f"‚ö†Ô∏è No se pudo geocodificar la direcci√≥n de {row['nombre_final']}")
    return df

# --- Ejemplo de uso ---
# Direcci√≥n inicial ‚Üí centro de Pontevedra
lat_user, lon_user = direccion_a_coordenadas("Praza da Ferrer√≠a, Pontevedra, Espa√±a")
print("üìç Coordenadas usuario:", lat_user, lon_user)

# Restaurantes en un radio de 3 km en coche
filtrar_proximidad_hibrido(lat_user, lon_user, radio_km=3, modo="driving-car", top_n=5)

"""## Filtro por disponibilidad horaria

Adem√°s de los filtros por categor√≠as o proximidad, el sistema permite comprobar si un restaurante est√° **abierto** en un momento concreto.  

El dataset incluye un campo `horarios_norm` con los horarios en formato string (a veces con guiones largos `‚Äì` o cierres en `24:00`).  
Para poder filtrarlos correctamente se siguen estos pasos:

1. **Conversi√≥n a lista**  
   - El string se convierte a una lista real mediante `ast.literal_eval`.  

2. **Normalizaci√≥n**  
   - Se sustituyen guiones largos `‚Äì` por guiones normales `-`.  
   - Se cambia `24:00` por `23:59` (para que Python lo acepte como hora v√°lida).  

3. **Parseo y comparaci√≥n**  
   - Se parsean los intervalos horarios con `datetime.strptime`.  
   - Se comprueba si la hora actual (o la especificada por el usuario) cae dentro de alguno de los rangos.  
   - Se contemplan casos especiales:
     - *‚Äúcerrado‚Äù* ‚Üí se devuelve como cerrado.  
     - *‚Äúabierto 24 horas‚Äù* ‚Üí se devuelve como abierto.  
     - *‚Äúhorario no disponible‚Äù* ‚Üí se devuelve como abierto.  
     - Rango que cruza medianoche (ej: 20:00‚Äì02:00) ‚Üí se considera abierto si la hora cae antes del cierre o despu√©s de la apertura.

De esta manera es posible filtrar resultados y mostrar √∫nicamente restaurantes que est√©n abiertos en el d√≠a y hora solicitados.


"""

import ast
from datetime import datetime, time
import unicodedata

def quitar_acentos(txt):
    return "".join(
        c for c in unicodedata.normalize("NFD", txt)
        if unicodedata.category(c) != "Mn"
    )

def parse_horarios(val):
    """Convierte string/lista de horarios a lista normalizada."""
    if val is None or (isinstance(val, str) and val.strip() == ""):
        return []
    if isinstance(val, list):
        return val
    if isinstance(val, str):
        if val.strip().startswith("[") and val.strip().endswith("]"):
            try:
                return ast.literal_eval(val)
            except:
                return []
        else:
            return [val]
    return []

def esta_abierto(horarios, fecha_hora=None):
    if fecha_hora is None:
        fecha_hora = datetime.now()

    dias = ["lunes","martes","miercoles","jueves","viernes","sabado","domingo"]
    dia_actual = dias[fecha_hora.weekday()]
    hora_actual = fecha_hora.time()

    if not horarios:
        return True  # si no hay horarios, asumimos abierto

    # Si todos son "horario no disponible", tambi√©n asumimos abierto
    if all("horario no disponible" in quitar_acentos(h.lower()) for h in horarios):
        return True

    for h in horarios:
        h_norm = quitar_acentos(h.lower().strip())

        # D√≠a correcto
        if h_norm.startswith(dia_actual):
            if "cerrado" in h_norm:
                return False
            if "abierto 24 horas" in h_norm:
                return True
            if "horario no disponible" in h_norm:
                continue

            try:
                _, horario_str = h.split(":", 1)
            except ValueError:
                continue

            # Normalizar
            horario_str = horario_str.replace("‚Äì", "-").replace(" ", "")
            rangos = horario_str.split(",")

            for r in rangos:
                if "-" not in r:
                    continue
                ini, fin = r.split("-")
                ini, fin = ini.strip(), fin.strip()

                # Casos especiales
                if fin in ["24:00","00:00","0:00"]:
                    fin = "23:59"

                # Asegurar formato hh:mm
                if len(ini) == 4: ini = "0" + ini
                if len(fin) == 4: fin = "0" + fin

                try:
                    t_ini = datetime.strptime(ini, "%H:%M").time()
                    t_fin = datetime.strptime(fin, "%H:%M").time()

                    if t_ini <= t_fin:
                        if t_ini <= hora_actual <= t_fin:
                            return True
                    else:  # cruza medianoche
                        if hora_actual >= t_ini or hora_actual <= t_fin:
                            return True
                except Exception as e:
                    print("‚ö†Ô∏è Error parseando:", r, "->", e)
    return False



# --- Paso 3: funci√≥n de filtrado ---
def filtrar_abiertos(df, fecha_hora=None):
    return df[df["horarios_lista"].apply(lambda x: esta_abierto(x, fecha_hora))]

# coger un restaurante al azar que tenga horarios
rest = df.iloc[0]
print("‚û°Ô∏è Restaurante:", rest["nombre_final"])
print("‚û°Ô∏è Horarios:", rest["horarios_lista"])

# probar si est√° abierto un viernes a las 22:00
fecha_test = datetime.strptime("2025-09-10 00:00", "%Y-%m-%d %H:%M")
print("‚û°Ô∏è ¬øEst√° abierto mi√©rcoles a las 00:00?:", esta_abierto(rest["horarios_lista"], fecha_hora=fecha_test))

"""## Funci√≥n integrada de recomendaci√≥n + mapa

La l√≥gica completa del recomendador se integra en una √∫nica funci√≥n interactiva **`asistente_recomendador(...)`**, que combina:

1. **B√∫squeda sem√°ntica h√≠brida**  
   - Se apoya en TF-IDF + SBERT, con la l√≥gica de prioridad (exactitud ‚Üí flags/subcategor√≠as ‚Üí sem√°ntica).  

2. **Filtros**  
   - Ciudad, flags y subcategor√≠as, precio m√°ximo (`price_num`) y rating m√≠nimo (`rating_final`).  

3. **Disponibilidad horaria**  
   - Se pueden introducir d√≠a y hora concretos, y solo se muestran restaurantes abiertos en ese momento (considerando tambi√©n casos de *‚Äúhorario no disponible‚Äù* como abiertos).  

4. **Proximidad geogr√°fica**  
   - Si el usuario introduce una direcci√≥n, se geocodifica con ORS.  
   - Se hace un prefiltrado r√°pido con Haversine.  
   - Para los candidatos cercanos se calculan distancias y tiempos reales en coche y andando mediante la API de ORS.  
   - Opcionalmente los resultados se ordenan por proximidad.  

5. **Visualizaci√≥n en mapa**  
   - Los resultados se muestran en un **mapa Folium interactivo** con popups informativos:  
     - Nombre, ciudad, rating, n¬∫ de rese√±as y precio.  
     - Tel√©fono, web, men√∫, reserva online.  
     - Indicadores de delivery y reserva.  
     - Imagen destacada (si est√° disponible).  
   - El mapa incluye una leyenda con iconos diferenciados:  
     - üî¥ Usuario  
     - ‚≠ê Top 3 restaurantes  
     - üç¥ Otros restaurantes  

De esta forma, el asistente gu√≠a al usuario en todo el proceso y devuelve resultados filtrados, ordenados y representados en un mapa interactivo.


"""

import folium
from IPython.display import display

def mostrar_mapa(resultados, lat_centro=None, lon_centro=None, lat_user=None, lon_user=None):
    if resultados.empty:
        print("‚ùå No hay resultados para mostrar en el mapa.")
        return None

    # Usar el primer restaurante como centro si no se pasan coords
    if lat_centro is None or lon_centro is None:
        if "latitude" in resultados.columns and "longitude" in resultados.columns:
            lat_centro = resultados.iloc[0]["latitude"]
            lon_centro = resultados.iloc[0]["longitude"]
        else:
            lat_centro, lon_centro = 42.8782, -8.5448  # fallback Galicia

    # Crear mapa
    mapa = folium.Map(location=[lat_centro, lon_centro], zoom_start=14)

    # Pin rojo: ubicaci√≥n del usuario
    if lat_user is not None and lon_user is not None:
        folium.Marker(
            [lat_user, lon_user],
            popup="üìç Tu ubicaci√≥n",
            icon=folium.Icon(color="red", icon="user", prefix="fa")
        ).add_to(mapa)

    # Seleccionar top 3 restaurantes
    top3_nombres = resultados.head(3)["nombre_final"].tolist()

    # A√±adir marcadores de restaurantes
    for _, row in resultados.iterrows():
        if "latitude" in row and "longitude" in row and pd.notna(row["latitude"]) and pd.notna(row["longitude"]):
            # --- Normalizar precio ---
            precio_txt = "?" if pd.isna(row["price_num"]) else row["price_num"]

            # --- Info b√°sica ---
            popup_html = f"""
            <div style="font-size:14px;">
                <b>{row['nombre_final']}</b><br>
                üìç {row['ciudad']}<br>
                ‚≠ê {row['rating_final']} ({row['reviews_final']} rese√±as) | üí∂ {precio_txt}<br>
                <a href="{row['maps_url_final']}" target="_blank">üåê Abrir en Google Maps</a><br>
            """

            # --- Tel√©fono: preferir international_phone ---
            if "international_phone" in row and pd.notna(row["international_phone"]):
                popup_html += f"üìû {row['international_phone']}<br>"
            elif "phone" in row and pd.notna(row["phone"]):
                popup_html += f"üìû {row['phone']}<br>"

            # --- Info extra ---
            if "website" in row and pd.notna(row["website"]):
                popup_html += f"<a href='{row['website']}' target='_blank'>üîó Web</a><br>"
            if "email" in row and pd.notna(row["email"]):
                popup_html += f"‚úâÔ∏è {row['email']}<br>"
            if "menu_link" in row and pd.notna(row["menu_link"]):
                popup_html += f"<a href='{row['menu_link']}' target='_blank'>üìã Men√∫</a><br>"
            if "reservation_link" in row and pd.notna(row["reservation_link"]):
                popup_html += f"<a href='{row['reservation_link']}' target='_blank'>üìÖ Reservar</a><br>"

            # --- Servicios ---
            if "delivery" in row and row["delivery"] == 1:
                popup_html += "üöö Delivery<br>"
            if "reservable" in row and row["reservable"] == 1:
                popup_html += "üìÖ Reservable<br>"

            # --- Imagen destacada ---
            if "featured_image" in row and pd.notna(row["featured_image"]):
                popup_html += f"<br><img src='{row['featured_image']}' width='200'>"

            popup_html += "</div>"

            # --- Icono seg√∫n top3 o no ---
            if row["nombre_final"] in top3_nombres:
                icono = folium.Icon(color="blue", icon="star", prefix="fa")
            else:
                icono = folium.Icon(color="gray", icon="cutlery", prefix="fa")

            folium.Marker(
                location=[row["latitude"], row["longitude"]],
                popup=folium.Popup(popup_html, max_width=300),
                tooltip=row["nombre_final"],
                icon=icono
            ).add_to(mapa)

    # üîé Leyenda con los mismos iconos que el mapa
    legend_html = """
    <div style="
        position: fixed;
        bottom: 20px; left: 20px; width: 240px;
        z-index:9999; font-size:14px;
        background:white; padding:10px; border:2px solid grey; border-radius:8px;">
    <b>üîé Leyenda</b><br>
    <i class="fa fa-user fa-lg" style="color:red"></i> Tu ubicaci√≥n<br>
    <i class="fa fa-star fa-lg" style="color:blue"></i> Top 3 restaurantes<br>
    <i class="fa fa-cutlery fa-lg" style="color:gray"></i> Otros restaurantes
    </div>
    """
    mapa.get_root().html.add_child(folium.Element(legend_html))


    display(mapa)
    return mapa

from datetime import datetime, timedelta
import time  #  aseguramos el import correcto

def asistente_recomendador():
    print("üëã Bienvenido al recomendador gastron√≥mico")

    # --- Preguntar ciudad ---
    ciudades_disponibles = sorted(df["ciudad"].dropna().str.lower().unique())
    while True:
        ciudad = input(f"\nüìç Ciudades disponibles: {', '.join(ciudades_disponibles)}\nüìç ¬øEn qu√© ciudad quieres comer? ").strip().lower()
        if ciudad in ciudades_disponibles:
            break
        print(f"‚ö†Ô∏è Ciudad no disponible: {ciudad}. Opciones v√°lidas: {', '.join(ciudades_disponibles)}")

    # --- Precio m√°ximo ---
    while True:
        precio_txt = input("\nüí∞ Precio m√°ximo (1=barato ... 4=caro, ENTER = sin Precio m√°ximo): ").strip()
        if not precio_txt:
            precio_max = None
            break
        if precio_txt.isdigit() and 1 <= int(precio_txt) <= 4:
            precio_max = float(precio_txt)
            break
        print("‚ö†Ô∏è Precio m√°ximo inv√°lido. Debe estar entre 1 y 4.")

    # --- Rating m√≠nimo ---
    while True:
        rating_txt = input("\n‚≠ê Rating m√≠nimo (ej: 4.0, ENTER = sin Rating m√≠nimo ): ").strip().replace(",", ".")
        if not rating_txt:
            rating_min = None
            break
        try:
            rating_val = float(rating_txt)
            if 0 <= rating_val <= 5:
                rating_min = rating_val
                break
            else:
                print("‚ö†Ô∏è Rating inv√°lido. Debe estar entre 0.0 y 5.0")
        except:
            print("‚ö†Ô∏è Introduce un n√∫mero v√°lido (ej: 4.2).")

    # --- Flags (filtro principal) ---
    flags_disponibles = [c.replace("flag_", "") for c in df.columns if c.startswith("flag_")]

    print("\nüìã Categor√≠as gastron√≥micas disponibles y sus subcategor√≠as:")
    for f in sorted(flags_disponibles):
        maestro = f
        posibles_subcats = [c for c in df.columns if c.startswith(f"subcat_{maestro}_")]
        if posibles_subcats:
            subcats_txt = ", ".join([c.replace(f"subcat_{maestro}_", "") for c in posibles_subcats])
            print(f"   - {maestro} ‚Üí [{subcats_txt}]")
        else:
            print(f"   - {maestro}")

    while True:
        flags_txt = input(
            "üçî Indica una o varias categor√≠as separadas por coma (ej: pizza, veggie, italiana). ENTER = ninguna: "
        ).strip()

        if not flags_txt:
            flags = []
            break

        # Normalizar entrada y validar
        flags_input = [x.strip().lower() for x in flags_txt.split(",") if x.strip()]
        flags_invalidas = [f for f in flags_input if f not in flags_disponibles]

        if flags_invalidas:
            print(f"‚ö†Ô∏è Las siguientes categor√≠as no son v√°lidas: {', '.join(flags_invalidas)}")
            print("üí° Revisa la lista anterior e intenta de nuevo.\n")
            continue  # vuelve a pedir toda la lista

        if not flags_input:
            print("‚ö†Ô∏è No se reconoci√≥ ning√∫n filtro v√°lido. Int√©ntalo de nuevo.\n")
            continue

        # Si todo es v√°lido, convertir a formato flag_ y respetar el orden
        flags = ["flag_" + f for f in flags_input]
        print(f"‚úÖ Categor√≠as seleccionadas: {', '.join(flags_input)}")
        break

    # --- Subcategor√≠as ---
    if flags:
        for f in flags.copy():
            maestro = f.replace("flag_", "")
            posibles_subcats = [c for c in df.columns if c.startswith(f"subcat_{maestro}_")]

            if posibles_subcats:
                subcats_limpias = [c.replace(f"subcat_{maestro}_", "") for c in posibles_subcats]

                print(f"\nüìã Subcategor√≠as disponibles para {maestro}:")
                for sc in subcats_limpias:
                    print(f"   - {sc}")
                while True:
                    subcat = input(f"üëâ ¬øQuieres alguna subcategor√≠a de {maestro}? (ENTER = sin subcategor√≠a): ").strip().lower().replace(" ", "_")
                    if not subcat:
                        break
                    elif f"subcat_{maestro}_{subcat}" in df.columns:
                        flags.append(f"subcat_{maestro}_{subcat}")
                        print(f"‚úÖ Subcategor√≠a a√±adida: {subcat}")
                        break
                    else:
                        print(f"‚ö†Ô∏è Subcategor√≠a no v√°lida: {subcat}.")
                        print("üí° Revisa la lista anterior e intenta de nuevo.\n")


    # --- Query (texto libre) ---
    query = input("\nüîé ¬øQuieres afinar con un t√©rmino libre (ej: terraza, barato, men√∫ degustaci√≥n...)? ENTER = ninguno: ").strip()

    # --- Preguntar si quiere filtrar por d√≠a y horario ---
    usar_horarios = None
    while usar_horarios not in ["s", "n"]:
        usar_horarios = input("\nüìÖ ¬øQuieres filtrar por d√≠a y horario de apertura? (s/n): ").strip().lower()
        if usar_horarios not in ["s", "n"]:
            print("‚ö†Ô∏è Respuesta no v√°lida. Escribe 's' o 'n'.")
    usar_horarios = (usar_horarios == "s")

    fecha_hora = None
    if usar_horarios:
        dias = ["lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo"]

        # --- Pedir d√≠a ---
        while True:
            dia_txt = input("\nüëâ D√≠a de la semana (ej: viernes, ENTER = hoy): ").strip().lower()
            if not dia_txt:
                # si el usuario da ENTER, se usa el d√≠a actual
                fecha = datetime.now().date()
                break
            elif dia_txt in dias:
                hoy = datetime.now()
                dia_idx = dias.index(dia_txt)
                delta_dias = (dia_idx - hoy.weekday()) % 7
                fecha = (hoy + timedelta(days=delta_dias)).date()
                break
            else:
                print("‚ö†Ô∏è D√≠a no v√°lido. Debe ser uno de:", ", ".join(dias))

        # --- Pedir hora ---
        while True:
            hora_txt = input("\nüëâ Hora en formato 24h (ej: 22:00, ENTER = ahora): ").strip().replace(".", ":")
            if not hora_txt:
                hora = datetime.now().time()
                break
            try:
                hora = datetime.strptime(hora_txt, "%H:%M").time()
                break
            except ValueError:
                print("‚ö†Ô∏è Hora no v√°lida. Usa el formato HH:MM (por ejemplo, 21:30).")

        fecha_hora = datetime.combine(fecha, hora)


    # --- Direcci√≥n del usuario (opcional) ---
    lat_user, lon_user = None, None  # valores por defecto
    intentos = 0

    while True:
        direccion_usuario = input(
            "\nüöó Introduce tu direcci√≥n (ej: 'R√∫a do Franco 10, Santiago de Compostela, A Coru√±a') "
            "ENTER = sin direcci√≥n: "
        ).strip()

        if not direccion_usuario:
            print("‚ÑπÔ∏è No se indic√≥ direcci√≥n. Se continuar√° sin ubicaci√≥n.")
            break

        lat_user, lon_user = direccion_a_coordenadas(direccion_usuario)
        if lat_user and lon_user:
            print(f"‚úÖ Direcci√≥n reconocida: {direccion_usuario} ‚Üí ({lat_user:.5f}, {lon_user:.5f})")
            break
        else:
            intentos += 1
            print("‚ö†Ô∏è No se pudo reconocer la direcci√≥n.")
            if intentos < 3:
                print("üí° Prueba a escribirla con m√°s detalle (ejemplo: 'Calle Real 5, A Coru√±a, Galicia').\n")
            else:
                print("‚ùå No se pudo obtener la ubicaci√≥n tras varios intentos. Se continuar√° sin ubicaci√≥n.")
                lat_user, lon_user = None, None
                break

    # --- Ordenar proximidad (solo si hay direcci√≥n) ---
    ordenar_por_proximidad = False

    if lat_user and lon_user:
        while True:
            respuesta = input("üìç ¬øQuieres ordenar los resultados por cercan√≠a en coche? (s/n): ").strip().lower()

            if respuesta in ["s", "n"]:
                ordenar_por_proximidad = (respuesta == "s")
                break

            print("\n‚ö†Ô∏è Respuesta no v√°lida. Escribe 's' para s√≠ o 'n' para no.")


    # --- B√∫squeda unificada ---
    resultados = buscar_y_filtrar(
        query=query,
        flags=flags,
        ciudad=ciudad,
        precio_max=precio_max,
        rating_min=rating_min,
        top_n=100
    )
    # --- Depuraci√≥n: verificar prioridades ---
    if "criterio" in resultados.columns:
        print("\nüß© Depuraci√≥n de prioridades:")
        print(resultados[["nombre_final", "criterio", "criterio_orden", "rating_final"]].head(20))


    # --- Normalizar horarios ---
    if "horarios_norm" in resultados.columns:
        resultados["horarios_lista"] = resultados["horarios_norm"].apply(parse_horarios)
    else:
        resultados = resultados.merge(df[["nombre_final", "horarios_norm"]], on="nombre_final", how="left")
        resultados["horarios_lista"] = resultados["horarios_norm"].apply(parse_horarios)

    # --- Aplicar filtro si corresponde ---
    if usar_horarios and fecha_hora:
        resultados = filtrar_abiertos(resultados, fecha_hora=fecha_hora)

    # --- Completar coordenadas ---
    resultados = rellenar_coordenadas(resultados)

    # --- Distancias coche y andando ---
    if not resultados.empty and lat_user and lon_user:
        resultados["distancia_haversine"] = resultados.apply(
            lambda row: haversine(lat_user, lon_user, row["latitude"], row["longitude"])
            if pd.notna(row["latitude"]) and pd.notna(row["longitude"]) else None,
            axis=1
        )

        candidatos = resultados.nsmallest(40, "distancia_haversine").copy()
        for _, row in candidatos.iterrows():
            try:
                d_car_km, d_car_min = distancia_real(lat_user, lon_user, row["latitude"], row["longitude"], modo="driving-car")
                d_walk_km, d_walk_min = distancia_real(lat_user, lon_user, row["latitude"], row["longitude"], modo="foot-walking")
                resultados.loc[resultados["nombre_final"] == row["nombre_final"], "distancia_car_km"] = d_car_km
                resultados.loc[resultados["nombre_final"] == row["nombre_final"], "tiempo_car_min"] = d_car_min
                resultados.loc[resultados["nombre_final"] == row["nombre_final"], "distancia_walk_km"] = d_walk_km
                resultados.loc[resultados["nombre_final"] == row["nombre_final"], "tiempo_walk_min"] = d_walk_min
                time.sleep(1)  # ‚úÖ ahora correcto
            except Exception as e:
                print(f"‚ö†Ô∏è Error calculando distancias para {row['nombre_final']}: {e}")


        # --- Reordenar resultados finales respetando prioridad ---
        orden_cols = ["criterio_orden", "rating_final", "reviews_final"]
        asc = [True, False, False]

        if ordenar_por_proximidad:
            # Insertamos distancia despu√©s de la prioridad, antes del rating
            orden_cols.insert(1, "distancia_car_km")
            asc.insert(1, True)

        resultados = resultados.sort_values(
            by=orden_cols, ascending=asc, kind="mergesort"
        )


    # --- Mostrar resultados ---
    if resultados.empty:
        print("‚ùå No se encontraron restaurantes con esos criterios.")
    else:
        print("\n‚úÖ Aqu√≠ tienes tus recomendaciones:")
        for _, row in resultados.head(15).iterrows():
            car_info = f" üöó {row['distancia_car_km']:.1f} km | {row['tiempo_car_min']:.0f} min" if pd.notna(row.get("distancia_car_km")) else ""
            walk_info = f" üö∂ {row['distancia_walk_km']:.1f} km | {row['tiempo_walk_min']:.0f} min" if pd.notna(row.get("distancia_walk_km")) else ""

            print(f"‚û°Ô∏è {row['nombre_final']} ({row['ciudad']}) ‚≠ê {row['rating_final']} "
                  f"({row['reviews_final']} rese√±as) üí∞ {row['price_num']}{car_info}{walk_info}\n"
                  f"   {row['maps_url_final']}\n"
                  f"   üìù Este restaurante se ha elegido porque cumple: {row['criterio']}\n")

        # --- Mapa interactivo ---
        resultados = resultados.dropna(subset=["latitude", "longitude"])
        if not resultados.empty:
            mostrar_mapa(resultados.head(15), lat_user=lat_user, lon_user=lon_user)

"""## ***Recomendador***"""

# Ejecutar solo esta celda
asistente_recomendador()

import pandas as pd

# Cargar datasets
df_core = pd.read_json("restaurantes_con_subcategorias.json")   # el limpio
df_extra = pd.read_json("restaurantes_extra.json")         # info adicional

import pandas as pd

# Cargar datasets
df_core = pd.read_json("restaurantes_con_subcategorias.json")   # el limpio
df_extra = pd.read_json("restaurantes_extra.json")         # info adicional

# Asegurar que tienen la columna id
print("Core:", df_core.shape, "Extra:", df_extra.shape)

# Hacemos el merge por id (left = mantener solo los del core limpio)
df_enriquecido = df_core.merge(df_extra, on="id", how="left")

print("‚úÖ Dataset enriquecido:", df_enriquecido.shape)

# Guardar resultado para usar en el recomendador
df_enriquecido.to_json("restaurantes_enriquecido.json", orient="records", force_ascii=False, indent=2)